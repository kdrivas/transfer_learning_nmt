{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘./data/translate/raw/Religioso/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./data/translate/preprocessed/Religioso/char/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./nb/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./src/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./results/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./joeynmt/joeynmt/.ipynb_checkpoints’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!find . -name .ipynb* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 500                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 32                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: True                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/translate/Religioso/char\n",
      "2020-02-27 11:33:26,235 Hello! This is Joey-NMT.\n",
      "2020-02-27 11:33:26.804016: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-02-27 11:33:26.804074: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-02-27 11:33:26.804083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2020-02-27 11:33:27,397 Total params: 12451936\n",
      "2020-02-27 11:33:27,397 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-27 11:33:29,186 cfg.name                           : my_experiment\n",
      "2020-02-27 11:33:29,186 cfg.data.src                       : es\n",
      "2020-02-27 11:33:29,186 cfg.data.trg                       : shp\n",
      "2020-02-27 11:33:29,186 cfg.data.train                     : data/translate/preprocessed/Religioso/char/train\n",
      "2020-02-27 11:33:29,186 cfg.data.dev                       : data/translate/preprocessed/Religioso/char/valid\n",
      "2020-02-27 11:33:29,186 cfg.data.test                      : data/translate/preprocessed/Religioso/char/test\n",
      "2020-02-27 11:33:29,186 cfg.data.level                     : char\n",
      "2020-02-27 11:33:29,186 cfg.data.lowercase                 : True\n",
      "2020-02-27 11:33:29,186 cfg.data.max_sent_length           : 150\n",
      "2020-02-27 11:33:29,186 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-27 11:33:29,186 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-27 11:33:29,186 cfg.testing.beam_size              : 5\n",
      "2020-02-27 11:33:29,186 cfg.testing.alpha                  : 1.0\n",
      "2020-02-27 11:33:29,186 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-27 11:33:29,186 cfg.training.reset_scheduler       : False\n",
      "2020-02-27 11:33:29,186 cfg.training.reset_optimizer       : False\n",
      "2020-02-27 11:33:29,186 cfg.training.random_seed           : 42\n",
      "2020-02-27 11:33:29,187 cfg.training.optimizer             : adam\n",
      "2020-02-27 11:33:29,187 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-27 11:33:29,187 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-27 11:33:29,187 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-27 11:33:29,187 cfg.training.weight_decay          : 0.0\n",
      "2020-02-27 11:33:29,187 cfg.training.batch_size            : 48\n",
      "2020-02-27 11:33:29,187 cfg.training.batch_type            : sentence\n",
      "2020-02-27 11:33:29,187 cfg.training.eval_batch_size       : 10\n",
      "2020-02-27 11:33:29,187 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-27 11:33:29,187 cfg.training.batch_multiplier      : 1\n",
      "2020-02-27 11:33:29,187 cfg.training.scheduling            : plateau\n",
      "2020-02-27 11:33:29,187 cfg.training.patience              : 500\n",
      "2020-02-27 11:33:29,187 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-27 11:33:29,187 cfg.training.epochs                : 32\n",
      "2020-02-27 11:33:29,187 cfg.training.validation_freq       : 10\n",
      "2020-02-27 11:33:29,187 cfg.training.logging_freq          : 1000\n",
      "2020-02-27 11:33:29,187 cfg.training.eval_metric           : bleu\n",
      "2020-02-27 11:33:29,187 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-27 11:33:29,187 cfg.training.model_dir             : results/translate/es-shp_Religioso_300_512/char\n",
      "2020-02-27 11:33:29,187 cfg.training.overwrite             : True\n",
      "2020-02-27 11:33:29,187 cfg.training.shuffle               : True\n",
      "2020-02-27 11:33:29,187 cfg.training.use_cuda              : True\n",
      "2020-02-27 11:33:29,187 cfg.training.max_output_length     : 60\n",
      "2020-02-27 11:33:29,187 cfg.training.print_valid_sents     : []\n",
      "2020-02-27 11:33:29,187 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-27 11:33:29,187 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-27 11:33:29,187 cfg.model.initializer              : xavier\n",
      "2020-02-27 11:33:29,187 cfg.model.init_weight              : 0.01\n",
      "2020-02-27 11:33:29,188 cfg.model.init_gain                : 1.0\n",
      "2020-02-27 11:33:29,188 cfg.model.bias_initializer         : zeros\n",
      "2020-02-27 11:33:29,188 cfg.model.embed_initializer        : normal\n",
      "2020-02-27 11:33:29,188 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-27 11:33:29,188 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-27 11:33:29,188 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-27 11:33:29,188 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-27 11:33:29,188 cfg.model.tied_embeddings          : False\n",
      "2020-02-27 11:33:29,188 cfg.model.tied_softmax             : False\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.type             : recurrent\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.embeddings.embedding_dim : 300\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.hidden_size      : 512\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-27 11:33:29,188 cfg.model.encoder.freeze           : False\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.type             : recurrent\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.embeddings.embedding_dim : 300\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.hidden_size      : 512\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-27 11:33:29,188 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-27 11:33:29,189 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-27 11:33:29,189 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-27 11:33:29,189 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-27 11:33:29,189 cfg.model.decoder.freeze           : False\n",
      "2020-02-27 11:33:29,189 Data set sizes: \n",
      "\ttrain 4975,\n",
      "\tvalid 749,\n",
      "\ttest 749\n",
      "2020-02-27 11:33:29,189 First training example:\n",
      "\t[SRC] @@ i k a x b i @@ j a @@ n o n @@ i b o n @@ j o i r a @@ j a w e t i a n b i @@ k e y ó y a m a i @@ i k i\n",
      "\t[TRG] @@ p e r o @@ l a @@ p a l a b r a @@ d e l @@ s e ñ o r @@ p e r m a n e c e @@ e t e r n a m e n t e @@ e s t a @@ p a l a b r a @@ e s @@ e l @@ e v a n g e l i o @@ q u e @@ s e @@ l e s @@ h a @@ a n u n c i a d o @@ a @@ u s t e d e s\n",
      "2020-02-27 11:33:29,189 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a (5) @@ (6) i (7) n (8) o (9) k\n",
      "2020-02-27 11:33:29,189 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) @@ (5) e (6) a (7) o (8) s (9) n\n",
      "2020-02-27 11:33:29,189 Number of Src words (types): 36\n",
      "2020-02-27 11:33:29,189 Number of Trg words (types): 36\n",
      "2020-02-27 11:33:29,189 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(300, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(812, 512, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=300, vocab_size=36),\n",
      "\ttrg_embed=Embeddings(embedding_dim=300, vocab_size=36))\n",
      "2020-02-27 11:33:29,189 EPOCH 1\n",
      "2020-02-27 11:33:47,984 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 11:33:47,984 Saving new checkpoint.\n",
      "2020-02-27 11:33:48,118 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 166065.5469, ppl:  17.8630, duration: 14.1675s\n",
      "2020-02-27 11:34:06,175 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 162445.1719, ppl:  16.7749, duration: 14.0633s\n",
      "2020-02-27 11:34:23,687 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 158454.1719, ppl:  15.6521, duration: 14.0735s\n",
      "2020-02-27 11:34:41,510 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 149395.0625, ppl:  13.3745, duration: 14.1426s\n",
      "2020-02-27 11:34:59,200 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 140036.0156, ppl:  11.3689, duration: 14.0203s\n",
      "2020-02-27 11:35:16,730 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 133486.5156, ppl:  10.1471, duration: 13.9388s\n",
      "2020-02-27 11:35:34,414 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 129335.3672, ppl:   9.4417, duration: 14.1125s\n",
      "2020-02-27 11:35:52,703 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 126376.6953, ppl:   8.9690, duration: 14.1098s\n",
      "2020-02-27 11:36:10,783 Validation result (greedy) at epoch   1, step       90: bleu:   0.00, loss: 123890.7500, ppl:   8.5902, duration: 14.0641s\n",
      "2020-02-27 11:36:28,566 Validation result (greedy) at epoch   1, step      100: bleu:   0.00, loss: 123115.6719, ppl:   8.4754, duration: 14.0924s\n",
      "2020-02-27 11:36:29,993 Epoch   1: total training loss 17488.72\n",
      "2020-02-27 11:36:29,994 EPOCH 2\n",
      "2020-02-27 11:36:46,640 Validation result (greedy) at epoch   2, step      110: bleu:   0.00, loss: 118772.6719, ppl:   7.8599, duration: 14.0802s\n",
      "2020-02-27 11:37:04,909 Validation result (greedy) at epoch   2, step      120: bleu:   0.00, loss: 117022.5547, ppl:   7.6247, duration: 14.0669s\n",
      "2020-02-27 11:37:22,624 Validation result (greedy) at epoch   2, step      130: bleu:   0.00, loss: 116022.0781, ppl:   7.4934, duration: 14.0118s\n",
      "2020-02-27 11:37:40,856 Validation result (greedy) at epoch   2, step      140: bleu:   0.00, loss: 114102.8594, ppl:   7.2479, duration: 14.0291s\n",
      "2020-02-27 11:37:58,934 Validation result (greedy) at epoch   2, step      150: bleu:   0.00, loss: 112728.2891, ppl:   7.0770, duration: 14.0090s\n",
      "2020-02-27 11:38:16,369 Validation result (greedy) at epoch   2, step      160: bleu:   0.00, loss: 112095.0859, ppl:   6.9996, duration: 14.4016s\n",
      "2020-02-27 11:38:34,166 Validation result (greedy) at epoch   2, step      170: bleu:   0.00, loss: 111015.9688, ppl:   6.8697, duration: 14.1824s\n",
      "2020-02-27 11:38:51,999 Validation result (greedy) at epoch   2, step      180: bleu:   0.00, loss: 109474.3438, ppl:   6.6883, duration: 13.6887s\n",
      "2020-02-27 11:39:10,442 Validation result (greedy) at epoch   2, step      190: bleu:   0.00, loss: 108567.6719, ppl:   6.5839, duration: 14.1037s\n",
      "2020-02-27 11:39:28,475 Validation result (greedy) at epoch   2, step      200: bleu:   0.00, loss: 107446.4062, ppl:   6.4570, duration: 14.0438s\n",
      "2020-02-27 11:39:31,841 Epoch   2: total training loss 13912.99\n",
      "2020-02-27 11:39:31,842 EPOCH 3\n",
      "2020-02-27 11:39:46,700 Validation result (greedy) at epoch   3, step      210: bleu:   0.00, loss: 106290.5469, ppl:   6.3287, duration: 14.0275s\n",
      "2020-02-27 11:40:04,528 Validation result (greedy) at epoch   3, step      220: bleu:   0.00, loss: 105321.1953, ppl:   6.2231, duration: 14.0779s\n",
      "2020-02-27 11:40:22,422 Validation result (greedy) at epoch   3, step      230: bleu:   0.00, loss: 105008.7422, ppl:   6.1895, duration: 14.0887s\n",
      "2020-02-27 11:40:40,425 Validation result (greedy) at epoch   3, step      240: bleu:   0.00, loss: 103714.2656, ppl:   6.0519, duration: 14.0983s\n",
      "2020-02-27 11:40:58,060 Validation result (greedy) at epoch   3, step      250: bleu:   0.00, loss: 102452.0312, ppl:   5.9208, duration: 13.6818s\n",
      "2020-02-27 11:41:16,092 Validation result (greedy) at epoch   3, step      260: bleu:   0.00, loss: 101921.1953, ppl:   5.8665, duration: 13.7024s\n",
      "2020-02-27 11:41:33,736 Validation result (greedy) at epoch   3, step      270: bleu:   0.00, loss: 101117.5000, ppl:   5.7852, duration: 13.7072s\n",
      "2020-02-27 11:41:52,069 Validation result (greedy) at epoch   3, step      280: bleu:   0.00, loss: 100314.0391, ppl:   5.7050, duration: 14.1099s\n",
      "2020-02-27 11:42:09,926 Validation result (greedy) at epoch   3, step      290: bleu:   0.00, loss: 99169.9922, ppl:   5.5929, duration: 13.9571s\n",
      "2020-02-27 11:42:28,009 Validation result (greedy) at epoch   3, step      300: bleu:   0.00, loss: 98637.1016, ppl:   5.5414, duration: 13.9909s\n",
      "2020-02-27 11:42:46,221 Validation result (greedy) at epoch   3, step      310: bleu:   0.00, loss: 97011.5000, ppl:   5.3872, duration: 14.1330s\n",
      "2020-02-27 11:42:47,079 Epoch   3: total training loss 12716.49\n",
      "2020-02-27 11:42:47,080 EPOCH 4\n",
      "2020-02-27 11:43:04,337 Validation result (greedy) at epoch   4, step      320: bleu:   0.00, loss: 98704.0391, ppl:   5.5478, duration: 14.0570s\n",
      "2020-02-27 11:43:22,926 Validation result (greedy) at epoch   4, step      330: bleu:   0.00, loss: 96423.9375, ppl:   5.3325, duration: 14.5653s\n",
      "2020-02-27 11:43:40,322 Validation result (greedy) at epoch   4, step      340: bleu:   0.00, loss: 95439.1406, ppl:   5.2421, duration: 13.6981s\n",
      "2020-02-27 11:43:58,523 Validation result (greedy) at epoch   4, step      350: bleu:   0.00, loss: 94611.1562, ppl:   5.1673, duration: 14.2329s\n",
      "2020-02-27 11:44:16,756 Validation result (greedy) at epoch   4, step      360: bleu:   0.00, loss: 93788.3125, ppl:   5.0940, duration: 14.1200s\n",
      "2020-02-27 11:44:35,025 Validation result (greedy) at epoch   4, step      370: bleu:   0.00, loss: 93840.4922, ppl:   5.0987, duration: 14.0608s\n",
      "2020-02-27 11:44:52,588 Validation result (greedy) at epoch   4, step      380: bleu:   0.00, loss: 92712.8516, ppl:   4.9998, duration: 13.7582s\n",
      "2020-02-27 11:45:09,789 Validation result (greedy) at epoch   4, step      390: bleu:   0.00, loss: 92106.8438, ppl:   4.9475, duration: 13.6061s\n",
      "2020-02-27 11:45:28,001 Validation result (greedy) at epoch   4, step      400: bleu:   0.00, loss: 91196.5703, ppl:   4.8699, duration: 13.9774s\n",
      "2020-02-27 11:45:45,993 Validation result (greedy) at epoch   4, step      410: bleu:   0.00, loss: 90440.4688, ppl:   4.8064, duration: 14.1691s\n",
      "2020-02-27 11:45:48,374 Epoch   4: total training loss 11773.55\n",
      "2020-02-27 11:45:48,375 EPOCH 5\n",
      "2020-02-27 11:46:03,854 Validation result (greedy) at epoch   5, step      420: bleu:   0.00, loss: 90045.0000, ppl:   4.7736, duration: 13.7247s\n",
      "2020-02-27 11:46:22,104 Validation result (greedy) at epoch   5, step      430: bleu:   0.00, loss: 89586.8359, ppl:   4.7357, duration: 13.7109s\n",
      "2020-02-27 11:46:39,782 Validation result (greedy) at epoch   5, step      440: bleu:   0.00, loss: 89010.5547, ppl:   4.6886, duration: 13.7538s\n",
      "2020-02-27 11:46:57,836 Validation result (greedy) at epoch   5, step      450: bleu:   0.00, loss: 88560.1406, ppl:   4.6521, duration: 13.7710s\n",
      "2020-02-27 11:47:14,877 Validation result (greedy) at epoch   5, step      460: bleu:   0.00, loss: 87632.2969, ppl:   4.5778, duration: 13.7050s\n",
      "2020-02-27 11:47:32,935 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 11:47:32,936 Saving new checkpoint.\n",
      "2020-02-27 11:47:33,102 Validation result (greedy) at epoch   5, step      470: bleu:   0.13, loss: 87842.1953, ppl:   4.5945, duration: 14.2320s\n",
      "2020-02-27 11:47:51,131 Validation result (greedy) at epoch   5, step      480: bleu:   0.00, loss: 87027.0312, ppl:   4.5299, duration: 13.9406s\n",
      "2020-02-27 11:48:09,135 Validation result (greedy) at epoch   5, step      490: bleu:   0.00, loss: 86692.1094, ppl:   4.5037, duration: 14.1109s\n",
      "2020-02-27 11:48:27,513 Validation result (greedy) at epoch   5, step      500: bleu:   0.00, loss: 85786.2578, ppl:   4.4334, duration: 14.0947s\n",
      "2020-02-27 11:48:45,581 Validation result (greedy) at epoch   5, step      510: bleu:   0.13, loss: 85021.4219, ppl:   4.3749, duration: 14.0685s\n",
      "2020-02-27 11:49:03,898 Validation result (greedy) at epoch   5, step      520: bleu:   0.00, loss: 84599.4297, ppl:   4.3430, duration: 14.2801s\n",
      "2020-02-27 11:49:03,899 Epoch   5: total training loss 11004.37\n",
      "2020-02-27 11:49:03,899 EPOCH 6\n",
      "2020-02-27 11:49:21,940 Validation result (greedy) at epoch   6, step      530: bleu:   0.00, loss: 84707.0469, ppl:   4.3511, duration: 14.0646s\n",
      "2020-02-27 11:49:39,643 Validation result (greedy) at epoch   6, step      540: bleu:   0.00, loss: 83765.2969, ppl:   4.2806, duration: 13.9925s\n",
      "2020-02-27 11:49:57,529 Validation result (greedy) at epoch   6, step      550: bleu:   0.00, loss: 84548.8984, ppl:   4.3392, duration: 14.0774s\n",
      "2020-02-27 11:50:15,931 Validation result (greedy) at epoch   6, step      560: bleu:   0.13, loss: 82969.3672, ppl:   4.2218, duration: 14.0174s\n",
      "2020-02-27 11:50:33,995 Validation result (greedy) at epoch   6, step      570: bleu:   0.00, loss: 83230.8594, ppl:   4.2410, duration: 13.7128s\n",
      "2020-02-27 11:50:51,543 Validation result (greedy) at epoch   6, step      580: bleu:   0.00, loss: 82949.6250, ppl:   4.2204, duration: 13.7561s\n",
      "2020-02-27 11:51:08,654 Validation result (greedy) at epoch   6, step      590: bleu:   0.00, loss: 83920.7500, ppl:   4.2921, duration: 13.9944s\n",
      "2020-02-27 11:51:26,541 Validation result (greedy) at epoch   6, step      600: bleu:   0.13, loss: 82182.8203, ppl:   4.1646, duration: 14.0453s\n",
      "2020-02-27 11:51:44,351 Validation result (greedy) at epoch   6, step      610: bleu:   0.00, loss: 81937.9297, ppl:   4.1469, duration: 14.1312s\n",
      "2020-02-27 11:52:02,224 Validation result (greedy) at epoch   6, step      620: bleu:   0.00, loss: 81284.6328, ppl:   4.1001, duration: 13.8038s\n",
      "2020-02-27 11:52:03,863 Epoch   6: total training loss 10420.91\n",
      "2020-02-27 11:52:03,864 EPOCH 7\n",
      "2020-02-27 11:52:20,426 Validation result (greedy) at epoch   7, step      630: bleu:   0.00, loss: 80767.8438, ppl:   4.0635, duration: 14.0482s\n",
      "2020-02-27 11:52:38,046 Validation result (greedy) at epoch   7, step      640: bleu:   0.13, loss: 80837.7500, ppl:   4.0685, duration: 14.1208s\n",
      "2020-02-27 11:52:55,533 Validation result (greedy) at epoch   7, step      650: bleu:   0.13, loss: 80825.5781, ppl:   4.0676, duration: 14.0757s\n",
      "2020-02-27 11:53:13,625 Validation result (greedy) at epoch   7, step      660: bleu:   0.00, loss: 79888.4141, ppl:   4.0020, duration: 14.1000s\n",
      "2020-02-27 11:53:31,580 Validation result (greedy) at epoch   7, step      670: bleu:   0.00, loss: 79408.5938, ppl:   3.9688, duration: 14.0865s\n",
      "2020-02-27 11:53:49,242 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 11:53:49,242 Saving new checkpoint.\n",
      "2020-02-27 11:53:49,385 Validation result (greedy) at epoch   7, step      680: bleu:   0.27, loss: 79109.9453, ppl:   3.9482, duration: 14.2304s\n",
      "2020-02-27 11:54:07,283 Validation result (greedy) at epoch   7, step      690: bleu:   0.13, loss: 78792.4531, ppl:   3.9265, duration: 14.1071s\n",
      "2020-02-27 11:54:24,915 Validation result (greedy) at epoch   7, step      700: bleu:   0.27, loss: 78442.1562, ppl:   3.9027, duration: 14.1285s\n",
      "2020-02-27 11:54:43,211 Validation result (greedy) at epoch   7, step      710: bleu:   0.27, loss: 78743.4141, ppl:   3.9232, duration: 14.1000s\n",
      "2020-02-27 11:55:01,447 Validation result (greedy) at epoch   7, step      720: bleu:   0.27, loss: 78414.0625, ppl:   3.9008, duration: 14.3714s\n",
      "2020-02-27 11:55:04,913 Epoch   7: total training loss 9923.59\n",
      "2020-02-27 11:55:04,914 EPOCH 8\n",
      "2020-02-27 11:55:19,935 Validation result (greedy) at epoch   8, step      730: bleu:   0.27, loss: 77710.3594, ppl:   3.8535, duration: 14.1032s\n",
      "2020-02-27 11:55:38,392 Validation result (greedy) at epoch   8, step      740: bleu:   0.27, loss: 77325.0078, ppl:   3.8278, duration: 14.0950s\n",
      "2020-02-27 11:55:55,966 Validation result (greedy) at epoch   8, step      750: bleu:   0.27, loss: 77395.6953, ppl:   3.8325, duration: 14.0536s\n",
      "2020-02-27 11:56:13,794 Validation result (greedy) at epoch   8, step      760: bleu:   0.27, loss: 77327.7266, ppl:   3.8280, duration: 14.0646s\n",
      "2020-02-27 11:56:31,999 Validation result (greedy) at epoch   8, step      770: bleu:   0.13, loss: 76507.5859, ppl:   3.7738, duration: 14.1831s\n",
      "2020-02-27 11:56:49,908 Validation result (greedy) at epoch   8, step      780: bleu:   0.13, loss: 76830.2891, ppl:   3.7950, duration: 13.7677s\n",
      "2020-02-27 11:57:07,608 Validation result (greedy) at epoch   8, step      790: bleu:   0.27, loss: 76544.9844, ppl:   3.7763, duration: 13.7016s\n",
      "2020-02-27 11:57:25,054 Validation result (greedy) at epoch   8, step      800: bleu:   0.13, loss: 77236.9453, ppl:   3.8219, duration: 13.7037s\n",
      "2020-02-27 11:57:42,594 Validation result (greedy) at epoch   8, step      810: bleu:   0.27, loss: 76235.5781, ppl:   3.7561, duration: 13.7478s\n",
      "2020-02-27 11:58:00,393 Validation result (greedy) at epoch   8, step      820: bleu:   0.13, loss: 75911.6250, ppl:   3.7350, duration: 13.7232s\n",
      "2020-02-27 11:58:18,234 Validation result (greedy) at epoch   8, step      830: bleu:   0.00, loss: 75632.6641, ppl:   3.7170, duration: 13.9439s\n",
      "2020-02-27 11:58:19,004 Epoch   8: total training loss 9528.19\n",
      "2020-02-27 11:58:19,005 EPOCH 9\n",
      "2020-02-27 11:58:36,892 Validation result (greedy) at epoch   9, step      840: bleu:   0.13, loss: 75213.0547, ppl:   3.6900, duration: 14.0413s\n",
      "2020-02-27 11:58:55,087 Validation result (greedy) at epoch   9, step      850: bleu:   0.00, loss: 74807.9062, ppl:   3.6641, duration: 13.8303s\n",
      "2020-02-27 11:59:13,797 Validation result (greedy) at epoch   9, step      860: bleu:   0.13, loss: 75234.8750, ppl:   3.6914, duration: 14.0799s\n",
      "2020-02-27 11:59:31,799 Validation result (greedy) at epoch   9, step      870: bleu:   0.27, loss: 74637.0000, ppl:   3.6533, duration: 13.7464s\n",
      "2020-02-27 11:59:50,096 Validation result (greedy) at epoch   9, step      880: bleu:   0.13, loss: 74462.5781, ppl:   3.6422, duration: 13.7609s\n",
      "2020-02-27 12:00:07,834 Validation result (greedy) at epoch   9, step      890: bleu:   0.27, loss: 74715.3516, ppl:   3.6582, duration: 13.7661s\n",
      "2020-02-27 12:00:25,765 Validation result (greedy) at epoch   9, step      900: bleu:   0.27, loss: 74202.9922, ppl:   3.6259, duration: 13.7126s\n",
      "2020-02-27 12:00:43,644 Validation result (greedy) at epoch   9, step      910: bleu:   0.27, loss: 74192.1562, ppl:   3.6252, duration: 13.7352s\n"
     ]
    }
   ],
   "source": [
    "base_path = 'data/translate/preprocessed'\n",
    "datas = os.listdir(base_path)\n",
    "emb_size = 300\n",
    "hidden_size = 512\n",
    "for data in ['Religioso']:\n",
    "    data_path = os.path.join(base_path, data)\n",
    "    for lang_in, lang_out in [['es', 'shp'], ['shp', 'es']]:\n",
    "        segmentations = os.listdir(data_path)\n",
    "        for segment in segmentations:\n",
    "            segment_path = os.path.join(data_path, segment)\n",
    "            print(os.path.join('results/translate', data, segment))\n",
    "            if 'bpe_drop' in segment:\n",
    "                level = 'bpe'\n",
    "            elif 'bpe' in segment:\n",
    "                level = 'bpe'\n",
    "            elif 'char' in segment:\n",
    "                level = 'char'\n",
    "            elif 'word' in segment:\n",
    "                level = 'word'\n",
    "            elif 'syl' in segment:\n",
    "                level = 'syl'\n",
    "            else:\n",
    "                level = None\n",
    "                \n",
    "            val_freq = 10\n",
    "            f_config = config.format(lang_src=lang_in, lang_tgt=lang_out, \n",
    "                                     train_path=os.path.join(segment_path, 'train'),\n",
    "                                     test_path=os.path.join(segment_path, 'test'),\n",
    "                                     dev_path=os.path.join(segment_path, 'valid'),\n",
    "                                     level=level,\n",
    "                                     emb_size=emb_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     val_freq=val_freq,\n",
    "                                     model_dir=os.path.join('results/translate',\\\n",
    "                                                            f'{lang_in}-{lang_out}_{data}_{emb_size}_{hidden_size}', segment))\n",
    "\n",
    "            with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=\"test\"),'w') as f:\n",
    "                f.write(f_config)\n",
    "\n",
    "            !python3 joeynmt/joeynmt train \"joeynmt/configs/transformer_test.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
