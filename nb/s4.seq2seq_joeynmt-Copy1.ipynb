{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘./nb/.ipynb_checkpoints’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!find . -name .ipynb* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 130             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 500                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 15                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: True                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/translate/Religioso/bpe_8000\n",
      "2020-03-04 15:56:01,720 Hello! This is Joey-NMT.\n",
      "2020-03-04 15:56:02.368723: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-04 15:56:02.368788: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-04 15:56:02.368800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2020-03-04 15:56:02,909 Total params: 17022800\n",
      "2020-03-04 15:56:02,909 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-03-04 15:56:04,754 cfg.name                           : my_experiment\n",
      "2020-03-04 15:56:04,754 cfg.data.src                       : shp\n",
      "2020-03-04 15:56:04,754 cfg.data.trg                       : es\n",
      "2020-03-04 15:56:04,754 cfg.data.train                     : data/translate/preprocessed/Religioso/bpe_8000/train\n",
      "2020-03-04 15:56:04,754 cfg.data.dev                       : data/translate/preprocessed/Religioso/bpe_8000/valid\n",
      "2020-03-04 15:56:04,754 cfg.data.test                      : data/translate/preprocessed/Religioso/bpe_8000/test\n",
      "2020-03-04 15:56:04,754 cfg.data.level                     : bpe\n",
      "2020-03-04 15:56:04,754 cfg.data.lowercase                 : True\n",
      "2020-03-04 15:56:04,754 cfg.data.max_sent_length           : 130\n",
      "2020-03-04 15:56:04,754 cfg.data.src_voc_min_freq          : 1\n",
      "2020-03-04 15:56:04,754 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-03-04 15:56:04,754 cfg.testing.beam_size              : 5\n",
      "2020-03-04 15:56:04,754 cfg.testing.alpha                  : 1.0\n",
      "2020-03-04 15:56:04,754 cfg.training.reset_best_ckpt       : False\n",
      "2020-03-04 15:56:04,754 cfg.training.reset_scheduler       : False\n",
      "2020-03-04 15:56:04,754 cfg.training.reset_optimizer       : False\n",
      "2020-03-04 15:56:04,754 cfg.training.random_seed           : 42\n",
      "2020-03-04 15:56:04,754 cfg.training.optimizer             : adam\n",
      "2020-03-04 15:56:04,754 cfg.training.learning_rate         : 0.0005\n",
      "2020-03-04 15:56:04,754 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-03-04 15:56:04,754 cfg.training.clip_grad_val         : 1.0\n",
      "2020-03-04 15:56:04,754 cfg.training.weight_decay          : 0.0\n",
      "2020-03-04 15:56:04,755 cfg.training.batch_size            : 48\n",
      "2020-03-04 15:56:04,755 cfg.training.batch_type            : sentence\n",
      "2020-03-04 15:56:04,755 cfg.training.eval_batch_size       : 10\n",
      "2020-03-04 15:56:04,755 cfg.training.eval_batch_type       : sentence\n",
      "2020-03-04 15:56:04,755 cfg.training.batch_multiplier      : 1\n",
      "2020-03-04 15:56:04,755 cfg.training.scheduling            : plateau\n",
      "2020-03-04 15:56:04,755 cfg.training.patience              : 500\n",
      "2020-03-04 15:56:04,755 cfg.training.decrease_factor       : 0.5\n",
      "2020-03-04 15:56:04,755 cfg.training.epochs                : 15\n",
      "2020-03-04 15:56:04,755 cfg.training.validation_freq       : 30\n",
      "2020-03-04 15:56:04,755 cfg.training.logging_freq          : 1000\n",
      "2020-03-04 15:56:04,755 cfg.training.eval_metric           : bleu\n",
      "2020-03-04 15:56:04,755 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-03-04 15:56:04,755 cfg.training.model_dir             : results/rnn/translate/shp-es_Religioso_300_512/bpe_8000\n",
      "2020-03-04 15:56:04,755 cfg.training.overwrite             : True\n",
      "2020-03-04 15:56:04,755 cfg.training.shuffle               : True\n",
      "2020-03-04 15:56:04,755 cfg.training.use_cuda              : True\n",
      "2020-03-04 15:56:04,755 cfg.training.max_output_length     : 60\n",
      "2020-03-04 15:56:04,755 cfg.training.print_valid_sents     : []\n",
      "2020-03-04 15:56:04,755 cfg.training.keep_last_ckpts       : 3\n",
      "2020-03-04 15:56:04,755 cfg.training.label_smoothing       : 0.0\n",
      "2020-03-04 15:56:04,755 cfg.model.initializer              : xavier\n",
      "2020-03-04 15:56:04,755 cfg.model.init_weight              : 0.01\n",
      "2020-03-04 15:56:04,755 cfg.model.init_gain                : 1.0\n",
      "2020-03-04 15:56:04,755 cfg.model.bias_initializer         : zeros\n",
      "2020-03-04 15:56:04,755 cfg.model.embed_initializer        : normal\n",
      "2020-03-04 15:56:04,755 cfg.model.embed_init_weight        : 0.1\n",
      "2020-03-04 15:56:04,755 cfg.model.embed_init_gain          : 1.0\n",
      "2020-03-04 15:56:04,755 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-03-04 15:56:04,756 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-03-04 15:56:04,756 cfg.model.tied_embeddings          : False\n",
      "2020-03-04 15:56:04,756 cfg.model.tied_softmax             : False\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.type             : recurrent\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.rnn_type         : gru\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.embeddings.embedding_dim : 300\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.embeddings.scale : False\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.hidden_size      : 512\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.bidirectional    : True\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.dropout          : 0.3\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.num_layers       : 2\n",
      "2020-03-04 15:56:04,756 cfg.model.encoder.freeze           : False\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.type             : recurrent\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.rnn_type         : gru\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.embeddings.embedding_dim : 300\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.embeddings.scale : False\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.hidden_size      : 512\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.dropout          : 0.3\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.num_layers       : 2\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.input_feeding    : True\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.init_hidden      : last\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.attention        : bahdanau\n",
      "2020-03-04 15:56:04,756 cfg.model.decoder.freeze           : False\n",
      "2020-03-04 15:56:04,756 Data set sizes: \n",
      "\ttrain 5996,\n",
      "\tvalid 749,\n",
      "\ttest 749\n",
      "2020-03-04 15:56:04,756 First training example:\n",
      "\t[SRC] pero la palabra del señor permanece eter@@ n@@ amente esta palabra es el evangelio que se les ha anunciado a ustedes\n",
      "\t[TRG] ikaxbi ja non ibon joira jawetianbi keyó@@ yamai iki\n",
      "2020-03-04 15:56:04,757 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) que (5) de (6) y (7) a (8) la (9) el\n",
      "2020-03-04 15:56:04,757 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) iki (5) ja (6) jato (7) mato (8) ea (9) en\n",
      "2020-03-04 15:56:04,757 Number of Src words (types): 4183\n",
      "2020-03-04 15:56:04,757 Number of Trg words (types): 4133\n",
      "2020-03-04 15:56:04,757 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(300, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(812, 512, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=300, vocab_size=4183),\n",
      "\ttrg_embed=Embeddings(embedding_dim=300, vocab_size=4133))\n",
      "2020-03-04 15:56:04,761 EPOCH 1\n",
      "2020-03-04 15:56:11,327 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 15:56:11,327 Saving new checkpoint.\n",
      "2020-03-04 15:56:11,488 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 88804.8047, ppl: 928.1946, duration: 3.1045s\n",
      "2020-03-04 15:56:20,730 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 87630.3594, ppl: 847.9921, duration: 4.6989s\n",
      "2020-03-04 15:56:29,385 Validation result (greedy) at epoch   1, step       90: bleu:   0.00, loss: 87156.3750, ppl: 817.6218, duration: 4.3433s\n",
      "2020-03-04 15:56:38,786 Validation result (greedy) at epoch   1, step      120: bleu:   0.00, loss: 86971.9531, ppl: 806.1012, duration: 4.8175s\n",
      "2020-03-04 15:56:39,396 Epoch   1: total training loss 14956.00\n",
      "2020-03-04 15:56:39,396 EPOCH 2\n",
      "2020-03-04 15:56:48,301 Validation result (greedy) at epoch   2, step      150: bleu:   0.00, loss: 86759.9297, ppl: 793.0567, duration: 5.3844s\n",
      "2020-03-04 15:56:56,509 Validation result (greedy) at epoch   2, step      180: bleu:   0.00, loss: 86694.3750, ppl: 789.0667, duration: 4.0966s\n",
      "2020-03-04 15:57:06,147 Validation result (greedy) at epoch   2, step      210: bleu:   0.00, loss: 86512.1797, ppl: 778.0815, duration: 4.5577s\n",
      "2020-03-04 15:57:15,703 Validation result (greedy) at epoch   2, step      240: bleu:   0.00, loss: 86228.2891, ppl: 761.2691, duration: 5.1113s\n",
      "2020-03-04 15:57:17,087 Epoch   2: total training loss 14531.94\n",
      "2020-03-04 15:57:17,087 EPOCH 3\n",
      "2020-03-04 15:57:25,447 Validation result (greedy) at epoch   3, step      270: bleu:   0.00, loss: 86418.1797, ppl: 772.4738, duration: 5.2454s\n",
      "2020-03-04 15:57:35,103 Validation result (greedy) at epoch   3, step      300: bleu:   0.00, loss: 86258.3906, ppl: 763.0342, duration: 5.0733s\n",
      "2020-03-04 15:57:46,171 Validation result (greedy) at epoch   3, step      330: bleu:   0.00, loss: 86340.3516, ppl: 767.8616, duration: 6.4300s\n",
      "2020-03-04 15:57:54,176 Validation result (greedy) at epoch   3, step      360: bleu:   0.00, loss: 85997.0703, ppl: 747.8445, duration: 4.2948s\n",
      "2020-03-04 15:57:55,788 Epoch   3: total training loss 14418.35\n",
      "2020-03-04 15:57:55,788 EPOCH 4\n",
      "2020-03-04 15:58:03,420 Validation result (greedy) at epoch   4, step      390: bleu:   0.00, loss: 85934.1328, ppl: 744.2317, duration: 5.8943s\n",
      "2020-03-04 15:58:12,288 Validation result (greedy) at epoch   4, step      420: bleu:   0.00, loss: 85812.6406, ppl: 737.3067, duration: 4.8482s\n",
      "2020-03-04 15:58:21,882 Validation result (greedy) at epoch   4, step      450: bleu:   0.00, loss: 85407.4766, ppl: 714.6750, duration: 5.0489s\n",
      "2020-03-04 15:58:31,359 Validation result (greedy) at epoch   4, step      480: bleu:   0.00, loss: 84403.7109, ppl: 661.5538, duration: 5.1214s\n",
      "2020-03-04 15:58:34,505 Epoch   4: total training loss 14233.85\n",
      "2020-03-04 15:58:34,505 EPOCH 5\n",
      "2020-03-04 15:58:40,117 Validation result (greedy) at epoch   5, step      510: bleu:   0.00, loss: 84110.4141, ppl: 646.7909, duration: 4.2718s\n",
      "2020-03-04 15:58:53,695 Validation result (greedy) at epoch   5, step      540: bleu:   0.00, loss: 82816.3047, ppl: 585.4880, duration: 8.8297s\n",
      "2020-03-04 15:59:07,043 Validation result (greedy) at epoch   5, step      570: bleu:   0.00, loss: 82396.6328, ppl: 566.8832, duration: 8.7920s\n",
      "2020-03-04 15:59:19,917 Validation result (greedy) at epoch   5, step      600: bleu:   0.00, loss: 81027.8047, ppl: 510.2121, duration: 9.3441s\n",
      "2020-03-04 15:59:22,828 Epoch   5: total training loss 13705.88\n",
      "2020-03-04 15:59:22,828 EPOCH 6\n",
      "2020-03-04 15:59:30,838 Validation result (greedy) at epoch   6, step      630: bleu:   0.00, loss: 80340.3750, ppl: 483.9255, duration: 7.4512s\n",
      "2020-03-04 15:59:43,614 Validation result (greedy) at epoch   6, step      660: bleu:   0.00, loss: 80367.3047, ppl: 484.9292, duration: 8.5589s\n",
      "2020-03-04 15:59:53,717 Validation result (greedy) at epoch   6, step      690: bleu:   0.00, loss: 79411.0859, ppl: 450.5301, duration: 6.2735s\n",
      "2020-03-04 16:00:03,653 Validation result (greedy) at epoch   6, step      720: bleu:   0.00, loss: 78348.1484, ppl: 415.1482, duration: 5.4673s\n",
      "2020-03-04 16:00:13,603 Validation result (greedy) at epoch   6, step      750: bleu:   0.00, loss: 77735.4219, ppl: 396.0293, duration: 5.4733s\n",
      "2020-03-04 16:00:13,604 Epoch   6: total training loss 13103.54\n",
      "2020-03-04 16:00:13,604 EPOCH 7\n",
      "2020-03-04 16:00:23,165 Validation result (greedy) at epoch   7, step      780: bleu:   0.00, loss: 77199.4531, ppl: 380.0288, duration: 5.4386s\n",
      "2020-03-04 16:00:35,160 Validation result (greedy) at epoch   7, step      810: bleu:   0.00, loss: 76802.8281, ppl: 368.6059, duration: 7.3089s\n",
      "2020-03-04 16:00:47,108 Validation result (greedy) at epoch   7, step      840: bleu:   0.00, loss: 75838.1328, ppl: 342.2351, duration: 7.7324s\n",
      "2020-03-04 16:01:00,271 Validation result (greedy) at epoch   7, step      870: bleu:   0.00, loss: 75417.2812, ppl: 331.3300, duration: 8.3793s\n",
      "2020-03-04 16:01:01,020 Epoch   7: total training loss 12475.18\n",
      "2020-03-04 16:01:01,021 EPOCH 8\n",
      "2020-03-04 16:01:12,547 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:01:12,547 Saving new checkpoint.\n",
      "2020-03-04 16:01:12,755 Validation result (greedy) at epoch   8, step      900: bleu:   0.19, loss: 75532.7656, ppl: 334.2873, duration: 8.4396s\n",
      "2020-03-04 16:01:23,957 Validation result (greedy) at epoch   8, step      930: bleu:   0.00, loss: 75008.7500, ppl: 321.0767, duration: 8.2847s\n",
      "2020-03-04 16:01:35,148 Validation result (greedy) at epoch   8, step      960: bleu:   0.00, loss: 74593.1562, ppl: 310.9715, duration: 8.5816s\n",
      "2020-03-04 16:01:46,878 Validation result (greedy) at epoch   8, step      990: bleu:   0.00, loss: 74116.4609, ppl: 299.7717, duration: 8.7114s\n",
      "2020-03-04 16:01:47,977 Epoch   8 Step:     1000 Batch Loss:    98.853020 Tokens per Sec:     8125, Lr: 0.000500\n",
      "2020-03-04 16:01:47,977 Epoch   8: total training loss 12025.30\n",
      "2020-03-04 16:01:47,977 EPOCH 9\n",
      "2020-03-04 16:01:57,959 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:01:57,959 Saving new checkpoint.\n",
      "2020-03-04 16:01:58,113 Validation result (greedy) at epoch   9, step     1020: bleu:   0.35, loss: 74146.3516, ppl: 300.4619, duration: 8.0115s\n",
      "2020-03-04 16:02:08,839 Validation result (greedy) at epoch   9, step     1050: bleu:   0.00, loss: 73881.8438, ppl: 294.4084, duration: 7.7562s\n",
      "2020-03-04 16:02:18,968 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:02:18,968 Saving new checkpoint.\n",
      "2020-03-04 16:02:19,133 Validation result (greedy) at epoch   9, step     1080: bleu:   0.40, loss: 73570.4609, ppl: 287.4382, duration: 7.5770s\n",
      "2020-03-04 16:02:29,618 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:02:29,618 Saving new checkpoint.\n",
      "2020-03-04 16:02:29,780 Validation result (greedy) at epoch   9, step     1110: bleu:   0.51, loss: 73222.0469, ppl: 279.8346, duration: 7.3667s\n",
      "2020-03-04 16:02:31,582 Epoch   9: total training loss 11668.01\n",
      "2020-03-04 16:02:31,583 EPOCH 10\n",
      "2020-03-04 16:02:41,486 Validation result (greedy) at epoch  10, step     1140: bleu:   0.43, loss: 73243.3906, ppl: 280.2945, duration: 7.9923s\n",
      "2020-03-04 16:02:52,226 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:02:52,226 Saving new checkpoint.\n",
      "2020-03-04 16:02:52,413 Validation result (greedy) at epoch  10, step     1170: bleu:   0.54, loss: 73098.7734, ppl: 277.1927, duration: 7.0511s\n",
      "2020-03-04 16:03:03,611 Validation result (greedy) at epoch  10, step     1200: bleu:   0.52, loss: 72926.7969, ppl: 273.5488, duration: 7.1489s\n",
      "2020-03-04 16:03:14,644 Validation result (greedy) at epoch  10, step     1230: bleu:   0.47, loss: 72650.1719, ppl: 267.7878, duration: 6.5795s\n",
      "2020-03-04 16:03:17,844 Epoch  10: total training loss 11358.99\n",
      "2020-03-04 16:03:17,845 EPOCH 11\n",
      "2020-03-04 16:03:25,937 Validation result (greedy) at epoch  11, step     1260: bleu:   0.48, loss: 72338.5391, ppl: 261.4428, duration: 6.5187s\n",
      "2020-03-04 16:03:36,301 Validation result (greedy) at epoch  11, step     1290: bleu:   0.52, loss: 72548.7891, ppl: 265.7068, duration: 6.0579s\n",
      "2020-03-04 16:03:45,936 Validation result (greedy) at epoch  11, step     1320: bleu:   0.48, loss: 72425.5938, ppl: 263.2000, duration: 5.9111s\n",
      "2020-03-04 16:03:56,523 Validation result (greedy) at epoch  11, step     1350: bleu:   0.46, loss: 71927.0547, ppl: 253.2946, duration: 6.8837s\n",
      "2020-03-04 16:03:59,701 Epoch  11: total training loss 11034.40\n",
      "2020-03-04 16:03:59,701 EPOCH 12\n",
      "2020-03-04 16:04:06,929 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:04:06,929 Saving new checkpoint.\n",
      "2020-03-04 16:04:07,161 Validation result (greedy) at epoch  12, step     1380: bleu:   0.57, loss: 71855.5078, ppl: 251.9040, duration: 6.8173s\n",
      "2020-03-04 16:04:18,217 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:04:18,217 Saving new checkpoint.\n",
      "2020-03-04 16:04:18,372 Validation result (greedy) at epoch  12, step     1410: bleu:   0.65, loss: 71935.9609, ppl: 253.4683, duration: 7.3503s\n",
      "2020-03-04 16:04:29,011 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:04:29,011 Saving new checkpoint.\n",
      "2020-03-04 16:04:29,193 Validation result (greedy) at epoch  12, step     1440: bleu:   0.80, loss: 71849.1406, ppl: 251.7805, duration: 6.2974s\n",
      "2020-03-04 16:04:39,531 Validation result (greedy) at epoch  12, step     1470: bleu:   0.71, loss: 71584.1328, ppl: 246.6983, duration: 6.0659s\n",
      "2020-03-04 16:04:48,792 Validation result (greedy) at epoch  12, step     1500: bleu:   0.69, loss: 71325.8047, ppl: 241.8430, duration: 5.8105s\n",
      "2020-03-04 16:04:48,793 Epoch  12: total training loss 10731.01\n",
      "2020-03-04 16:04:48,793 EPOCH 13\n",
      "2020-03-04 16:04:56,990 Validation result (greedy) at epoch  13, step     1530: bleu:   0.00, loss: 71471.3672, ppl: 244.5670, duration: 5.5664s\n",
      "2020-03-04 16:05:05,315 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:05:05,315 Saving new checkpoint.\n",
      "2020-03-04 16:05:05,560 Validation result (greedy) at epoch  13, step     1560: bleu:   0.90, loss: 71347.9844, ppl: 242.2561, duration: 6.3202s\n",
      "2020-03-04 16:05:15,929 Validation result (greedy) at epoch  13, step     1590: bleu:   0.71, loss: 71250.3125, ppl: 240.4423, duration: 5.8172s\n",
      "2020-03-04 16:05:25,987 Validation result (greedy) at epoch  13, step     1620: bleu:   0.86, loss: 71199.5469, ppl: 239.5049, duration: 5.6593s\n",
      "2020-03-04 16:05:26,682 Epoch  13: total training loss 10443.54\n",
      "2020-03-04 16:05:26,683 EPOCH 14\n",
      "2020-03-04 16:05:36,705 Validation result (greedy) at epoch  14, step     1650: bleu:   0.77, loss: 71136.0547, ppl: 238.3376, duration: 6.5406s\n",
      "2020-03-04 16:05:48,114 Validation result (greedy) at epoch  14, step     1680: bleu:   0.55, loss: 71214.2656, ppl: 239.7763, duration: 7.1537s\n",
      "2020-03-04 16:05:58,436 Validation result (greedy) at epoch  14, step     1710: bleu:   0.64, loss: 71089.7578, ppl: 237.4902, duration: 6.0334s\n",
      "2020-03-04 16:06:08,691 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:06:08,691 Saving new checkpoint.\n",
      "2020-03-04 16:06:08,854 Validation result (greedy) at epoch  14, step     1740: bleu:   1.01, loss: 70878.7344, ppl: 233.6650, duration: 6.0188s\n",
      "2020-03-04 16:06:10,205 Epoch  14: total training loss 10151.32\n",
      "2020-03-04 16:06:10,206 EPOCH 15\n",
      "2020-03-04 16:06:18,746 Validation result (greedy) at epoch  15, step     1770: bleu:   0.64, loss: 70930.5000, ppl: 234.5975, duration: 5.8359s\n",
      "2020-03-04 16:06:29,977 Validation result (greedy) at epoch  15, step     1800: bleu:   0.84, loss: 71010.1406, ppl: 236.0396, duration: 6.7669s\n",
      "2020-03-04 16:06:40,049 Validation result (greedy) at epoch  15, step     1830: bleu:   0.92, loss: 70892.8281, ppl: 233.9185, duration: 5.7561s\n",
      "2020-03-04 16:06:50,292 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:06:50,293 Saving new checkpoint.\n",
      "2020-03-04 16:06:50,455 Validation result (greedy) at epoch  15, step     1860: bleu:   1.07, loss: 70710.7500, ppl: 230.6640, duration: 6.4894s\n",
      "2020-03-04 16:06:52,496 Epoch  15: total training loss 9864.78\n",
      "2020-03-04 16:06:52,496 Training ended after  15 epochs.\n",
      "2020-03-04 16:06:52,496 Best validation result (greedy) at step     1860:   1.07 eval_metric.\n",
      "2020-03-04 16:06:59,984  dev bleu:   1.30 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-03-04 16:06:59,985 Translations saved to: results/rnn/translate/shp-es_Religioso_300_512/bpe_8000/00001860.hyps.dev\n",
      "2020-03-04 16:07:06,643 test bleu:   1.39 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-03-04 16:07:06,644 Translations saved to: results/rnn/translate/shp-es_Religioso_300_512/bpe_8000/00001860.hyps.test\n"
     ]
    }
   ],
   "source": [
    "base_path = 'data/translate/preprocessed'\n",
    "datas = os.listdir(base_path)\n",
    "emb_size = 300\n",
    "hidden_size = 512\n",
    "for data in ['Religioso']:\n",
    "    data_path = os.path.join(base_path, data)\n",
    "    for lang_in, lang_out in [['shp', 'es']]:\n",
    "        segmentations = os.listdir(data_path)\n",
    "        for segment in segmentations:\n",
    "            if segment == 'bpe_8000':\n",
    "                segment_path = os.path.join(data_path, segment)\n",
    "                print(os.path.join('results/translate', data, segment))\n",
    "                if 'bpe_drop' in segment:\n",
    "                    level = 'bpe'\n",
    "                elif 'bpe' in segment:\n",
    "                    level = 'bpe'\n",
    "                elif 'char' in segment:\n",
    "                    level = 'char'\n",
    "                elif 'word' in segment:\n",
    "                    level = 'word'\n",
    "                elif 'syl' in segment:\n",
    "                    level = 'syl'\n",
    "                else:\n",
    "                    level = None\n",
    "\n",
    "                val_freq = 30\n",
    "                f_config = config.format(lang_src=lang_in, lang_tgt=lang_out, \n",
    "                                         train_path=os.path.join(segment_path, 'train'),\n",
    "                                         test_path=os.path.join(segment_path, 'test'),\n",
    "                                         dev_path=os.path.join(segment_path, 'valid'),\n",
    "                                         level=level,\n",
    "                                         emb_size=emb_size,\n",
    "                                         hidden_size=hidden_size,\n",
    "                                         val_freq=val_freq,\n",
    "                                         model_dir=os.path.join('results/rnn/translate',\\\n",
    "                                                                f'{lang_in}-{lang_out}_{data}_{emb_size}_{hidden_size}', segment))\n",
    "\n",
    "                with open(\"joeynmt/configs/transformer_test_translate_2.yaml\",'w') as f:\n",
    "                    f.write(f_config)\n",
    "\n",
    "                !python3 joeynmt/joeynmt train \"joeynmt/configs/transformer_test_translate_2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/translate/Religioso/bpe_drop_5000\n",
      "2020-03-04 16:07:09,203 Hello! This is Joey-NMT.\n",
      "2020-03-04 16:07:09.905640: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-04 16:07:09.905706: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-04 16:07:09.905718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2020-03-04 16:07:10,558 Total params: 15524156\n",
      "2020-03-04 16:07:10,559 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-03-04 16:07:12,551 cfg.name                           : my_experiment\n",
      "2020-03-04 16:07:12,551 cfg.data.src                       : shp\n",
      "2020-03-04 16:07:12,551 cfg.data.trg                       : es\n",
      "2020-03-04 16:07:12,551 cfg.data.train                     : data/translate/preprocessed/Religioso/bpe_drop_5000/train\n",
      "2020-03-04 16:07:12,552 cfg.data.dev                       : data/translate/preprocessed/Religioso/bpe_drop_5000/valid\n",
      "2020-03-04 16:07:12,552 cfg.data.test                      : data/translate/preprocessed/Religioso/bpe_drop_5000/test\n",
      "2020-03-04 16:07:12,552 cfg.data.level                     : bpe\n",
      "2020-03-04 16:07:12,552 cfg.data.lowercase                 : True\n",
      "2020-03-04 16:07:12,552 cfg.data.max_sent_length           : 130\n",
      "2020-03-04 16:07:12,552 cfg.data.src_voc_min_freq          : 1\n",
      "2020-03-04 16:07:12,552 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-03-04 16:07:12,552 cfg.testing.beam_size              : 5\n",
      "2020-03-04 16:07:12,552 cfg.testing.alpha                  : 1.0\n",
      "2020-03-04 16:07:12,552 cfg.training.reset_best_ckpt       : False\n",
      "2020-03-04 16:07:12,552 cfg.training.reset_scheduler       : False\n",
      "2020-03-04 16:07:12,552 cfg.training.reset_optimizer       : False\n",
      "2020-03-04 16:07:12,552 cfg.training.random_seed           : 42\n",
      "2020-03-04 16:07:12,552 cfg.training.optimizer             : adam\n",
      "2020-03-04 16:07:12,552 cfg.training.learning_rate         : 0.0005\n",
      "2020-03-04 16:07:12,552 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-03-04 16:07:12,552 cfg.training.clip_grad_val         : 1.0\n",
      "2020-03-04 16:07:12,552 cfg.training.weight_decay          : 0.0\n",
      "2020-03-04 16:07:12,552 cfg.training.batch_size            : 48\n",
      "2020-03-04 16:07:12,552 cfg.training.batch_type            : sentence\n",
      "2020-03-04 16:07:12,552 cfg.training.eval_batch_size       : 10\n",
      "2020-03-04 16:07:12,552 cfg.training.eval_batch_type       : sentence\n",
      "2020-03-04 16:07:12,552 cfg.training.batch_multiplier      : 1\n",
      "2020-03-04 16:07:12,552 cfg.training.scheduling            : plateau\n",
      "2020-03-04 16:07:12,552 cfg.training.patience              : 500\n",
      "2020-03-04 16:07:12,552 cfg.training.decrease_factor       : 0.5\n",
      "2020-03-04 16:07:12,552 cfg.training.epochs                : 15\n",
      "2020-03-04 16:07:12,552 cfg.training.validation_freq       : 30\n",
      "2020-03-04 16:07:12,552 cfg.training.logging_freq          : 1000\n",
      "2020-03-04 16:07:12,553 cfg.training.eval_metric           : bleu\n",
      "2020-03-04 16:07:12,553 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-03-04 16:07:12,553 cfg.training.model_dir             : results/rnn/translate/shp-es_Religioso_300_512/bpe_drop_5000\n",
      "2020-03-04 16:07:12,553 cfg.training.overwrite             : True\n",
      "2020-03-04 16:07:12,553 cfg.training.shuffle               : True\n",
      "2020-03-04 16:07:12,553 cfg.training.use_cuda              : True\n",
      "2020-03-04 16:07:12,553 cfg.training.max_output_length     : 60\n",
      "2020-03-04 16:07:12,553 cfg.training.print_valid_sents     : []\n",
      "2020-03-04 16:07:12,553 cfg.training.keep_last_ckpts       : 3\n",
      "2020-03-04 16:07:12,553 cfg.training.label_smoothing       : 0.0\n",
      "2020-03-04 16:07:12,553 cfg.model.initializer              : xavier\n",
      "2020-03-04 16:07:12,553 cfg.model.init_weight              : 0.01\n",
      "2020-03-04 16:07:12,553 cfg.model.init_gain                : 1.0\n",
      "2020-03-04 16:07:12,553 cfg.model.bias_initializer         : zeros\n",
      "2020-03-04 16:07:12,553 cfg.model.embed_initializer        : normal\n",
      "2020-03-04 16:07:12,553 cfg.model.embed_init_weight        : 0.1\n",
      "2020-03-04 16:07:12,553 cfg.model.embed_init_gain          : 1.0\n",
      "2020-03-04 16:07:12,553 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-03-04 16:07:12,553 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-03-04 16:07:12,553 cfg.model.tied_embeddings          : False\n",
      "2020-03-04 16:07:12,553 cfg.model.tied_softmax             : False\n",
      "2020-03-04 16:07:12,553 cfg.model.encoder.type             : recurrent\n",
      "2020-03-04 16:07:12,553 cfg.model.encoder.rnn_type         : gru\n",
      "2020-03-04 16:07:12,553 cfg.model.encoder.embeddings.embedding_dim : 300\n",
      "2020-03-04 16:07:12,553 cfg.model.encoder.embeddings.scale : False\n",
      "2020-03-04 16:07:12,553 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-03-04 16:07:12,553 cfg.model.encoder.hidden_size      : 512\n",
      "2020-03-04 16:07:12,554 cfg.model.encoder.bidirectional    : True\n",
      "2020-03-04 16:07:12,554 cfg.model.encoder.dropout          : 0.3\n",
      "2020-03-04 16:07:12,554 cfg.model.encoder.num_layers       : 2\n",
      "2020-03-04 16:07:12,554 cfg.model.encoder.freeze           : False\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.type             : recurrent\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.rnn_type         : gru\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.embeddings.embedding_dim : 300\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.embeddings.scale : False\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.hidden_size      : 512\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.dropout          : 0.3\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.num_layers       : 2\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.input_feeding    : True\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.init_hidden      : last\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.attention        : bahdanau\n",
      "2020-03-04 16:07:12,554 cfg.model.decoder.freeze           : False\n",
      "2020-03-04 16:07:12,554 Data set sizes: \n",
      "\ttrain 5996,\n",
      "\tvalid 749,\n",
      "\ttest 749\n",
      "2020-03-04 16:07:12,554 First training example:\n",
      "\t[SRC] pero la palabra del señor permane@@ ce eter@@ nam@@ en@@ te est@@ a palabra es el evangelio que se les ha anunci@@ ado a ustedes\n",
      "\t[TRG] ikaxbi ja non ibon jo@@ ira jawetianbi ke@@ yó@@ yamai iki\n",
      "2020-03-04 16:07:12,554 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a (5) y (6) de (7) que (8) la (9) el\n",
      "2020-03-04 16:07:12,554 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) iki (5) ja (6) i@@ (7) a (8) i (9) jato\n",
      "2020-03-04 16:07:12,555 Number of Src words (types): 2874\n",
      "2020-03-04 16:07:12,555 Number of Trg words (types): 2771\n",
      "2020-03-04 16:07:12,555 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(300, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(812, 512, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=300, vocab_size=2874),\n",
      "\ttrg_embed=Embeddings(embedding_dim=300, vocab_size=2771))\n",
      "2020-03-04 16:07:12,557 EPOCH 1\n",
      "2020-03-04 16:07:20,473 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:07:20,473 Saving new checkpoint.\n",
      "2020-03-04 16:07:20,650 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 115100.0547, ppl: 737.9965, duration: 3.7870s\n",
      "2020-03-04 16:07:31,603 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 114109.2578, ppl: 697.2134, duration: 7.5389s\n",
      "2020-03-04 16:07:41,371 Validation result (greedy) at epoch   1, step       90: bleu:   0.00, loss: 113551.0078, ppl: 675.2354, duration: 6.2350s\n",
      "2020-03-04 16:07:51,492 Validation result (greedy) at epoch   1, step      120: bleu:   0.00, loss: 113276.2812, ppl: 664.6752, duration: 6.6239s\n",
      "2020-03-04 16:07:52,275 Epoch   1: total training loss 19303.02\n",
      "2020-03-04 16:07:52,275 EPOCH 2\n",
      "2020-03-04 16:08:02,796 Validation result (greedy) at epoch   2, step      150: bleu:   0.00, loss: 113492.4766, ppl: 672.9716, duration: 7.0573s\n",
      "2020-03-04 16:08:13,291 Validation result (greedy) at epoch   2, step      180: bleu:   0.00, loss: 113258.3281, ppl: 663.9910, duration: 6.9576s\n",
      "2020-03-04 16:08:22,127 Validation result (greedy) at epoch   2, step      210: bleu:   0.00, loss: 113081.0781, ppl: 657.2728, duration: 5.1461s\n",
      "2020-03-04 16:08:32,727 Validation result (greedy) at epoch   2, step      240: bleu:   0.00, loss: 112955.2500, ppl: 652.5445, duration: 6.0602s\n",
      "2020-03-04 16:08:33,702 Epoch   2: total training loss 18866.19\n",
      "2020-03-04 16:08:33,702 EPOCH 3\n",
      "2020-03-04 16:08:42,391 Validation result (greedy) at epoch   3, step      270: bleu:   0.00, loss: 112792.6562, ppl: 646.4853, duration: 6.4696s\n",
      "2020-03-04 16:08:51,825 Validation result (greedy) at epoch   3, step      300: bleu:   0.00, loss: 112990.3438, ppl: 653.8599, duration: 6.1014s\n",
      "2020-03-04 16:09:02,620 Validation result (greedy) at epoch   3, step      330: bleu:   0.00, loss: 112817.8047, ppl: 647.4188, duration: 6.0040s\n",
      "2020-03-04 16:09:12,063 Validation result (greedy) at epoch   3, step      360: bleu:   0.00, loss: 112664.4531, ppl: 641.7474, duration: 6.2777s\n",
      "2020-03-04 16:09:13,716 Epoch   3: total training loss 18780.33\n",
      "2020-03-04 16:09:13,717 EPOCH 4\n",
      "2020-03-04 16:09:24,030 Validation result (greedy) at epoch   4, step      390: bleu:   0.00, loss: 112868.6562, ppl: 649.3106, duration: 8.5288s\n",
      "2020-03-04 16:09:34,708 Validation result (greedy) at epoch   4, step      420: bleu:   0.00, loss: 112406.8594, ppl: 632.3324, duration: 6.7553s\n",
      "2020-03-04 16:09:45,430 Validation result (greedy) at epoch   4, step      450: bleu:   0.00, loss: 111382.1562, ppl: 596.2274, duration: 6.5518s\n",
      "2020-03-04 16:09:55,022 Validation result (greedy) at epoch   4, step      480: bleu:   0.00, loss: 109198.5312, ppl: 526.0178, duration: 5.6847s\n",
      "2020-03-04 16:09:57,471 Epoch   4: total training loss 18481.29\n",
      "2020-03-04 16:09:57,471 EPOCH 5\n",
      "2020-03-04 16:10:04,072 Validation result (greedy) at epoch   5, step      510: bleu:   0.00, loss: 106684.6406, ppl: 455.3649, duration: 5.6681s\n",
      "2020-03-04 16:10:14,490 Validation result (greedy) at epoch   5, step      540: bleu:   0.00, loss: 104111.0469, ppl: 392.8539, duration: 5.8227s\n",
      "2020-03-04 16:10:23,722 Validation result (greedy) at epoch   5, step      570: bleu:   0.00, loss: 103161.9297, ppl: 372.0326, duration: 5.5668s\n",
      "2020-03-04 16:10:31,897 Validation result (greedy) at epoch   5, step      600: bleu:   0.00, loss: 102187.7109, ppl: 351.8077, duration: 5.1315s\n",
      "2020-03-04 16:10:35,660 Epoch   5: total training loss 17231.74\n",
      "2020-03-04 16:10:35,660 EPOCH 6\n",
      "2020-03-04 16:10:42,300 Validation result (greedy) at epoch   6, step      630: bleu:   0.00, loss: 101504.6562, ppl: 338.2870, duration: 5.7728s\n",
      "2020-03-04 16:10:51,783 Validation result (greedy) at epoch   6, step      660: bleu:   0.00, loss: 101714.0078, ppl: 342.3748, duration: 6.0536s\n",
      "2020-03-04 16:11:00,765 Validation result (greedy) at epoch   6, step      690: bleu:   0.00, loss: 100626.3984, ppl: 321.6628, duration: 5.4195s\n",
      "2020-03-04 16:11:10,969 Validation result (greedy) at epoch   6, step      720: bleu:   0.00, loss: 99827.0391, ppl: 307.2433, duration: 6.2450s\n",
      "2020-03-04 16:11:20,822 Validation result (greedy) at epoch   6, step      750: bleu:   0.00, loss: 99321.9688, ppl: 298.4676, duration: 6.4941s\n",
      "2020-03-04 16:11:20,823 Epoch   6: total training loss 16638.41\n",
      "2020-03-04 16:11:20,823 EPOCH 7\n",
      "2020-03-04 16:11:30,707 Validation result (greedy) at epoch   7, step      780: bleu:   0.00, loss: 99093.8281, ppl: 294.5862, duration: 6.3014s\n",
      "2020-03-04 16:11:42,500 Validation result (greedy) at epoch   7, step      810: bleu:   0.00, loss: 98724.3672, ppl: 288.4072, duration: 7.3784s\n",
      "2020-03-04 16:11:54,467 Validation result (greedy) at epoch   7, step      840: bleu:   0.00, loss: 97991.0625, ppl: 276.5246, duration: 8.4344s\n",
      "2020-03-04 16:12:04,867 Validation result (greedy) at epoch   7, step      870: bleu:   0.00, loss: 97839.0234, ppl: 274.1229, duration: 6.7175s\n",
      "2020-03-04 16:12:05,486 Epoch   7: total training loss 16163.91\n",
      "2020-03-04 16:12:05,486 EPOCH 8\n",
      "2020-03-04 16:12:16,240 Validation result (greedy) at epoch   8, step      900: bleu:   0.00, loss: 97847.5781, ppl: 274.2574, duration: 8.1297s\n",
      "2020-03-04 16:12:28,968 Validation result (greedy) at epoch   8, step      930: bleu:   0.00, loss: 96953.1641, ppl: 260.5382, duration: 8.2140s\n",
      "2020-03-04 16:12:40,354 Validation result (greedy) at epoch   8, step      960: bleu:   0.00, loss: 96830.2109, ppl: 258.7067, duration: 7.0415s\n",
      "2020-03-04 16:12:51,608 Validation result (greedy) at epoch   8, step      990: bleu:   0.00, loss: 96289.0781, ppl: 250.7979, duration: 6.5595s\n",
      "2020-03-04 16:12:52,615 Epoch   8 Step:     1000 Batch Loss:   132.938171 Tokens per Sec:     8150, Lr: 0.000500\n",
      "2020-03-04 16:12:52,616 Epoch   8: total training loss 15785.79\n",
      "2020-03-04 16:12:52,616 EPOCH 9\n",
      "2020-03-04 16:13:05,123 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:13:05,123 Saving new checkpoint.\n",
      "2020-03-04 16:13:05,437 Validation result (greedy) at epoch   9, step     1020: bleu:   0.29, loss: 95997.0938, ppl: 246.6313, duration: 9.2276s\n",
      "2020-03-04 16:13:18,015 Validation result (greedy) at epoch   9, step     1050: bleu:   0.00, loss: 95792.5078, ppl: 243.7532, duration: 8.1754s\n",
      "2020-03-04 16:13:30,181 Validation result (greedy) at epoch   9, step     1080: bleu:   0.00, loss: 95475.4453, ppl: 239.3590, duration: 8.2004s\n",
      "2020-03-04 16:13:41,658 Validation result (greedy) at epoch   9, step     1110: bleu:   0.00, loss: 95194.5312, ppl: 235.5320, duration: 7.2320s\n",
      "2020-03-04 16:13:43,452 Epoch   9: total training loss 15420.45\n",
      "2020-03-04 16:13:43,452 EPOCH 10\n",
      "2020-03-04 16:13:52,812 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:13:52,812 Saving new checkpoint.\n",
      "2020-03-04 16:13:52,945 Validation result (greedy) at epoch  10, step     1140: bleu:   0.39, loss: 94927.3984, ppl: 231.9495, duration: 7.2272s\n",
      "2020-03-04 16:14:03,016 Validation result (greedy) at epoch  10, step     1170: bleu:   0.38, loss: 94779.8906, ppl: 229.9948, duration: 6.4922s\n",
      "2020-03-04 16:14:12,615 Validation result (greedy) at epoch  10, step     1200: bleu:   0.00, loss: 94483.6172, ppl: 226.1181, duration: 6.6514s\n",
      "2020-03-04 16:14:22,508 Validation result (greedy) at epoch  10, step     1230: bleu:   0.00, loss: 94081.2344, ppl: 220.9576, duration: 6.9757s\n",
      "2020-03-04 16:14:25,371 Epoch  10: total training loss 15090.24\n",
      "2020-03-04 16:14:25,371 EPOCH 11\n",
      "2020-03-04 16:14:34,478 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:14:34,478 Saving new checkpoint.\n",
      "2020-03-04 16:14:34,662 Validation result (greedy) at epoch  11, step     1260: bleu:   0.42, loss: 94023.9609, ppl: 220.2327, duration: 8.0214s\n",
      "2020-03-04 16:14:46,136 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:14:46,136 Saving new checkpoint.\n",
      "2020-03-04 16:14:46,286 Validation result (greedy) at epoch  11, step     1290: bleu:   0.43, loss: 94002.4531, ppl: 219.9611, duration: 7.2367s\n",
      "2020-03-04 16:14:56,213 Validation result (greedy) at epoch  11, step     1320: bleu:   0.00, loss: 93966.1953, ppl: 219.5040, duration: 6.5721s\n",
      "2020-03-04 16:15:07,911 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:15:07,911 Saving new checkpoint.\n",
      "2020-03-04 16:15:08,063 Validation result (greedy) at epoch  11, step     1350: bleu:   0.57, loss: 93331.9531, ppl: 211.6598, duration: 7.5520s\n",
      "2020-03-04 16:15:11,105 Epoch  11: total training loss 14772.02\n",
      "2020-03-04 16:15:11,106 EPOCH 12\n",
      "2020-03-04 16:15:19,026 Validation result (greedy) at epoch  12, step     1380: bleu:   0.00, loss: 93181.0703, ppl: 209.8354, duration: 7.4154s\n",
      "2020-03-04 16:15:29,011 Validation result (greedy) at epoch  12, step     1410: bleu:   0.00, loss: 93067.6484, ppl: 208.4743, duration: 6.4890s\n",
      "2020-03-04 16:15:39,880 Validation result (greedy) at epoch  12, step     1440: bleu:   0.00, loss: 93045.1484, ppl: 208.2053, duration: 6.9603s\n",
      "2020-03-04 16:15:50,162 Validation result (greedy) at epoch  12, step     1470: bleu:   0.00, loss: 92628.5625, ppl: 203.2878, duration: 6.6951s\n",
      "2020-03-04 16:16:00,785 Validation result (greedy) at epoch  12, step     1500: bleu:   0.00, loss: 92504.5078, ppl: 201.8460, duration: 6.6821s\n",
      "2020-03-04 16:16:00,785 Epoch  12: total training loss 14461.40\n",
      "2020-03-04 16:16:00,785 EPOCH 13\n",
      "2020-03-04 16:16:11,273 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:16:11,273 Saving new checkpoint.\n",
      "2020-03-04 16:16:11,424 Validation result (greedy) at epoch  13, step     1530: bleu:   0.68, loss: 92574.1328, ppl: 202.6540, duration: 6.7366s\n",
      "2020-03-04 16:16:21,887 Validation result (greedy) at epoch  13, step     1560: bleu:   0.64, loss: 92468.4844, ppl: 201.4292, duration: 6.5934s\n",
      "2020-03-04 16:16:32,810 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:16:32,810 Saving new checkpoint.\n",
      "2020-03-04 16:16:32,952 Validation result (greedy) at epoch  13, step     1590: bleu:   0.91, loss: 92282.5859, ppl: 199.2922, duration: 7.0484s\n",
      "2020-03-04 16:16:42,269 Validation result (greedy) at epoch  13, step     1620: bleu:   0.70, loss: 92426.4453, ppl: 200.9440, duration: 5.3911s\n",
      "2020-03-04 16:16:43,062 Epoch  13: total training loss 14144.05\n",
      "2020-03-04 16:16:43,063 EPOCH 14\n",
      "2020-03-04 16:16:53,311 Validation result (greedy) at epoch  14, step     1650: bleu:   0.52, loss: 92051.7109, ppl: 196.6697, duration: 6.7159s\n",
      "2020-03-04 16:17:04,561 Validation result (greedy) at epoch  14, step     1680: bleu:   0.47, loss: 92170.0078, ppl: 198.0091, duration: 7.4842s\n",
      "2020-03-04 16:17:14,850 Validation result (greedy) at epoch  14, step     1710: bleu:   0.78, loss: 91786.5000, ppl: 193.6996, duration: 6.3214s\n",
      "2020-03-04 16:17:25,590 Validation result (greedy) at epoch  14, step     1740: bleu:   0.49, loss: 91435.1328, ppl: 189.8338, duration: 6.5748s\n",
      "2020-03-04 16:17:26,513 Epoch  14: total training loss 13847.14\n",
      "2020-03-04 16:17:26,513 EPOCH 15\n",
      "2020-03-04 16:17:35,953 Validation result (greedy) at epoch  15, step     1770: bleu:   0.00, loss: 91609.3203, ppl: 191.7405, duration: 6.9953s\n",
      "2020-03-04 16:17:46,705 Validation result (greedy) at epoch  15, step     1800: bleu:   0.00, loss: 91502.0625, ppl: 190.5642, duration: 7.5277s\n",
      "2020-03-04 16:17:56,639 Validation result (greedy) at epoch  15, step     1830: bleu:   0.71, loss: 91329.4062, ppl: 188.6857, duration: 6.5737s\n",
      "2020-03-04 16:18:07,106 Validation result (greedy) at epoch  15, step     1860: bleu:   0.76, loss: 91272.5625, ppl: 188.0713, duration: 6.6039s\n",
      "2020-03-04 16:18:09,226 Epoch  15: total training loss 13522.02\n",
      "2020-03-04 16:18:09,226 Training ended after  15 epochs.\n",
      "2020-03-04 16:18:09,226 Best validation result (greedy) at step     1590:   0.91 eval_metric.\n",
      "2020-03-04 16:18:18,395  dev bleu:   0.87 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-03-04 16:18:18,395 Translations saved to: results/rnn/translate/shp-es_Religioso_300_512/bpe_drop_5000/00001590.hyps.dev\n",
      "2020-03-04 16:18:26,964 test bleu:   0.99 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-03-04 16:18:26,964 Translations saved to: results/rnn/translate/shp-es_Religioso_300_512/bpe_drop_5000/00001590.hyps.test\n"
     ]
    }
   ],
   "source": [
    "base_path = 'data/translate/preprocessed'\n",
    "datas = os.listdir(base_path)\n",
    "emb_size = 300\n",
    "hidden_size = 512\n",
    "for data in ['Religioso']:\n",
    "    data_path = os.path.join(base_path, data)\n",
    "    for lang_in, lang_out in [['shp', 'es']]:\n",
    "        segmentations = os.listdir(data_path)\n",
    "        for segment in segmentations:\n",
    "            if segment == 'bpe_drop_5000':\n",
    "                segment_path = os.path.join(data_path, segment)\n",
    "                print(os.path.join('results/translate', data, segment))\n",
    "                if 'bpe_drop' in segment:\n",
    "                    level = 'bpe'\n",
    "                elif 'bpe' in segment:\n",
    "                    level = 'bpe'\n",
    "                elif 'char' in segment:\n",
    "                    level = 'char'\n",
    "                elif 'word' in segment:\n",
    "                    level = 'word'\n",
    "                elif 'syl' in segment:\n",
    "                    level = 'syl'\n",
    "                else:\n",
    "                    level = None\n",
    "\n",
    "                val_freq = 30\n",
    "                f_config = config.format(lang_src=lang_in, lang_tgt=lang_out, \n",
    "                                         train_path=os.path.join(segment_path, 'train'),\n",
    "                                         test_path=os.path.join(segment_path, 'test'),\n",
    "                                         dev_path=os.path.join(segment_path, 'valid'),\n",
    "                                         level=level,\n",
    "                                         emb_size=emb_size,\n",
    "                                         hidden_size=hidden_size,\n",
    "                                         val_freq=val_freq,\n",
    "                                         model_dir=os.path.join('results/rnn/translate',\\\n",
    "                                                                f'{lang_in}-{lang_out}_{data}_{emb_size}_{hidden_size}', segment))\n",
    "\n",
    "                with open(\"joeynmt/configs/transformer_test_translate_2.yaml\",'w') as f:\n",
    "                    f.write(f_config)\n",
    "\n",
    "                !python3 joeynmt/joeynmt train \"joeynmt/configs/transformer_test_translate_2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
