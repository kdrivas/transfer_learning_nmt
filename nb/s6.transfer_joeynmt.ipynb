{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "pretrained_data: # specify your data here\n",
    "    src: {pretrained_lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {pretrained_lang_tgt}                       # trg language\n",
    "    train: {pretrained_train_path}     # training data\n",
    "    dev: {pretrained_dev_path}         # development data for validation\n",
    "    test: {pretrained_test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0002          # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 30                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: True                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "pretraining:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0004           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.00001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 45                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: True                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"joeynmt/configs/sample_{name}.yaml\".format(name=\"transfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-02 00:11:34,601 Hello! This is Joey-NMT.\n",
      "2020-03-02 00:11:35.195984: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-02 00:11:35.196041: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-02 00:11:35.196051: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2020-03-02 00:11:35,781 Total params: 14029756\n",
      "2020-03-02 00:11:35,781 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-03-02 00:11:37,641 cfg.name                           : my_experiment\n",
      "2020-03-02 00:11:37,641 cfg.data.src                       : en\n",
      "2020-03-02 00:11:37,641 cfg.data.trg                       : shp\n",
      "2020-03-02 00:11:37,641 cfg.data.train                     : data/transfer/preprocessed/splits.en/en-shp/char/train\n",
      "2020-03-02 00:11:37,641 cfg.data.dev                       : data/transfer/preprocessed/splits.en/en-shp/char/valid\n",
      "2020-03-02 00:11:37,641 cfg.data.test                      : data/transfer/preprocessed/splits.en/en-shp/char/test\n",
      "2020-03-02 00:11:37,641 cfg.data.level                     : char\n",
      "2020-03-02 00:11:37,641 cfg.data.lowercase                 : True\n",
      "2020-03-02 00:11:37,641 cfg.data.max_sent_length           : 150\n",
      "2020-03-02 00:11:37,641 cfg.data.src_voc_min_freq          : 1\n",
      "2020-03-02 00:11:37,641 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-03-02 00:11:37,641 cfg.pretrained_data.src            : en\n",
      "2020-03-02 00:11:37,641 cfg.pretrained_data.trg            : yue\n",
      "2020-03-02 00:11:37,642 cfg.pretrained_data.train          : data/transfer/preprocessed/splits.en/en-yue/char/train\n",
      "2020-03-02 00:11:37,642 cfg.pretrained_data.dev            : data/transfer/preprocessed/splits.en/en-yue/char/valid\n",
      "2020-03-02 00:11:37,642 cfg.pretrained_data.test           : data/transfer/preprocessed/splits.en/en-yue/char/test\n",
      "2020-03-02 00:11:37,642 cfg.pretrained_data.level          : char\n",
      "2020-03-02 00:11:37,642 cfg.pretrained_data.lowercase      : True\n",
      "2020-03-02 00:11:37,642 cfg.pretrained_data.max_sent_length : 150\n",
      "2020-03-02 00:11:37,642 cfg.pretrained_data.src_voc_min_freq : 1\n",
      "2020-03-02 00:11:37,642 cfg.pretrained_data.trg_voc_min_freq : 1\n",
      "2020-03-02 00:11:37,642 cfg.testing.beam_size              : 5\n",
      "2020-03-02 00:11:37,642 cfg.testing.alpha                  : 1.0\n",
      "2020-03-02 00:11:37,642 cfg.training.reset_best_ckpt       : False\n",
      "2020-03-02 00:11:37,642 cfg.training.reset_scheduler       : False\n",
      "2020-03-02 00:11:37,642 cfg.training.reset_optimizer       : False\n",
      "2020-03-02 00:11:37,642 cfg.training.random_seed           : 42\n",
      "2020-03-02 00:11:37,642 cfg.training.optimizer             : adam\n",
      "2020-03-02 00:11:37,642 cfg.training.learning_rate         : 0.0002\n",
      "2020-03-02 00:11:37,642 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-03-02 00:11:37,642 cfg.training.clip_grad_val         : 1.0\n",
      "2020-03-02 00:11:37,642 cfg.training.weight_decay          : 0.0\n",
      "2020-03-02 00:11:37,642 cfg.training.batch_size            : 48\n",
      "2020-03-02 00:11:37,642 cfg.training.batch_type            : sentence\n",
      "2020-03-02 00:11:37,642 cfg.training.eval_batch_size       : 10\n",
      "2020-03-02 00:11:37,642 cfg.training.eval_batch_type       : sentence\n",
      "2020-03-02 00:11:37,642 cfg.training.batch_multiplier      : 1\n",
      "2020-03-02 00:11:37,642 cfg.training.scheduling            : plateau\n",
      "2020-03-02 00:11:37,642 cfg.training.patience              : 600\n",
      "2020-03-02 00:11:37,642 cfg.training.decrease_factor       : 0.5\n",
      "2020-03-02 00:11:37,642 cfg.training.epochs                : 30\n",
      "2020-03-02 00:11:37,643 cfg.training.validation_freq       : 10\n",
      "2020-03-02 00:11:37,643 cfg.training.logging_freq          : 1000\n",
      "2020-03-02 00:11:37,643 cfg.training.eval_metric           : bleu\n",
      "2020-03-02 00:11:37,643 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-03-02 00:11:37,643 cfg.training.model_dir             : results/rnn/transfer/splits.en/en-yue_300_512/char\n",
      "2020-03-02 00:11:37,643 cfg.training.overwrite             : True\n",
      "2020-03-02 00:11:37,643 cfg.training.shuffle               : True\n",
      "2020-03-02 00:11:37,643 cfg.training.use_cuda              : True\n",
      "2020-03-02 00:11:37,643 cfg.training.max_output_length     : 60\n",
      "2020-03-02 00:11:37,643 cfg.training.print_valid_sents     : []\n",
      "2020-03-02 00:11:37,643 cfg.training.keep_last_ckpts       : 3\n",
      "2020-03-02 00:11:37,643 cfg.training.label_smoothing       : 0.0\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.reset_best_ckpt    : False\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.reset_scheduler    : False\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.reset_optimizer    : False\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.random_seed        : 42\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.optimizer          : adam\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.learning_rate      : 0.0004\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.learning_rate_min  : 1e-05\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.clip_grad_val      : 1.0\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.weight_decay       : 0.0\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.batch_size         : 48\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.batch_type         : sentence\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.eval_batch_size    : 10\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.eval_batch_type    : sentence\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.batch_multiplier   : 1\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.scheduling         : plateau\n",
      "2020-03-02 00:11:37,643 cfg.pretraining.patience           : 600\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.decrease_factor    : 0.5\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.epochs             : 45\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.validation_freq    : 10\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.logging_freq       : 1000\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.eval_metric        : bleu\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.early_stopping_metric : eval_metric\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.model_dir          : results/rnn/transfer/splits.en/en-yue_300_512/char\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.overwrite          : True\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.shuffle            : True\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.use_cuda           : True\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.max_output_length  : 60\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.print_valid_sents  : []\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.keep_last_ckpts    : 3\n",
      "2020-03-02 00:11:37,644 cfg.pretraining.label_smoothing    : 0.0\n",
      "2020-03-02 00:11:37,644 cfg.model.initializer              : xavier\n",
      "2020-03-02 00:11:37,644 cfg.model.init_weight              : 0.01\n",
      "2020-03-02 00:11:37,644 cfg.model.init_gain                : 1.0\n",
      "2020-03-02 00:11:37,644 cfg.model.bias_initializer         : zeros\n",
      "2020-03-02 00:11:37,644 cfg.model.embed_initializer        : normal\n",
      "2020-03-02 00:11:37,644 cfg.model.embed_init_weight        : 0.1\n",
      "2020-03-02 00:11:37,644 cfg.model.embed_init_gain          : 1.0\n",
      "2020-03-02 00:11:37,644 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-03-02 00:11:37,644 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-03-02 00:11:37,644 cfg.model.tied_embeddings          : False\n",
      "2020-03-02 00:11:37,644 cfg.model.tied_softmax             : False\n",
      "2020-03-02 00:11:37,644 cfg.model.encoder.type             : recurrent\n",
      "2020-03-02 00:11:37,644 cfg.model.encoder.rnn_type         : gru\n",
      "2020-03-02 00:11:37,644 cfg.model.encoder.embeddings.embedding_dim : 300\n",
      "2020-03-02 00:11:37,644 cfg.model.encoder.embeddings.scale : False\n",
      "2020-03-02 00:11:37,645 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-03-02 00:11:37,645 cfg.model.encoder.hidden_size      : 512\n",
      "2020-03-02 00:11:37,645 cfg.model.encoder.bidirectional    : True\n",
      "2020-03-02 00:11:37,645 cfg.model.encoder.dropout          : 0.3\n",
      "2020-03-02 00:11:37,645 cfg.model.encoder.num_layers       : 2\n",
      "2020-03-02 00:11:37,645 cfg.model.encoder.freeze           : False\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.type             : recurrent\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.rnn_type         : gru\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.embeddings.embedding_dim : 300\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.embeddings.scale : False\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.hidden_size      : 512\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.dropout          : 0.3\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.num_layers       : 2\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.input_feeding    : True\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.init_hidden      : last\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.attention        : bahdanau\n",
      "2020-03-02 00:11:37,645 cfg.model.decoder.freeze           : False\n",
      "2020-03-02 00:11:37,645 Data set sizes: \n",
      "\ttrain 4695,\n",
      "\tvalid 264,\n",
      "\ttest 264\n",
      "2020-03-02 00:11:37,645 First training example:\n",
      "\t[SRC] @@ \" @@ a r e @@ y o u @@ a @@ j a p a n e s e @@ s t u d e n t @@ ? @@ \" @@ \" @@ y e s @@ , @@ i @@ a m @@ . @@ \"\n",
      "\t[TRG] @@ 「 @@ 你 係 咪 日 本 學 生 嚟 㗎 @@ ? @@ 」 @@ 「 @@ 係 呀 @@ 。 @@ 」\n",
      "2020-03-02 00:11:37,645 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) @@ (5) e (6) t (7) o (8) a (9) i\n",
      "2020-03-02 00:11:37,645 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) @@ (5) 。 (6) 我 (7) 唔 (8) 你 (9) 佢\n",
      "2020-03-02 00:11:37,646 Number of Src words (types): 58\n",
      "2020-03-02 00:11:37,646 Number of Trg words (types): 1971\n",
      "2020-03-02 00:11:37,646 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(300, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(812, 512, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=300, vocab_size=58),\n",
      "\ttrg_embed=Embeddings(embedding_dim=300, vocab_size=1971))\n",
      "2020-03-02 00:11:37,647 EPOCH 1\n",
      "2020-03-02 00:11:39,594 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-02 00:11:39,595 Saving new checkpoint.\n",
      "2020-03-02 00:11:39,793 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 16141.9990, ppl: 176.2776, duration: 0.9337s\n",
      "2020-03-02 00:11:43,455 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 14823.2041, ppl: 115.5270, duration: 2.4287s\n",
      "2020-03-02 00:11:44,991 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 14402.4033, ppl: 100.9551, duration: 0.7430s\n",
      "2020-03-02 00:11:48,598 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 14453.3115, ppl: 102.6153, duration: 2.3367s\n",
      "2020-03-02 00:11:50,319 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 14269.6904, ppl:  96.7522, duration: 0.8996s\n",
      "2020-03-02 00:11:54,292 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 14407.6943, ppl: 101.1264, duration: 2.4399s\n",
      "2020-03-02 00:11:58,062 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 13917.7490, ppl:  86.4346, duration: 2.4346s\n",
      "2020-03-02 00:12:01,598 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 13752.5537, ppl:  81.9785, duration: 2.3848s\n",
      "2020-03-02 00:12:05,226 Validation result (greedy) at epoch   1, step       90: bleu:   0.00, loss: 14304.5820, ppl:  97.8400, duration: 2.4190s\n",
      "2020-03-02 00:12:05,946 Epoch   1: total training loss 7855.44\n",
      "2020-03-02 00:12:05,946 EPOCH 2\n",
      "2020-03-02 00:12:07,148 Validation result (greedy) at epoch   2, step      100: bleu:   0.00, loss: 13830.0352, ppl:  84.0392, duration: 1.0331s\n",
      "2020-03-02 00:12:10,578 Validation result (greedy) at epoch   2, step      110: bleu:   0.00, loss: 13987.4736, ppl:  88.3873, duration: 2.3404s\n",
      "2020-03-02 00:12:14,053 Validation result (greedy) at epoch   2, step      120: bleu:   0.00, loss: 13834.6904, ppl:  84.1646, duration: 2.3775s\n",
      "2020-03-02 00:12:15,575 Validation result (greedy) at epoch   2, step      130: bleu:   0.00, loss: 13651.6455, ppl:  79.3704, duration: 0.8107s\n",
      "2020-03-02 00:12:18,464 Validation result (greedy) at epoch   2, step      140: bleu:   0.00, loss: 14041.0547, ppl:  89.9178, duration: 1.8699s\n",
      "2020-03-02 00:12:20,819 Validation result (greedy) at epoch   2, step      150: bleu:   0.00, loss: 13697.1826, ppl:  80.5369, duration: 1.1524s\n",
      "2020-03-02 00:12:24,106 Validation result (greedy) at epoch   2, step      160: bleu:   0.00, loss: 13270.3145, ppl:  70.2418, duration: 1.8083s\n",
      "2020-03-02 00:12:26,319 Validation result (greedy) at epoch   2, step      170: bleu:   0.00, loss: 12778.5488, ppl:  60.0019, duration: 1.0092s\n",
      "2020-03-02 00:12:28,491 Validation result (greedy) at epoch   2, step      180: bleu:   0.00, loss: 12312.8457, ppl:  51.6846, duration: 0.9360s\n",
      "2020-03-02 00:12:31,080 Validation result (greedy) at epoch   2, step      190: bleu:   0.00, loss: 12216.6143, ppl:  50.1153, duration: 0.9096s\n",
      "2020-03-02 00:12:31,848 Epoch   2: total training loss 7188.24\n",
      "2020-03-02 00:12:31,849 EPOCH 3\n",
      "2020-03-02 00:12:33,317 Validation result (greedy) at epoch   3, step      200: bleu:   0.00, loss: 12056.2139, ppl:  47.6048, duration: 0.9887s\n",
      "2020-03-02 00:12:35,762 Validation result (greedy) at epoch   3, step      210: bleu:   0.00, loss: 11995.0684, ppl:  46.6812, duration: 1.0188s\n",
      "2020-03-02 00:12:38,074 Validation result (greedy) at epoch   3, step      220: bleu:   0.00, loss: 11774.2832, ppl:  43.4930, duration: 0.9993s\n",
      "2020-03-02 00:12:40,852 Validation result (greedy) at epoch   3, step      230: bleu:   0.00, loss: 11932.0469, ppl:  45.7480, duration: 1.1602s\n",
      "2020-03-02 00:12:43,074 Validation result (greedy) at epoch   3, step      240: bleu:   0.00, loss: 11662.2480, ppl:  41.9594, duration: 0.9911s\n",
      "2020-03-02 00:12:45,024 Validation result (greedy) at epoch   3, step      250: bleu:   0.00, loss: 11623.2021, ppl:  41.4377, duration: 0.8952s\n",
      "2020-03-02 00:12:47,357 Validation result (greedy) at epoch   3, step      260: bleu:   0.00, loss: 11435.2109, ppl:  39.0154, duration: 1.0589s\n",
      "2020-03-02 00:12:49,670 Validation result (greedy) at epoch   3, step      270: bleu:   0.00, loss: 11399.3311, ppl:  38.5695, duration: 1.0466s\n",
      "2020-03-02 00:12:52,334 Validation result (greedy) at epoch   3, step      280: bleu:   0.00, loss: 11365.8350, ppl:  38.1577, duration: 1.1910s\n",
      "2020-03-02 00:12:54,775 Validation result (greedy) at epoch   3, step      290: bleu:   0.00, loss: 11275.1611, ppl:  37.0651, duration: 1.1011s\n",
      "2020-03-02 00:12:55,478 Epoch   3: total training loss 6434.06\n",
      "2020-03-02 00:12:55,478 EPOCH 4\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from joeynmt.training import train_transfer\n",
    "\n",
    "emb_size = 300\n",
    "hidden_size = 512\n",
    "base_dir = Path('data/transfer/preprocessed/')\n",
    "for base_lang in ['splits.en']:\n",
    "    #tr tk bn mr lt\n",
    "    base_lang_dir = base_dir / base_lang\n",
    "    for lang in os.listdir(base_lang_dir):\n",
    "        if 'shp' not in lang:\n",
    "            lang_dir = base_lang_dir / lang\n",
    "            for segment in os.listdir(lang_dir):\n",
    "                segment_dir = lang_dir / segment\n",
    "                \n",
    "                pretrained_lang_src, pretrained_lang_tgt = lang.split('-')\n",
    "                lang_src = pretrained_lang_src\n",
    "                lang_tgt = 'shp'\n",
    "\n",
    "                training_dir = Path(str(segment_dir).replace(pretrained_lang_tgt, 'shp'))\n",
    "                \n",
    "                if 'bpe_drop' in segment:\n",
    "                    level = 'bpe'\n",
    "                elif 'bpe' in segment:\n",
    "                    level = 'bpe'\n",
    "                elif 'char' in segment:\n",
    "                    level = 'char'\n",
    "                elif 'word' in segment:\n",
    "                    level = 'word'\n",
    "                elif 'syl' in segment:\n",
    "                    level = 'syl'\n",
    "                else:\n",
    "                    level = None         \n",
    "                    \n",
    "                f_config = config.format(lang_src=lang_src, lang_tgt=lang_tgt, \\\n",
    "                    train_path=os.path.join(training_dir, 'train'),\\\n",
    "                    test_path=os.path.join(training_dir, 'test'),\\\n",
    "                    dev_path=os.path.join(training_dir, 'valid'),\\\n",
    "                    pretrained_lang_src=pretrained_lang_src,\\\n",
    "                    pretrained_lang_tgt=pretrained_lang_tgt,\\\n",
    "                    pretrained_train_path=os.path.join(segment_dir, 'train'),\\\n",
    "                    pretrained_test_path=os.path.join(segment_dir, 'test'),\\\n",
    "                    pretrained_dev_path=os.path.join(segment_dir, 'valid'),\\\n",
    "                    level=level,\\\n",
    "                    emb_size=emb_size,\\\n",
    "                    hidden_size=hidden_size,\\\n",
    "                    val_freq=10,\\\n",
    "                    model_dir=os.path.join('results/rnn/transfer', 'splits.en',\\\n",
    "                                        f'{pretrained_lang_src}-{pretrained_lang_tgt}_{emb_size}_{hidden_size}', segment))\n",
    "                \n",
    "                with open(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"transfer\"),'w') as f:\n",
    "                    f.write(f_config)\n",
    "                    \n",
    "                !python3 joeynmt/joeynmt train_transfer \"joeynmt/configs/sample_transfer.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
