{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "from typing import List\n",
    "import os\n",
    "import queue\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.data import Dataset\n",
    "\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import log_data_info, load_config, log_cfg, \\\n",
    "    store_attention_plots, load_checkpoint, make_model_dir, \\\n",
    "    make_logger, set_seed, symlink_update, ConfigurationError\n",
    "from joeynmt.model import Model\n",
    "from joeynmt.prediction import validate_on_data\n",
    "from joeynmt.loss import XentLoss\n",
    "from joeynmt.data import load_data, make_data_iter\n",
    "from joeynmt.builders import build_optimizer, build_scheduler, \\\n",
    "    build_gradient_clipper\n",
    "from joeynmt.training import TrainManager\n",
    "from joeynmt.prediction import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joeynmt.model import Model\n",
    "from joeynmt.initialization import initialize_model\n",
    "from joeynmt.embeddings import Embeddings\n",
    "from joeynmt.encoders import Encoder, RecurrentEncoder, TransformerEncoder\n",
    "from joeynmt.decoders import Decoder, RecurrentDecoder, TransformerDecoder\n",
    "from joeynmt.constants import PAD_TOKEN, EOS_TOKEN, BOS_TOKEN\n",
    "from joeynmt.search import beam_search, greedy\n",
    "from joeynmt.vocabulary import Vocabulary\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(module, val):\n",
    "    for p in module.parameters():\n",
    "        p = p.detach()\n",
    "        p.requires_grad = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "from typing import List\n",
    "import os\n",
    "import queue\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.data import Dataset\n",
    "\n",
    "from joeynmt.model import build_model, build_pretrained_model\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import log_data_info, load_config, log_cfg, \\\n",
    "    store_attention_plots, load_checkpoint, make_model_dir, \\\n",
    "    make_logger, set_seed, symlink_update, ConfigurationError\n",
    "from joeynmt.model import Model\n",
    "from joeynmt.prediction import validate_on_data\n",
    "from joeynmt.loss import XentLoss\n",
    "from joeynmt.data import load_data, make_data_iter\n",
    "from joeynmt.builders import build_optimizer, build_scheduler, \\\n",
    "    build_gradient_clipper\n",
    "from joeynmt.prediction import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer(cfg_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Main training function. After training, also test on test data if given.\n",
    "\n",
    "    :param cfg_file: path to configuration yaml file\n",
    "    \"\"\"\n",
    "    cfg = load_config(cfg_file)\n",
    "\n",
    "    # set the random seed\n",
    "    set_seed(seed=cfg[\"pretraining\"].get(\"random_seed\", 42))\n",
    "\n",
    "    # load the data\n",
    "    pre_train_data, pre_dev_data, pre_test_data, pre_src_vocab, pre_trg_vocab = load_data(\n",
    "        data_cfg=cfg[\"pretrained_data\"])\n",
    "\n",
    "    # build an encoder-decoder model\n",
    "    pretrained_model = build_model(cfg[\"model\"], src_vocab=pre_src_vocab, trg_vocab=pre_trg_vocab)\n",
    "\n",
    "    # for training management, e.g. early stopping and model selection\n",
    "    trainer = TrainManager(model=pretrained_model, config=cfg, training_key=\"pretraining\")\n",
    "\n",
    "    # store copy of original training config in model dir\n",
    "    shutil.copy2(cfg_file, trainer.model_dir+\"/config.yaml\")\n",
    "\n",
    "    # log all entries of config\n",
    "    log_cfg(cfg, trainer.logger)\n",
    "\n",
    "    log_data_info(train_data=pre_train_data, valid_data=pre_dev_data,\n",
    "                  test_data=pre_test_data, src_vocab=pre_src_vocab, trg_vocab=pre_trg_vocab,\n",
    "                  logging_function=trainer.logger.info)\n",
    "\n",
    "    trainer.logger.info(str(pretrained_model))\n",
    "\n",
    "    # store the vocabs\n",
    "    src_vocab_file = \"{}/src_vocab.txt\".format(cfg[\"pretraining\"][\"model_dir\"])\n",
    "    pre_src_vocab.to_file(src_vocab_file)\n",
    "    trg_vocab_file = \"{}/trg_vocab.txt\".format(cfg[\"pretraining\"][\"model_dir\"])\n",
    "    pre_trg_vocab.to_file(trg_vocab_file)\n",
    "\n",
    "    # train the model\n",
    "    trainer.train_and_validate(train_data=pre_train_data, valid_data=pre_dev_data)\n",
    "\n",
    "    # predict with the best model on validation and test\n",
    "    # (if test data is available)\n",
    "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
    "    output_name = \"{:08d}.hyps\".format(trainer.best_ckpt_iteration)\n",
    "    output_path = os.path.join(trainer.model_dir, output_name)\n",
    "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=trainer.logger,\n",
    "        key_training=\"pretraining\", key_data=\"pretrained_data\")\n",
    "    \n",
    "    # set the random seed\n",
    "    set_seed(seed=cfg[\"training\"].get(\"random_seed\", 42))\n",
    "\n",
    "    # load the data\n",
    "    train_data, dev_data, test_data, src_vocab, trg_vocab = load_data(\n",
    "        data_cfg=cfg[\"data\"])\n",
    "\n",
    "    # build an encoder-decoder model\n",
    "    model = build_pretrained_model(cfg[\"model\"], \n",
    "                                   pretrained_model=pretrained_model, \n",
    "                                   pretrained_src_vocab=pre_src_vocab,\n",
    "                                   src_vocab=src_vocab, \n",
    "                                   trg_vocab=trg_vocab)\n",
    "\n",
    "    # for training management, e.g. early stopping and model selection\n",
    "    trainer = TrainManager(model=model, config=cfg, training_key=\"training\", logger=trainer.logger)\n",
    "\n",
    "    # store copy of original training config in model dir\n",
    "    shutil.copy2(cfg_file, trainer.model_dir+\"/config.yaml\")\n",
    "\n",
    "    # log all entries of config\n",
    "    log_cfg(cfg, trainer.logger)\n",
    "\n",
    "    log_data_info(train_data=train_data, valid_data=dev_data,\n",
    "                  test_data=test_data, src_vocab=src_vocab, trg_vocab=trg_vocab,\n",
    "                  logging_function=trainer.logger.info)\n",
    "\n",
    "    trainer.logger.info(str(model))\n",
    "\n",
    "    # store the vocabs\n",
    "    src_vocab_file = \"{}/src_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    src_vocab.to_file(src_vocab_file)\n",
    "    trg_vocab_file = \"{}/trg_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    trg_vocab.to_file(trg_vocab_file)\n",
    "\n",
    "    # train the model\n",
    "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
    "\n",
    "    # predict with the best model on validation and test\n",
    "    # (if test data is available)\n",
    "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
    "    output_name = \"{:08d}.hyps\".format(trainer.best_ckpt_iteration)\n",
    "    output_path = os.path.join(trainer.model_dir, output_name)\n",
    "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=trainer.logger,\n",
    "        key_training=\"training\", key_data=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "pretrained_data: # specify your data here\n",
    "    src: {pretrained_lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {pretrained_lang_tgt}                       # trg language\n",
    "    train: {pretrained_train_path}     # training data\n",
    "    dev: {pretrained_dev_path}         # development data for validation\n",
    "    test: {pretrained_test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 20                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: False                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "pretraining:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0002           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.00001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 30                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: False                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path('data/transfer/preprocessed/')\n",
    "for base_lang in ['splits.en']:\n",
    "    base_lang_dir = base_dir / base_lang\n",
    "    for lang in os.listdir(base_lang_dir):\n",
    "        if 'shp' not in lang:\n",
    "            lang_dir = base_lang_dir / lang\n",
    "            for segment in os.listdir(lang_dir):\n",
    "                segment_dir = lang_dir / segment\n",
    "                \n",
    "                pretrained_lang_src, pretrained_lang_tgt = lang.split('-')\n",
    "                lang_src = lanpretrained_lang_src\n",
    "                lang_tgt = 'shp'\n",
    "\n",
    "                training_dir = Path(str(segment_dir).replace(pretrained_lang_tgt, 'shp'))\n",
    "                \n",
    "                f_config = config.format(lang_src=lang_src, lang_tgt=lang_tgt, \n",
    "                    train_path=os.path.join(training_dir, 'train'),\n",
    "                    test_path=os.path.join(training_dir, 'test'),\n",
    "                    dev_path=os.path.join(training_dir, 'valid'),\n",
    "                    pretrained_lang_src=lanpretrained_lang_src,\\\n",
    "                    pretrained_lang_tgt=pretrained_lang_tgt,\\ \n",
    "                    pretrained_train_path=os.path.join(segment_dir, 'train'),\n",
    "                    pretrained_test_path=os.path.join(segment_dir, 'test'),\n",
    "                    pretrained_dev_path=os.path.join(segment_dir, 'valid'),\n",
    "                    level='word',\n",
    "                    emb_size=2,\n",
    "                    hidden_size=5,\n",
    "                    val_freq=2,\n",
    "                    model_dir=os.path.join('results/temp'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f_config = config.format(lang_src='es', lang_tgt='shp', \n",
    "                    train_path=os.path.join('data/translate/preprocessed/Religioso/word', 'train'),\n",
    "                    test_path=os.path.join('data/translate/preprocessed/Religioso/word', 'test'),\n",
    "                    dev_path=os.path.join('data/translate/preprocessed/Religioso/word', 'valid'),\n",
    "                    pretrained_lang_src='es', pretrained_lang_tgt='shp', \n",
    "                    pretrained_train_path=os.path.join('data/translate/preprocessed/Educativo/word', 'train'),\n",
    "                    pretrained_test_path=os.path.join('data/translate/preprocessed/Educativo/word', 'test'),\n",
    "                    pretrained_dev_path=os.path.join('data/translate/preprocessed/Educativo/word', 'valid'),\n",
    "                    level='word',\n",
    "                    emb_size=2,\n",
    "                    hidden_size=5,\n",
    "                    val_freq=2,\n",
    "                    model_dir=os.path.join('results/temp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\"),'w') as f:\n",
    "    f.write(f_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-28 00:15:19,646 Hello! This is Joey-NMT.\n",
      "2020-02-28 00:15:20,820 Total params: 31891\n",
      "2020-02-28 00:15:20,821 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-28 00:15:20,822 cfg.name                           : my_experiment\n",
      "2020-02-28 00:15:20,822 cfg.data.src                       : es\n",
      "2020-02-28 00:15:20,823 cfg.data.trg                       : shp\n",
      "2020-02-28 00:15:20,823 cfg.data.train                     : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-28 00:15:20,824 cfg.data.dev                       : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-28 00:15:20,824 cfg.data.test                      : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-28 00:15:20,825 cfg.data.level                     : word\n",
      "2020-02-28 00:15:20,825 cfg.data.lowercase                 : True\n",
      "2020-02-28 00:15:20,825 cfg.data.max_sent_length           : 150\n",
      "2020-02-28 00:15:20,826 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-28 00:15:20,826 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-28 00:15:20,827 cfg.pretrained_data.src            : es\n",
      "2020-02-28 00:15:20,827 cfg.pretrained_data.trg            : shp\n",
      "2020-02-28 00:15:20,828 cfg.pretrained_data.train          : data/translate/preprocessed/Educativo/word/train\n",
      "2020-02-28 00:15:20,828 cfg.pretrained_data.dev            : data/translate/preprocessed/Educativo/word/valid\n",
      "2020-02-28 00:15:20,828 cfg.pretrained_data.test           : data/translate/preprocessed/Educativo/word/test\n",
      "2020-02-28 00:15:20,829 cfg.pretrained_data.level          : word\n",
      "2020-02-28 00:15:20,829 cfg.pretrained_data.lowercase      : True\n",
      "2020-02-28 00:15:20,830 cfg.pretrained_data.max_sent_length : 150\n",
      "2020-02-28 00:15:20,830 cfg.pretrained_data.src_voc_min_freq : 1\n",
      "2020-02-28 00:15:20,830 cfg.pretrained_data.trg_voc_min_freq : 1\n",
      "2020-02-28 00:15:20,831 cfg.testing.beam_size              : 5\n",
      "2020-02-28 00:15:20,831 cfg.testing.alpha                  : 1.0\n",
      "2020-02-28 00:15:20,832 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-28 00:15:20,832 cfg.training.reset_scheduler       : False\n",
      "2020-02-28 00:15:20,832 cfg.training.reset_optimizer       : False\n",
      "2020-02-28 00:15:20,833 cfg.training.random_seed           : 42\n",
      "2020-02-28 00:15:20,833 cfg.training.optimizer             : adam\n",
      "2020-02-28 00:15:20,834 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-28 00:15:20,834 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-28 00:15:20,834 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-28 00:15:20,835 cfg.training.weight_decay          : 0.0\n",
      "2020-02-28 00:15:20,835 cfg.training.batch_size            : 48\n",
      "2020-02-28 00:15:20,836 cfg.training.batch_type            : sentence\n",
      "2020-02-28 00:15:20,836 cfg.training.eval_batch_size       : 10\n",
      "2020-02-28 00:15:20,836 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-28 00:15:20,837 cfg.training.batch_multiplier      : 1\n",
      "2020-02-28 00:15:20,837 cfg.training.scheduling            : plateau\n",
      "2020-02-28 00:15:20,838 cfg.training.patience              : 600\n",
      "2020-02-28 00:15:20,838 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-28 00:15:20,838 cfg.training.epochs                : 20\n",
      "2020-02-28 00:15:20,839 cfg.training.validation_freq       : 2\n",
      "2020-02-28 00:15:20,841 cfg.training.logging_freq          : 1000\n",
      "2020-02-28 00:15:20,842 cfg.training.eval_metric           : bleu\n",
      "2020-02-28 00:15:20,842 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-28 00:15:20,842 cfg.training.model_dir             : results/temp\n",
      "2020-02-28 00:15:20,843 cfg.training.overwrite             : True\n",
      "2020-02-28 00:15:20,843 cfg.training.shuffle               : True\n",
      "2020-02-28 00:15:20,844 cfg.training.use_cuda              : False\n",
      "2020-02-28 00:15:20,844 cfg.training.max_output_length     : 60\n",
      "2020-02-28 00:15:20,844 cfg.training.print_valid_sents     : []\n",
      "2020-02-28 00:15:20,845 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-28 00:15:20,845 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-28 00:15:20,846 cfg.pretraining.reset_best_ckpt    : False\n",
      "2020-02-28 00:15:20,846 cfg.pretraining.reset_scheduler    : False\n",
      "2020-02-28 00:15:20,846 cfg.pretraining.reset_optimizer    : False\n",
      "2020-02-28 00:15:20,847 cfg.pretraining.random_seed        : 42\n",
      "2020-02-28 00:15:20,847 cfg.pretraining.optimizer          : adam\n",
      "2020-02-28 00:15:20,848 cfg.pretraining.learning_rate      : 0.0002\n",
      "2020-02-28 00:15:20,848 cfg.pretraining.learning_rate_min  : 1e-05\n",
      "2020-02-28 00:15:20,848 cfg.pretraining.clip_grad_val      : 1.0\n",
      "2020-02-28 00:15:20,849 cfg.pretraining.weight_decay       : 0.0\n",
      "2020-02-28 00:15:20,849 cfg.pretraining.batch_size         : 48\n",
      "2020-02-28 00:15:20,849 cfg.pretraining.batch_type         : sentence\n",
      "2020-02-28 00:15:20,850 cfg.pretraining.eval_batch_size    : 10\n",
      "2020-02-28 00:15:20,850 cfg.pretraining.eval_batch_type    : sentence\n",
      "2020-02-28 00:15:20,851 cfg.pretraining.batch_multiplier   : 1\n",
      "2020-02-28 00:15:20,851 cfg.pretraining.scheduling         : plateau\n",
      "2020-02-28 00:15:20,851 cfg.pretraining.patience           : 600\n",
      "2020-02-28 00:15:20,852 cfg.pretraining.decrease_factor    : 0.5\n",
      "2020-02-28 00:15:20,852 cfg.pretraining.epochs             : 30\n",
      "2020-02-28 00:15:20,853 cfg.pretraining.validation_freq    : 2\n",
      "2020-02-28 00:15:20,853 cfg.pretraining.logging_freq       : 1000\n",
      "2020-02-28 00:15:20,853 cfg.pretraining.eval_metric        : bleu\n",
      "2020-02-28 00:15:20,854 cfg.pretraining.early_stopping_metric : eval_metric\n",
      "2020-02-28 00:15:20,854 cfg.pretraining.model_dir          : results/temp\n",
      "2020-02-28 00:15:20,854 cfg.pretraining.overwrite          : True\n",
      "2020-02-28 00:15:20,855 cfg.pretraining.shuffle            : True\n",
      "2020-02-28 00:15:20,855 cfg.pretraining.use_cuda           : False\n",
      "2020-02-28 00:15:20,856 cfg.pretraining.max_output_length  : 60\n",
      "2020-02-28 00:15:20,856 cfg.pretraining.print_valid_sents  : []\n",
      "2020-02-28 00:15:20,856 cfg.pretraining.keep_last_ckpts    : 3\n",
      "2020-02-28 00:15:20,857 cfg.pretraining.label_smoothing    : 0.0\n",
      "2020-02-28 00:15:20,857 cfg.model.initializer              : xavier\n",
      "2020-02-28 00:15:20,857 cfg.model.init_weight              : 0.01\n",
      "2020-02-28 00:15:20,858 cfg.model.init_gain                : 1.0\n",
      "2020-02-28 00:15:20,858 cfg.model.bias_initializer         : zeros\n",
      "2020-02-28 00:15:20,859 cfg.model.embed_initializer        : normal\n",
      "2020-02-28 00:15:20,859 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-28 00:15:20,859 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-28 00:15:20,860 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-28 00:15:20,860 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-28 00:15:20,860 cfg.model.tied_embeddings          : False\n",
      "2020-02-28 00:15:20,861 cfg.model.tied_softmax             : False\n",
      "2020-02-28 00:15:20,861 cfg.model.encoder.type             : recurrent\n",
      "2020-02-28 00:15:20,862 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-28 00:15:20,862 cfg.model.encoder.embeddings.embedding_dim : 2\n",
      "2020-02-28 00:15:20,862 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-28 00:15:20,863 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-28 00:15:20,863 cfg.model.encoder.hidden_size      : 5\n",
      "2020-02-28 00:15:20,863 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-28 00:15:20,864 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-28 00:15:20,864 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-28 00:15:20,872 cfg.model.encoder.freeze           : False\n",
      "2020-02-28 00:15:20,873 cfg.model.decoder.type             : recurrent\n",
      "2020-02-28 00:15:20,873 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-28 00:15:20,873 cfg.model.decoder.embeddings.embedding_dim : 2\n",
      "2020-02-28 00:15:20,874 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-28 00:15:20,874 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-28 00:15:20,875 cfg.model.decoder.hidden_size      : 5\n",
      "2020-02-28 00:15:20,875 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-28 00:15:20,876 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-28 00:15:20,876 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-28 00:15:20,876 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-28 00:15:20,877 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-28 00:15:20,877 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-28 00:15:20,878 cfg.model.decoder.freeze           : False\n",
      "2020-02-28 00:15:20,878 Data set sizes: \n",
      "\ttrain 3979,\n",
      "\tvalid 498,\n",
      "\ttest 498\n",
      "2020-02-28 00:15:20,878 First training example:\n",
      "\t[SRC] nenora non aki kai tanakin westiorabo kirika wishamaxon onankiakana ixon\n",
      "\t[TRG] se evaluará mediante problemas propuestos en una hoja de aplicación individual\n",
      "2020-02-28 00:15:20,879 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) itan (5) ja (6) kopi (7) ? (8) ¿ (9) yoyo\n",
      "2020-02-28 00:15:20,879 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) de (5) y (6) que (7) los (8) la (9) en\n",
      "2020-02-28 00:15:20,880 Number of Src words (types): 4056\n",
      "2020-02-28 00:15:20,880 Number of Trg words (types): 3207\n",
      "2020-02-28 00:15:20,881 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(2, 5, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(7, 5, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=2, vocab_size=4056),\n",
      "\ttrg_embed=Embeddings(embedding_dim=2, vocab_size=3207))\n",
      "2020-02-28 00:15:20,887 EPOCH 1\n",
      "2020-02-28 00:15:24,205 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-28 00:15:24,206 Saving new checkpoint.\n",
      "2020-02-28 00:15:24,209 Validation result (greedy) at epoch   1, step        2: bleu:   0.00, loss: 39097.5117, ppl: 3206.6890, duration: 3.2158s\n",
      "2020-02-28 00:15:27,508 Validation result (greedy) at epoch   1, step        4: bleu:   0.00, loss: 39097.0977, ppl: 3206.4138, duration: 3.1331s\n",
      "2020-02-28 00:15:30,769 Validation result (greedy) at epoch   1, step        6: bleu:   0.00, loss: 39096.5742, ppl: 3206.0684, duration: 3.1367s\n",
      "2020-02-28 00:15:34,255 Validation result (greedy) at epoch   1, step        8: bleu:   0.00, loss: 39095.9648, ppl: 3205.6648, duration: 3.3684s\n",
      "2020-02-28 00:15:37,470 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 39095.2500, ppl: 3205.1909, duration: 3.1260s\n",
      "2020-02-28 00:15:41,083 Validation result (greedy) at epoch   1, step       12: bleu:   0.00, loss: 39094.4492, ppl: 3204.6621, duration: 3.4517s\n",
      "2020-02-28 00:15:44,390 Validation result (greedy) at epoch   1, step       14: bleu:   0.00, loss: 39093.5078, ppl: 3204.0388, duration: 3.1509s\n",
      "2020-02-28 00:15:47,637 Validation result (greedy) at epoch   1, step       16: bleu:   0.00, loss: 39092.4375, ppl: 3203.3330, duration: 3.1170s\n",
      "2020-02-28 00:15:50,926 Validation result (greedy) at epoch   1, step       18: bleu:   0.00, loss: 39091.3320, ppl: 3202.5999, duration: 3.1606s\n",
      "2020-02-28 00:15:54,120 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 39090.2031, ppl: 3201.8547, duration: 3.1143s\n",
      "2020-02-28 00:15:57,345 Validation result (greedy) at epoch   1, step       22: bleu:   0.00, loss: 39088.8945, ppl: 3200.9875, duration: 3.1112s\n",
      "2020-02-28 00:16:00,988 Validation result (greedy) at epoch   1, step       24: bleu:   0.00, loss: 39087.3945, ppl: 3199.9988, duration: 3.5425s\n",
      "2020-02-28 00:16:04,245 Validation result (greedy) at epoch   1, step       26: bleu:   0.00, loss: 39085.6328, ppl: 3198.8330, duration: 3.1141s\n",
      "2020-02-28 00:16:07,786 Validation result (greedy) at epoch   1, step       28: bleu:   0.00, loss: 39083.6055, ppl: 3197.4941, duration: 3.4079s\n",
      "2020-02-28 00:16:10,976 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 39081.5312, ppl: 3196.1252, duration: 3.1142s\n",
      "2020-02-28 00:16:14,168 Validation result (greedy) at epoch   1, step       32: bleu:   0.00, loss: 39079.4414, ppl: 3194.7478, duration: 3.1165s\n",
      "2020-02-28 00:16:17,340 Validation result (greedy) at epoch   1, step       34: bleu:   0.00, loss: 39077.3125, ppl: 3193.3435, duration: 3.1080s\n",
      "2020-02-28 00:16:20,573 Validation result (greedy) at epoch   1, step       36: bleu:   0.00, loss: 39074.9531, ppl: 3191.7878, duration: 3.1274s\n",
      "2020-02-28 00:16:23,850 Validation result (greedy) at epoch   1, step       38: bleu:   0.00, loss: 39072.3320, ppl: 3190.0593, duration: 3.1748s\n",
      "2020-02-28 00:16:27,402 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 39069.5703, ppl: 3188.2405, duration: 3.4499s\n",
      "2020-02-28 00:16:30,552 Validation result (greedy) at epoch   1, step       42: bleu:   0.00, loss: 39066.5273, ppl: 3186.2405, duration: 3.0507s\n",
      "2020-02-28 00:16:33,650 Validation result (greedy) at epoch   1, step       44: bleu:   0.00, loss: 39063.3750, ppl: 3184.1658, duration: 2.9961s\n",
      "2020-02-28 00:16:36,849 Validation result (greedy) at epoch   1, step       46: bleu:   0.00, loss: 39060.2656, ppl: 3182.1228, duration: 3.0938s\n",
      "2020-02-28 00:16:40,060 Validation result (greedy) at epoch   1, step       48: bleu:   0.00, loss: 39057.0234, ppl: 3179.9932, duration: 3.0907s\n",
      "2020-02-28 00:16:43,183 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 39053.3125, ppl: 3177.5559, duration: 2.9995s\n",
      "2020-02-28 00:16:46,197 Validation result (greedy) at epoch   1, step       52: bleu:   0.00, loss: 39049.4336, ppl: 3175.0144, duration: 2.9255s\n",
      "2020-02-28 00:16:49,120 Validation result (greedy) at epoch   1, step       54: bleu:   0.00, loss: 39044.9336, ppl: 3172.0635, duration: 2.7379s\n",
      "2020-02-28 00:16:52,429 Validation result (greedy) at epoch   1, step       56: bleu:   0.00, loss: 39039.7734, ppl: 3168.6863, duration: 3.1659s\n",
      "2020-02-28 00:16:55,486 Validation result (greedy) at epoch   1, step       58: bleu:   0.00, loss: 39034.8086, ppl: 3165.4395, duration: 2.9928s\n",
      "2020-02-28 00:16:57,922 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 39029.7148, ppl: 3162.1113, duration: 2.3155s\n",
      "2020-02-28 00:16:59,951 Validation result (greedy) at epoch   1, step       62: bleu:   0.00, loss: 39023.9609, ppl: 3158.3562, duration: 1.8956s\n",
      "2020-02-28 00:17:01,761 Validation result (greedy) at epoch   1, step       64: bleu:   0.00, loss: 39017.9062, ppl: 3154.4099, duration: 1.6552s\n",
      "2020-02-28 00:17:03,422 Validation result (greedy) at epoch   1, step       66: bleu:   0.00, loss: 39011.9453, ppl: 3150.5315, duration: 1.5960s\n",
      "2020-02-28 00:17:05,067 Validation result (greedy) at epoch   1, step       68: bleu:   0.00, loss: 39005.9922, ppl: 3146.6611, duration: 1.5550s\n",
      "2020-02-28 00:17:06,643 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 38999.6836, ppl: 3142.5645, duration: 1.4202s\n",
      "2020-02-28 00:17:08,128 Validation result (greedy) at epoch   1, step       72: bleu:   0.00, loss: 38992.6797, ppl: 3138.0244, duration: 1.2550s\n",
      "2020-02-28 00:17:09,504 Validation result (greedy) at epoch   1, step       74: bleu:   0.00, loss: 38985.0820, ppl: 3133.1052, duration: 1.2389s\n",
      "2020-02-28 00:17:10,642 Validation result (greedy) at epoch   1, step       76: bleu:   0.00, loss: 38977.4336, ppl: 3128.1611, duration: 1.0345s\n",
      "2020-02-28 00:17:11,783 Validation result (greedy) at epoch   1, step       78: bleu:   0.00, loss: 38969.8281, ppl: 3123.2515, duration: 1.0395s\n",
      "2020-02-28 00:17:12,935 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 38962.1992, ppl: 3118.3350, duration: 1.0352s\n",
      "2020-02-28 00:17:14,104 Validation result (greedy) at epoch   1, step       82: bleu:   0.00, loss: 38954.6211, ppl: 3113.4585, duration: 1.0696s\n",
      "2020-02-28 00:17:14,152 Epoch   1: total training loss 6565.01\n",
      "2020-02-28 00:17:14,153 EPOCH 2\n",
      "2020-02-28 00:17:15,271 Validation result (greedy) at epoch   2, step       84: bleu:   0.00, loss: 38947.2031, ppl: 3108.6936, duration: 1.0665s\n",
      "2020-02-28 00:17:16,513 Validation result (greedy) at epoch   2, step       86: bleu:   0.00, loss: 38939.3750, ppl: 3103.6726, duration: 1.0884s\n",
      "2020-02-28 00:17:17,814 Validation result (greedy) at epoch   2, step       88: bleu:   0.00, loss: 38931.1016, ppl: 3098.3760, duration: 1.1606s\n",
      "2020-02-28 00:17:18,984 Validation result (greedy) at epoch   2, step       90: bleu:   0.00, loss: 38923.5703, ppl: 3093.5603, duration: 1.0819s\n",
      "2020-02-28 00:17:20,206 Validation result (greedy) at epoch   2, step       92: bleu:   0.00, loss: 38915.9023, ppl: 3088.6667, duration: 1.0807s\n",
      "2020-02-28 00:17:21,417 Validation result (greedy) at epoch   2, step       94: bleu:   0.00, loss: 38908.7109, ppl: 3084.0840, duration: 1.1358s\n",
      "2020-02-28 00:17:22,637 Validation result (greedy) at epoch   2, step       96: bleu:   0.00, loss: 38900.8750, ppl: 3079.0996, duration: 1.0623s\n",
      "2020-02-28 00:17:23,901 Validation result (greedy) at epoch   2, step       98: bleu:   0.00, loss: 38893.1797, ppl: 3074.2085, duration: 1.1938s\n",
      "2020-02-28 00:17:25,026 Validation result (greedy) at epoch   2, step      100: bleu:   0.00, loss: 38885.4609, ppl: 3069.3132, duration: 1.0265s\n",
      "2020-02-28 00:17:26,151 Validation result (greedy) at epoch   2, step      102: bleu:   0.00, loss: 38877.2305, ppl: 3064.1016, duration: 1.0220s\n",
      "2020-02-28 00:17:27,296 Validation result (greedy) at epoch   2, step      104: bleu:   0.00, loss: 38868.8047, ppl: 3058.7761, duration: 1.0454s\n",
      "2020-02-28 00:17:28,442 Validation result (greedy) at epoch   2, step      106: bleu:   0.00, loss: 38859.8203, ppl: 3053.1077, duration: 1.0442s\n",
      "2020-02-28 00:17:29,721 Validation result (greedy) at epoch   2, step      108: bleu:   0.00, loss: 38849.5469, ppl: 3046.6362, duration: 1.0596s\n",
      "2020-02-28 00:17:30,854 Validation result (greedy) at epoch   2, step      110: bleu:   0.00, loss: 38839.3398, ppl: 3040.2217, duration: 1.0294s\n",
      "2020-02-28 00:17:32,080 Validation result (greedy) at epoch   2, step      112: bleu:   0.00, loss: 38828.5703, ppl: 3033.4709, duration: 1.0682s\n",
      "2020-02-28 00:17:33,466 Validation result (greedy) at epoch   2, step      114: bleu:   0.00, loss: 38816.8281, ppl: 3026.1230, duration: 1.2776s\n",
      "2020-02-28 00:17:34,775 Validation result (greedy) at epoch   2, step      116: bleu:   0.00, loss: 38805.2109, ppl: 3018.8735, duration: 1.2059s\n",
      "2020-02-28 00:17:35,870 Validation result (greedy) at epoch   2, step      118: bleu:   0.00, loss: 38794.5742, ppl: 3012.2505, duration: 1.0204s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-35d4fc1632ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoeynmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_transfer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"joeynmt/configs/sample_{name}.yaml\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/training.py\u001b[0m in \u001b[0;36mtrain_transfer\u001b[0;34m(cfg_file)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;31m# predict with the best model on validation and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/training.py\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(self, train_data, valid_data)\u001b[0m\n\u001b[1;32m    341\u001b[0m                             \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                             \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# greedy validations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                             \u001b[0mbatch_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_batch_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m                         )\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/prediction.py\u001b[0m in \u001b[0;36mvalidate_on_data\u001b[0;34m(model, data, logger, batch_size, use_cuda, max_output_length, level, eval_metric, loss_function, beam_size, beam_alpha, batch_type)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 batch_loss = model.get_loss_for_batch(\n\u001b[0;32m---> 98\u001b[0;31m                     batch, loss_function=loss_function)\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mtotal_ntokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/model.py\u001b[0m in \u001b[0;36mget_loss_for_batch\u001b[0;34m(self, batch, loss_function)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             trg_mask=batch.trg_mask)\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# compute log probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg_input, src_mask, src_lengths, trg_mask)\u001b[0m\n\u001b[1;32m     78\u001b[0m                            \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                            \u001b[0munroll_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                            trg_mask=trg_mask)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/model.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, encoder_output, encoder_hidden, src_mask, trg_input, unroll_steps, decoder_hidden, trg_mask)\u001b[0m\n\u001b[1;32m    115\u001b[0m                             \u001b[0munroll_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                             \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                             trg_mask=trg_mask)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loss_for_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/decoders.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, trg_embed, encoder_output, encoder_hidden, src_mask, unroll_steps, hidden, prev_att_vector, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mencoder_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 hidden=hidden)\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0matt_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_att_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0matt_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/decoders.py\u001b[0m in \u001b[0;36m_forward_step\u001b[0;34m(self, prev_embed, prev_att_vector, encoder_output, src_mask, hidden)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# key projections are pre-computed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         context, att_probs = self.attention(\n\u001b[0;32m--> 262\u001b[0;31m             query=query, values=encoder_output, mask=src_mask)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# return attention vector (Luong)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, mask, values)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# We first project the query (the decoder state).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# The projected keys (the encoder states) were already pre-computated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_proj_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Calculate scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/attention.py\u001b[0m in \u001b[0;36mcompute_proj_query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     def _check_input_shapes_forward(self, query: torch.Tensor,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from joeynmt.training import train_transfer\n",
    "\n",
    "train_transfer(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
