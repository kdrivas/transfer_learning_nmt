{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "from typing import List\n",
    "import os\n",
    "import queue\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.data import Dataset\n",
    "\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import log_data_info, load_config, log_cfg, \\\n",
    "    store_attention_plots, load_checkpoint, make_model_dir, \\\n",
    "    make_logger, set_seed, symlink_update, ConfigurationError\n",
    "from joeynmt.model import Model\n",
    "from joeynmt.prediction import validate_on_data\n",
    "from joeynmt.loss import XentLoss\n",
    "from joeynmt.data import load_data, make_data_iter\n",
    "from joeynmt.builders import build_optimizer, build_scheduler, \\\n",
    "    build_gradient_clipper\n",
    "from joeynmt.training import TrainManager\n",
    "from joeynmt.prediction import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joeynmt.model import Model\n",
    "from joeynmt.initialization import initialize_model\n",
    "from joeynmt.embeddings import Embeddings\n",
    "from joeynmt.encoders import Encoder, RecurrentEncoder, TransformerEncoder\n",
    "from joeynmt.decoders import Decoder, RecurrentDecoder, TransformerDecoder\n",
    "from joeynmt.constants import PAD_TOKEN, EOS_TOKEN, BOS_TOKEN\n",
    "from joeynmt.search import beam_search, greedy\n",
    "from joeynmt.vocabulary import Vocabulary\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(module, val):\n",
    "    for p in module.parameters():\n",
    "        p = p.detach()\n",
    "        p.requires_grad = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "from typing import List\n",
    "import os\n",
    "import queue\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from logging import Logger\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.data import Dataset\n",
    "\n",
    "from joeynmt.model import build_model\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import log_data_info, load_config, log_cfg, \\\n",
    "    store_attention_plots, load_checkpoint, make_model_dir, \\\n",
    "    make_logger, set_seed, symlink_update, ConfigurationError\n",
    "from joeynmt.model import Model\n",
    "from joeynmt.prediction import validate_on_data\n",
    "from joeynmt.loss import XentLoss\n",
    "from joeynmt.data import load_data, make_data_iter\n",
    "from joeynmt.builders import build_optimizer, build_scheduler, \\\n",
    "    build_gradient_clipper\n",
    "from joeynmt.prediction import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "def build_pretrained_model(cfg: dict = None,\n",
    "                pretrained_model: Model = None,\n",
    "                pretrained_src_vocab: Vocabulary = None,\n",
    "                src_vocab: Vocabulary = None,\n",
    "                trg_vocab: Vocabulary = None) -> Model:\n",
    "    \"\"\"\n",
    "    Build and initialize the model according to the configuration.\n",
    "\n",
    "    :param cfg: dictionary configuration containing model specifications\n",
    "    :param src_vocab: source vocabulary\n",
    "    :param trg_vocab: target vocabulary\n",
    "    :return: built and initialized model\n",
    "    \"\"\"\n",
    "    src_padding_idx = src_vocab.stoi[PAD_TOKEN]\n",
    "    trg_padding_idx = trg_vocab.stoi[PAD_TOKEN]\n",
    "\n",
    "    src_embed = Embeddings(\n",
    "        **cfg[\"encoder\"][\"embeddings\"], vocab_size=len(src_vocab),\n",
    "        padding_idx=src_padding_idx)\n",
    "\n",
    "    embedding_matrix = np.zeros((len(src_vocab), src_embed.embedding_dim))\n",
    "    unknown_words = []\n",
    "    for w in pretrained_src_vocab.itos:\n",
    "        try:\n",
    "            ix = pretrained_src_vocab.stoi[w]\n",
    "            embedding_matrix[ix] = pretrained_model.src_embed.lut.weight[ix].cpu().detach().numpy()\n",
    "        except KeyError:\n",
    "            unknown_words.append(w)\n",
    "    \n",
    "    src_embed.lut.weight = torch.nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "\n",
    "    trg_embed = Embeddings(\n",
    "            **cfg[\"decoder\"][\"embeddings\"], vocab_size=len(trg_vocab),\n",
    "            padding_idx=trg_padding_idx)\n",
    "\n",
    "    # build decoder\n",
    "    dec_dropout = cfg[\"decoder\"].get(\"dropout\", 0.)\n",
    "    dec_emb_dropout = cfg[\"decoder\"][\"embeddings\"].get(\"dropout\", dec_dropout)\n",
    "    \n",
    "    encoder = pretrained_model.encoder\n",
    "    encoder.train()\n",
    "    set_requires_grad(encoder, True)\n",
    "\n",
    "    # build encoder\n",
    "    #enc_dropout = cfg[\"encoder\"].get(\"dropout\", 0.)\n",
    "    #enc_emb_dropout = cfg[\"encoder\"][\"embeddings\"].get(\"dropout\", enc_dropout)\n",
    "    #if cfg[\"encoder\"].get(\"type\", \"recurrent\") == \"transformer\":\n",
    "    #    assert cfg[\"encoder\"][\"embeddings\"][\"embedding_dim\"] == \\\n",
    "    #           cfg[\"encoder\"][\"hidden_size\"], \\\n",
    "    #           \"for transformer, emb_size must be hidden_size\"\n",
    "\n",
    "    #    encoder = TransformerEncoder(**cfg[\"encoder\"],\n",
    "    #                                 emb_size=src_embed.embedding_dim,\n",
    "    #                                 emb_dropout=enc_emb_dropout)\n",
    "    #else:\n",
    "    #    encoder = RecurrentEncoder(**cfg[\"encoder\"],\n",
    "    #                               emb_size=src_embed.embedding_dim,\n",
    "    #                               emb_dropout=enc_emb_dropout)\n",
    "    \n",
    "    if cfg[\"decoder\"].get(\"type\", \"recurrent\") == \"transformer\":\n",
    "        decoder = TransformerDecoder(\n",
    "            **cfg[\"decoder\"], encoder=encoder, vocab_size=len(trg_vocab),\n",
    "            emb_size=trg_embed.embedding_dim, emb_dropout=dec_emb_dropout)\n",
    "    else:\n",
    "        decoder = RecurrentDecoder(\n",
    "            **cfg[\"decoder\"], encoder=encoder, vocab_size=len(trg_vocab),\n",
    "            emb_size=trg_embed.embedding_dim, emb_dropout=dec_emb_dropout)\n",
    "\n",
    "    model = Model(encoder=encoder, decoder=decoder,\n",
    "                  src_embed=src_embed, trg_embed=trg_embed,\n",
    "                  src_vocab=pretrained_model.src_vocab, trg_vocab=trg_vocab)\n",
    "\n",
    "    # tie softmax layer with trg embeddings\n",
    "    if cfg.get(\"tied_softmax\", False):\n",
    "        if trg_embed.lut.weight.shape == \\\n",
    "                model.decoder.output_layer.weight.shape:\n",
    "            # (also) share trg embeddings and softmax layer:\n",
    "            model.decoder.output_layer.weight = trg_embed.lut.weight\n",
    "        else:\n",
    "            raise ConfigurationError(\n",
    "                \"For tied_softmax, the decoder embedding_dim and decoder \"\n",
    "                \"hidden_size must be the same.\"\n",
    "                \"The decoder must be a Transformer.\")\n",
    "\n",
    "    # custom initialization of model parameters\n",
    "    initialize_model(model, cfg, src_padding_idx, trg_padding_idx)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer(cfg_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Main training function. After training, also test on test data if given.\n",
    "\n",
    "    :param cfg_file: path to configuration yaml file\n",
    "    \"\"\"\n",
    "    cfg = load_config(cfg_file)\n",
    "\n",
    "    # set the random seed\n",
    "    set_seed(seed=cfg[\"pretraining\"].get(\"random_seed\", 42))\n",
    "\n",
    "    # load the data\n",
    "    pre_train_data, pre_dev_data, pre_test_data, pre_src_vocab, pre_trg_vocab = load_data(\n",
    "        data_cfg=cfg[\"pretrained_data\"])\n",
    "\n",
    "    # build an encoder-decoder model\n",
    "    pretrained_model = build_model(cfg[\"model\"], src_vocab=pre_src_vocab, trg_vocab=pre_trg_vocab)\n",
    "\n",
    "    # for training management, e.g. early stopping and model selection\n",
    "    trainer = TrainManager(model=pretrained_model, config=cfg, training_key=\"pretraining\")\n",
    "\n",
    "    # store copy of original training config in model dir\n",
    "    shutil.copy2(cfg_file, trainer.model_dir+\"/config.yaml\")\n",
    "\n",
    "    # log all entries of config\n",
    "    log_cfg(cfg, trainer.logger)\n",
    "\n",
    "    log_data_info(train_data=pre_train_data, valid_data=pre_dev_data,\n",
    "                  test_data=pre_test_data, src_vocab=pre_src_vocab, trg_vocab=pre_trg_vocab,\n",
    "                  logging_function=trainer.logger.info)\n",
    "\n",
    "    trainer.logger.info(str(pretrained_model))\n",
    "\n",
    "    # store the vocabs\n",
    "    src_vocab_file = \"{}/src_vocab.txt\".format(cfg[\"pretraining\"][\"model_dir\"])\n",
    "    pre_src_vocab.to_file(src_vocab_file)\n",
    "    trg_vocab_file = \"{}/trg_vocab.txt\".format(cfg[\"pretraining\"][\"model_dir\"])\n",
    "    pre_trg_vocab.to_file(trg_vocab_file)\n",
    "\n",
    "    # train the model\n",
    "    trainer.train_and_validate(train_data=pre_train_data, valid_data=pre_dev_data)\n",
    "\n",
    "    # predict with the best model on validation and test\n",
    "    # (if test data is available)\n",
    "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
    "    output_name = \"{:08d}.hyps\".format(trainer.best_ckpt_iteration)\n",
    "    output_path = os.path.join(trainer.model_dir, output_name)\n",
    "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=trainer.logger,\n",
    "        key_training=\"pretraining\", key_data=\"pretrained_data\")\n",
    "    \n",
    "    # set the random seed\n",
    "    set_seed(seed=cfg[\"training\"].get(\"random_seed\", 42))\n",
    "\n",
    "    # load the data\n",
    "    train_data, dev_data, test_data, src_vocab, trg_vocab = load_data(\n",
    "        data_cfg=cfg[\"data\"])\n",
    "\n",
    "    # build an encoder-decoder model\n",
    "    model = build_pretrained_model(cfg[\"model\"], \n",
    "                                   pretrained_model=pretrained_model, \n",
    "                                   pretrained_src_vocab=pre_src_vocab,\n",
    "                                   src_vocab=src_vocab, \n",
    "                                   trg_vocab=trg_vocab)\n",
    "\n",
    "    # for training management, e.g. early stopping and model selection\n",
    "    trainer = TrainManager(model=model, config=cfg, training_key=\"training\", logger=trainer.logger)\n",
    "\n",
    "    # store copy of original training config in model dir\n",
    "    shutil.copy2(cfg_file, trainer.model_dir+\"/config.yaml\")\n",
    "\n",
    "    # log all entries of config\n",
    "    log_cfg(cfg, trainer.logger)\n",
    "\n",
    "    log_data_info(train_data=train_data, valid_data=dev_data,\n",
    "                  test_data=test_data, src_vocab=src_vocab, trg_vocab=trg_vocab,\n",
    "                  logging_function=trainer.logger.info)\n",
    "\n",
    "    trainer.logger.info(str(model))\n",
    "\n",
    "    # store the vocabs\n",
    "    src_vocab_file = \"{}/src_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    src_vocab.to_file(src_vocab_file)\n",
    "    trg_vocab_file = \"{}/trg_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    trg_vocab.to_file(trg_vocab_file)\n",
    "\n",
    "    # train the model\n",
    "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
    "\n",
    "    # predict with the best model on validation and test\n",
    "    # (if test data is available)\n",
    "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
    "    output_name = \"{:08d}.hyps\".format(trainer.best_ckpt_iteration)\n",
    "    output_path = os.path.join(trainer.model_dir, output_name)\n",
    "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=trainer.logger,\n",
    "        key_training=\"training\", key_data=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "pretrained_data: # specify your data here\n",
    "    src: {pretrained_lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {pretrained_lang_tgt}                       # trg language\n",
    "    train: {pretrained_train_path}     # training data\n",
    "    dev: {pretrained_dev_path}         # development data for validation\n",
    "    test: {pretrained_test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 1                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: False                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "pretraining:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0002           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.00001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 1                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: False                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_config = config.format(lang_src='es', lang_tgt='shp', \n",
    "                    train_path=os.path.join('data/translate/preprocessed/Religioso/word', 'train'),\n",
    "                    test_path=os.path.join('data/translate/preprocessed/Religioso/word', 'test'),\n",
    "                    dev_path=os.path.join('data/translate/preprocessed/Religioso/word', 'valid'),\n",
    "                    pretrained_lang_src='es', pretrained_lang_tgt='shp', \n",
    "                    pretrained_train_path=os.path.join('data/translate/preprocessed/Educativo/word', 'train'),\n",
    "                    pretrained_test_path=os.path.join('data/translate/preprocessed/Educativo/word', 'test'),\n",
    "                    pretrained_dev_path=os.path.join('data/translate/preprocessed/Educativo/word', 'valid'),\n",
    "                    level='word',\n",
    "                    emb_size=2,\n",
    "                    hidden_size=5,\n",
    "                    val_freq=2,\n",
    "                    model_dir=os.path.join('results/temp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\"),'w') as f:\n",
    "    f.write(f_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-26 17:44:34,454 Hello! This is Joey-NMT.\n",
      "2020-02-26 17:44:35,590 Total params: 31891\n",
      "2020-02-26 17:44:35,591 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-26 17:44:35,593 cfg.name                           : my_experiment\n",
      "2020-02-26 17:44:35,594 cfg.data.src                       : es\n",
      "2020-02-26 17:44:35,594 cfg.data.trg                       : shp\n",
      "2020-02-26 17:44:35,595 cfg.data.train                     : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-26 17:44:35,595 cfg.data.dev                       : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-26 17:44:35,596 cfg.data.test                      : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-26 17:44:35,597 cfg.data.level                     : word\n",
      "2020-02-26 17:44:35,597 cfg.data.lowercase                 : True\n",
      "2020-02-26 17:44:35,598 cfg.data.max_sent_length           : 150\n",
      "2020-02-26 17:44:35,598 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-26 17:44:35,599 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-26 17:44:35,599 cfg.pretrained_data.src            : es\n",
      "2020-02-26 17:44:35,600 cfg.pretrained_data.trg            : shp\n",
      "2020-02-26 17:44:35,601 cfg.pretrained_data.train          : data/translate/preprocessed/Educativo/word/train\n",
      "2020-02-26 17:44:35,601 cfg.pretrained_data.dev            : data/translate/preprocessed/Educativo/word/valid\n",
      "2020-02-26 17:44:35,602 cfg.pretrained_data.test           : data/translate/preprocessed/Educativo/word/test\n",
      "2020-02-26 17:44:35,602 cfg.pretrained_data.level          : word\n",
      "2020-02-26 17:44:35,603 cfg.pretrained_data.lowercase      : True\n",
      "2020-02-26 17:44:35,604 cfg.pretrained_data.max_sent_length : 150\n",
      "2020-02-26 17:44:35,604 cfg.pretrained_data.src_voc_min_freq : 1\n",
      "2020-02-26 17:44:35,605 cfg.pretrained_data.trg_voc_min_freq : 1\n",
      "2020-02-26 17:44:35,605 cfg.testing.beam_size              : 5\n",
      "2020-02-26 17:44:35,606 cfg.testing.alpha                  : 1.0\n",
      "2020-02-26 17:44:35,607 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-26 17:44:35,607 cfg.training.reset_scheduler       : False\n",
      "2020-02-26 17:44:35,608 cfg.training.reset_optimizer       : False\n",
      "2020-02-26 17:44:35,608 cfg.training.random_seed           : 42\n",
      "2020-02-26 17:44:35,609 cfg.training.optimizer             : adam\n",
      "2020-02-26 17:44:35,610 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-26 17:44:35,610 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-26 17:44:35,611 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-26 17:44:35,611 cfg.training.weight_decay          : 0.0\n",
      "2020-02-26 17:44:35,612 cfg.training.batch_size            : 48\n",
      "2020-02-26 17:44:35,613 cfg.training.batch_type            : sentence\n",
      "2020-02-26 17:44:35,613 cfg.training.eval_batch_size       : 10\n",
      "2020-02-26 17:44:35,614 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-26 17:44:35,614 cfg.training.batch_multiplier      : 1\n",
      "2020-02-26 17:44:35,615 cfg.training.scheduling            : plateau\n",
      "2020-02-26 17:44:35,616 cfg.training.patience              : 600\n",
      "2020-02-26 17:44:35,616 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-26 17:44:35,617 cfg.training.epochs                : 1\n",
      "2020-02-26 17:44:35,617 cfg.training.validation_freq       : 2\n",
      "2020-02-26 17:44:35,618 cfg.training.logging_freq          : 1000\n",
      "2020-02-26 17:44:35,619 cfg.training.eval_metric           : bleu\n",
      "2020-02-26 17:44:35,619 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-26 17:44:35,620 cfg.training.model_dir             : results/temp\n",
      "2020-02-26 17:44:35,620 cfg.training.overwrite             : True\n",
      "2020-02-26 17:44:35,621 cfg.training.shuffle               : True\n",
      "2020-02-26 17:44:35,622 cfg.training.use_cuda              : False\n",
      "2020-02-26 17:44:35,622 cfg.training.max_output_length     : 60\n",
      "2020-02-26 17:44:35,623 cfg.training.print_valid_sents     : []\n",
      "2020-02-26 17:44:35,623 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-26 17:44:35,624 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-26 17:44:35,624 cfg.pretraining.reset_best_ckpt    : False\n",
      "2020-02-26 17:44:35,625 cfg.pretraining.reset_scheduler    : False\n",
      "2020-02-26 17:44:35,626 cfg.pretraining.reset_optimizer    : False\n",
      "2020-02-26 17:44:35,626 cfg.pretraining.random_seed        : 42\n",
      "2020-02-26 17:44:35,627 cfg.pretraining.optimizer          : adam\n",
      "2020-02-26 17:44:35,627 cfg.pretraining.learning_rate      : 0.0002\n",
      "2020-02-26 17:44:35,628 cfg.pretraining.learning_rate_min  : 1e-05\n",
      "2020-02-26 17:44:35,629 cfg.pretraining.clip_grad_val      : 1.0\n",
      "2020-02-26 17:44:35,629 cfg.pretraining.weight_decay       : 0.0\n",
      "2020-02-26 17:44:35,630 cfg.pretraining.batch_size         : 48\n",
      "2020-02-26 17:44:35,630 cfg.pretraining.batch_type         : sentence\n",
      "2020-02-26 17:44:35,631 cfg.pretraining.eval_batch_size    : 10\n",
      "2020-02-26 17:44:35,631 cfg.pretraining.eval_batch_type    : sentence\n",
      "2020-02-26 17:44:35,632 cfg.pretraining.batch_multiplier   : 1\n",
      "2020-02-26 17:44:35,633 cfg.pretraining.scheduling         : plateau\n",
      "2020-02-26 17:44:35,633 cfg.pretraining.patience           : 600\n",
      "2020-02-26 17:44:35,634 cfg.pretraining.decrease_factor    : 0.5\n",
      "2020-02-26 17:44:35,634 cfg.pretraining.epochs             : 1\n",
      "2020-02-26 17:44:35,635 cfg.pretraining.validation_freq    : 2\n",
      "2020-02-26 17:44:35,635 cfg.pretraining.logging_freq       : 1000\n",
      "2020-02-26 17:44:35,636 cfg.pretraining.eval_metric        : bleu\n",
      "2020-02-26 17:44:35,637 cfg.pretraining.early_stopping_metric : eval_metric\n",
      "2020-02-26 17:44:35,637 cfg.pretraining.model_dir          : results/temp\n",
      "2020-02-26 17:44:35,638 cfg.pretraining.overwrite          : True\n",
      "2020-02-26 17:44:35,638 cfg.pretraining.shuffle            : True\n",
      "2020-02-26 17:44:35,639 cfg.pretraining.use_cuda           : False\n",
      "2020-02-26 17:44:35,639 cfg.pretraining.max_output_length  : 60\n",
      "2020-02-26 17:44:35,640 cfg.pretraining.print_valid_sents  : []\n",
      "2020-02-26 17:44:35,641 cfg.pretraining.keep_last_ckpts    : 3\n",
      "2020-02-26 17:44:35,641 cfg.pretraining.label_smoothing    : 0.0\n",
      "2020-02-26 17:44:35,642 cfg.model.initializer              : xavier\n",
      "2020-02-26 17:44:35,642 cfg.model.init_weight              : 0.01\n",
      "2020-02-26 17:44:35,643 cfg.model.init_gain                : 1.0\n",
      "2020-02-26 17:44:35,644 cfg.model.bias_initializer         : zeros\n",
      "2020-02-26 17:44:35,644 cfg.model.embed_initializer        : normal\n",
      "2020-02-26 17:44:35,645 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-26 17:44:35,645 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-26 17:44:35,646 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-26 17:44:35,646 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-26 17:44:35,647 cfg.model.tied_embeddings          : False\n",
      "2020-02-26 17:44:35,647 cfg.model.tied_softmax             : False\n",
      "2020-02-26 17:44:35,648 cfg.model.encoder.type             : recurrent\n",
      "2020-02-26 17:44:35,649 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-26 17:44:35,649 cfg.model.encoder.embeddings.embedding_dim : 2\n",
      "2020-02-26 17:44:35,650 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-26 17:44:35,650 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-26 17:44:35,651 cfg.model.encoder.hidden_size      : 5\n",
      "2020-02-26 17:44:35,652 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-26 17:44:35,652 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-26 17:44:35,653 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-26 17:44:35,653 cfg.model.encoder.freeze           : False\n",
      "2020-02-26 17:44:35,654 cfg.model.decoder.type             : recurrent\n",
      "2020-02-26 17:44:35,655 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-26 17:44:35,655 cfg.model.decoder.embeddings.embedding_dim : 2\n",
      "2020-02-26 17:44:35,656 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-26 17:44:35,657 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-26 17:44:35,657 cfg.model.decoder.hidden_size      : 5\n",
      "2020-02-26 17:44:35,658 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-26 17:44:35,658 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-26 17:44:35,659 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-26 17:44:35,659 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-26 17:44:35,660 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-26 17:44:35,660 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-26 17:44:35,661 cfg.model.decoder.freeze           : False\n",
      "2020-02-26 17:44:35,662 Data set sizes: \n",
      "\ttrain 3979,\n",
      "\tvalid 498,\n",
      "\ttest 498\n",
      "2020-02-26 17:44:35,662 First training example:\n",
      "\t[SRC] nenora non aki kai tanakin westiorabo kirika wishamaxon onankiakana ixon\n",
      "\t[TRG] se evaluará mediante problemas propuestos en una hoja de aplicación individual\n",
      "2020-02-26 17:44:35,663 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) itan (5) ja (6) kopi (7) ? (8) ¿ (9) yoyo\n",
      "2020-02-26 17:44:35,663 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) de (5) y (6) que (7) los (8) la (9) en\n",
      "2020-02-26 17:44:35,664 Number of Src words (types): 4056\n",
      "2020-02-26 17:44:35,664 Number of Trg words (types): 3207\n",
      "2020-02-26 17:44:35,665 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(2, 5, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(7, 5, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=2, vocab_size=4056),\n",
      "\ttrg_embed=Embeddings(embedding_dim=2, vocab_size=3207))\n",
      "2020-02-26 17:44:35,689 EPOCH 1\n",
      "2020-02-26 17:44:38,853 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-26 17:44:38,854 Saving new checkpoint.\n",
      "2020-02-26 17:44:38,857 Validation result (greedy) at epoch   1, step        2: bleu:   0.00, loss: 39097.5117, ppl: 3206.6890, duration: 3.0416s\n",
      "2020-02-26 17:44:42,032 Validation result (greedy) at epoch   1, step        4: bleu:   0.00, loss: 39097.0977, ppl: 3206.4138, duration: 3.0510s\n",
      "2020-02-26 17:44:45,215 Validation result (greedy) at epoch   1, step        6: bleu:   0.00, loss: 39096.5742, ppl: 3206.0684, duration: 3.0717s\n",
      "2020-02-26 17:44:48,375 Validation result (greedy) at epoch   1, step        8: bleu:   0.00, loss: 39095.9648, ppl: 3205.6648, duration: 3.0582s\n",
      "2020-02-26 17:44:51,535 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 39095.2500, ppl: 3205.1909, duration: 3.0468s\n",
      "2020-02-26 17:44:55,214 Validation result (greedy) at epoch   1, step       12: bleu:   0.00, loss: 39094.4492, ppl: 3204.6621, duration: 3.4825s\n",
      "2020-02-26 17:44:58,563 Validation result (greedy) at epoch   1, step       14: bleu:   0.00, loss: 39093.5078, ppl: 3204.0388, duration: 3.1690s\n",
      "2020-02-26 17:45:01,881 Validation result (greedy) at epoch   1, step       16: bleu:   0.00, loss: 39092.4375, ppl: 3203.3330, duration: 3.1770s\n",
      "2020-02-26 17:45:05,142 Validation result (greedy) at epoch   1, step       18: bleu:   0.00, loss: 39091.3320, ppl: 3202.5999, duration: 3.1658s\n",
      "2020-02-26 17:45:08,438 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 39090.2031, ppl: 3201.8547, duration: 3.2165s\n",
      "2020-02-26 17:45:12,286 Validation result (greedy) at epoch   1, step       22: bleu:   0.00, loss: 39088.8945, ppl: 3200.9875, duration: 3.7404s\n",
      "2020-02-26 17:45:15,591 Validation result (greedy) at epoch   1, step       24: bleu:   0.00, loss: 39087.3945, ppl: 3199.9988, duration: 3.1867s\n",
      "2020-02-26 17:45:18,895 Validation result (greedy) at epoch   1, step       26: bleu:   0.00, loss: 39085.6328, ppl: 3198.8330, duration: 3.1528s\n",
      "2020-02-26 17:45:22,262 Validation result (greedy) at epoch   1, step       28: bleu:   0.00, loss: 39083.6055, ppl: 3197.4941, duration: 3.2312s\n",
      "2020-02-26 17:45:25,488 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 39081.5312, ppl: 3196.1252, duration: 3.1431s\n",
      "2020-02-26 17:45:28,846 Validation result (greedy) at epoch   1, step       32: bleu:   0.00, loss: 39079.4414, ppl: 3194.7478, duration: 3.2912s\n",
      "2020-02-26 17:45:32,547 Validation result (greedy) at epoch   1, step       34: bleu:   0.00, loss: 39077.3125, ppl: 3193.3435, duration: 3.6315s\n",
      "2020-02-26 17:45:35,809 Validation result (greedy) at epoch   1, step       36: bleu:   0.00, loss: 39074.9531, ppl: 3191.7878, duration: 3.1586s\n",
      "2020-02-26 17:45:39,069 Validation result (greedy) at epoch   1, step       38: bleu:   0.00, loss: 39072.3320, ppl: 3190.0593, duration: 3.1367s\n",
      "2020-02-26 17:45:42,235 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 39069.5703, ppl: 3188.2405, duration: 3.0438s\n",
      "2020-02-26 17:45:45,370 Validation result (greedy) at epoch   1, step       42: bleu:   0.00, loss: 39066.5273, ppl: 3186.2405, duration: 3.0302s\n",
      "2020-02-26 17:45:48,490 Validation result (greedy) at epoch   1, step       44: bleu:   0.00, loss: 39063.3750, ppl: 3184.1658, duration: 3.0272s\n",
      "2020-02-26 17:45:52,685 Validation result (greedy) at epoch   1, step       46: bleu:   0.00, loss: 39060.2656, ppl: 3182.1228, duration: 4.0887s\n",
      "2020-02-26 17:45:55,958 Validation result (greedy) at epoch   1, step       48: bleu:   0.00, loss: 39057.0234, ppl: 3179.9932, duration: 3.1599s\n",
      "2020-02-26 17:45:59,171 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 39053.3125, ppl: 3177.5559, duration: 3.1204s\n",
      "2020-02-26 17:46:02,321 Validation result (greedy) at epoch   1, step       52: bleu:   0.00, loss: 39049.4336, ppl: 3175.0144, duration: 3.0706s\n",
      "2020-02-26 17:46:05,539 Validation result (greedy) at epoch   1, step       54: bleu:   0.00, loss: 39044.9336, ppl: 3172.0635, duration: 3.0325s\n",
      "2020-02-26 17:46:08,444 Validation result (greedy) at epoch   1, step       56: bleu:   0.00, loss: 39039.7734, ppl: 3168.6863, duration: 2.7656s\n",
      "2020-02-26 17:46:11,008 Validation result (greedy) at epoch   1, step       58: bleu:   0.00, loss: 39034.8086, ppl: 3165.4395, duration: 2.5091s\n",
      "2020-02-26 17:46:13,609 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 39029.7148, ppl: 3162.1113, duration: 2.4858s\n",
      "2020-02-26 17:46:15,736 Validation result (greedy) at epoch   1, step       62: bleu:   0.00, loss: 39023.9609, ppl: 3158.3562, duration: 1.9908s\n",
      "2020-02-26 17:46:17,532 Validation result (greedy) at epoch   1, step       64: bleu:   0.00, loss: 39017.9062, ppl: 3154.4099, duration: 1.6749s\n",
      "2020-02-26 17:46:19,219 Validation result (greedy) at epoch   1, step       66: bleu:   0.00, loss: 39011.9453, ppl: 3150.5315, duration: 1.6174s\n",
      "2020-02-26 17:46:20,892 Validation result (greedy) at epoch   1, step       68: bleu:   0.00, loss: 39005.9922, ppl: 3146.6611, duration: 1.5811s\n",
      "2020-02-26 17:46:22,469 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 38999.6836, ppl: 3142.5645, duration: 1.4258s\n",
      "2020-02-26 17:46:23,920 Validation result (greedy) at epoch   1, step       72: bleu:   0.00, loss: 38992.6797, ppl: 3138.0244, duration: 1.2696s\n",
      "2020-02-26 17:46:25,466 Validation result (greedy) at epoch   1, step       74: bleu:   0.00, loss: 38985.0820, ppl: 3133.1052, duration: 1.3946s\n",
      "2020-02-26 17:46:26,759 Validation result (greedy) at epoch   1, step       76: bleu:   0.00, loss: 38977.4336, ppl: 3128.1611, duration: 1.1708s\n",
      "2020-02-26 17:46:28,356 Validation result (greedy) at epoch   1, step       78: bleu:   0.00, loss: 38969.8281, ppl: 3123.2515, duration: 1.4772s\n",
      "2020-02-26 17:46:29,731 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 38962.1992, ppl: 3118.3350, duration: 1.2564s\n",
      "2020-02-26 17:46:30,923 Validation result (greedy) at epoch   1, step       82: bleu:   0.00, loss: 38954.6211, ppl: 3113.4585, duration: 1.0850s\n",
      "2020-02-26 17:46:30,971 Epoch   1: total training loss 6565.01\n",
      "2020-02-26 17:46:30,972 Training ended after   1 epochs.\n",
      "2020-02-26 17:46:30,973 Best validation result (greedy) at step        2:   0.00 eval_metric.\n",
      "2020-02-26 17:46:39,184  dev bleu:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-02-26 17:46:39,185 Translations saved to: results/temp/00000002.hyps.dev\n",
      "2020-02-26 17:46:46,960 test bleu:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-02-26 17:46:46,962 Translations saved to: results/temp/00000002.hyps.test\n",
      "2020-02-26 17:46:49,175 Total params: 87379\n",
      "2020-02-26 17:46:49,175 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-26 17:46:49,177 cfg.name                           : my_experiment\n",
      "2020-02-26 17:46:49,178 cfg.data.src                       : es\n",
      "2020-02-26 17:46:49,178 cfg.data.trg                       : shp\n",
      "2020-02-26 17:46:49,179 cfg.data.train                     : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-26 17:46:49,180 cfg.data.dev                       : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-26 17:46:49,180 cfg.data.test                      : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-26 17:46:49,181 cfg.data.level                     : word\n",
      "2020-02-26 17:46:49,182 cfg.data.lowercase                 : True\n",
      "2020-02-26 17:46:49,183 cfg.data.max_sent_length           : 150\n",
      "2020-02-26 17:46:49,183 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-26 17:46:49,184 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-26 17:46:49,185 cfg.pretrained_data.src            : es\n",
      "2020-02-26 17:46:49,185 cfg.pretrained_data.trg            : shp\n",
      "2020-02-26 17:46:49,186 cfg.pretrained_data.train          : data/translate/preprocessed/Educativo/word/train\n",
      "2020-02-26 17:46:49,189 cfg.pretrained_data.dev            : data/translate/preprocessed/Educativo/word/valid\n",
      "2020-02-26 17:46:49,190 cfg.pretrained_data.test           : data/translate/preprocessed/Educativo/word/test\n",
      "2020-02-26 17:46:49,190 cfg.pretrained_data.level          : word\n",
      "2020-02-26 17:46:49,191 cfg.pretrained_data.lowercase      : True\n",
      "2020-02-26 17:46:49,192 cfg.pretrained_data.max_sent_length : 150\n",
      "2020-02-26 17:46:49,192 cfg.pretrained_data.src_voc_min_freq : 1\n",
      "2020-02-26 17:46:49,193 cfg.pretrained_data.trg_voc_min_freq : 1\n",
      "2020-02-26 17:46:49,194 cfg.testing.beam_size              : 5\n",
      "2020-02-26 17:46:49,194 cfg.testing.alpha                  : 1.0\n",
      "2020-02-26 17:46:49,195 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-26 17:46:49,196 cfg.training.reset_scheduler       : False\n",
      "2020-02-26 17:46:49,196 cfg.training.reset_optimizer       : False\n",
      "2020-02-26 17:46:49,197 cfg.training.random_seed           : 42\n",
      "2020-02-26 17:46:49,198 cfg.training.optimizer             : adam\n",
      "2020-02-26 17:46:49,198 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-26 17:46:49,199 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-26 17:46:49,200 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-26 17:46:49,200 cfg.training.weight_decay          : 0.0\n",
      "2020-02-26 17:46:49,201 cfg.training.batch_size            : 48\n",
      "2020-02-26 17:46:49,202 cfg.training.batch_type            : sentence\n",
      "2020-02-26 17:46:49,202 cfg.training.eval_batch_size       : 10\n",
      "2020-02-26 17:46:49,206 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-26 17:46:49,207 cfg.training.batch_multiplier      : 1\n",
      "2020-02-26 17:46:49,208 cfg.training.scheduling            : plateau\n",
      "2020-02-26 17:46:49,208 cfg.training.patience              : 600\n",
      "2020-02-26 17:46:49,209 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-26 17:46:49,210 cfg.training.epochs                : 1\n",
      "2020-02-26 17:46:49,211 cfg.training.validation_freq       : 2\n",
      "2020-02-26 17:46:49,211 cfg.training.logging_freq          : 1000\n",
      "2020-02-26 17:46:49,212 cfg.training.eval_metric           : bleu\n",
      "2020-02-26 17:46:49,213 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-26 17:46:49,213 cfg.training.model_dir             : results/temp\n",
      "2020-02-26 17:46:49,214 cfg.training.overwrite             : True\n",
      "2020-02-26 17:46:49,216 cfg.training.shuffle               : True\n",
      "2020-02-26 17:46:49,216 cfg.training.use_cuda              : False\n",
      "2020-02-26 17:46:49,217 cfg.training.max_output_length     : 60\n",
      "2020-02-26 17:46:49,218 cfg.training.print_valid_sents     : []\n",
      "2020-02-26 17:46:49,219 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-26 17:46:49,220 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-26 17:46:49,220 cfg.pretraining.reset_best_ckpt    : False\n",
      "2020-02-26 17:46:49,221 cfg.pretraining.reset_scheduler    : False\n",
      "2020-02-26 17:46:49,222 cfg.pretraining.reset_optimizer    : False\n",
      "2020-02-26 17:46:49,222 cfg.pretraining.random_seed        : 42\n",
      "2020-02-26 17:46:49,223 cfg.pretraining.optimizer          : adam\n",
      "2020-02-26 17:46:49,223 cfg.pretraining.learning_rate      : 0.0002\n",
      "2020-02-26 17:46:49,224 cfg.pretraining.learning_rate_min  : 1e-05\n",
      "2020-02-26 17:46:49,225 cfg.pretraining.clip_grad_val      : 1.0\n",
      "2020-02-26 17:46:49,225 cfg.pretraining.weight_decay       : 0.0\n",
      "2020-02-26 17:46:49,226 cfg.pretraining.batch_size         : 48\n",
      "2020-02-26 17:46:49,227 cfg.pretraining.batch_type         : sentence\n",
      "2020-02-26 17:46:49,227 cfg.pretraining.eval_batch_size    : 10\n",
      "2020-02-26 17:46:49,230 cfg.pretraining.eval_batch_type    : sentence\n",
      "2020-02-26 17:46:49,230 cfg.pretraining.batch_multiplier   : 1\n",
      "2020-02-26 17:46:49,231 cfg.pretraining.scheduling         : plateau\n",
      "2020-02-26 17:46:49,232 cfg.pretraining.patience           : 600\n",
      "2020-02-26 17:46:49,232 cfg.pretraining.decrease_factor    : 0.5\n",
      "2020-02-26 17:46:49,233 cfg.pretraining.epochs             : 1\n",
      "2020-02-26 17:46:49,234 cfg.pretraining.validation_freq    : 2\n",
      "2020-02-26 17:46:49,234 cfg.pretraining.logging_freq       : 1000\n",
      "2020-02-26 17:46:49,235 cfg.pretraining.eval_metric        : bleu\n",
      "2020-02-26 17:46:49,236 cfg.pretraining.early_stopping_metric : eval_metric\n",
      "2020-02-26 17:46:49,236 cfg.pretraining.model_dir          : results/temp\n",
      "2020-02-26 17:46:49,237 cfg.pretraining.overwrite          : True\n",
      "2020-02-26 17:46:49,237 cfg.pretraining.shuffle            : True\n",
      "2020-02-26 17:46:49,239 cfg.pretraining.use_cuda           : False\n",
      "2020-02-26 17:46:49,240 cfg.pretraining.max_output_length  : 60\n",
      "2020-02-26 17:46:49,241 cfg.pretraining.print_valid_sents  : []\n",
      "2020-02-26 17:46:49,241 cfg.pretraining.keep_last_ckpts    : 3\n",
      "2020-02-26 17:46:49,242 cfg.pretraining.label_smoothing    : 0.0\n",
      "2020-02-26 17:46:49,243 cfg.model.initializer              : xavier\n",
      "2020-02-26 17:46:49,243 cfg.model.init_weight              : 0.01\n",
      "2020-02-26 17:46:49,244 cfg.model.init_gain                : 1.0\n",
      "2020-02-26 17:46:49,244 cfg.model.bias_initializer         : zeros\n",
      "2020-02-26 17:46:49,246 cfg.model.embed_initializer        : normal\n",
      "2020-02-26 17:46:49,246 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-26 17:46:49,247 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-26 17:46:49,248 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-26 17:46:49,248 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-26 17:46:49,249 cfg.model.tied_embeddings          : False\n",
      "2020-02-26 17:46:49,249 cfg.model.tied_softmax             : False\n",
      "2020-02-26 17:46:49,250 cfg.model.encoder.type             : recurrent\n",
      "2020-02-26 17:46:49,251 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-26 17:46:49,252 cfg.model.encoder.embeddings.embedding_dim : 2\n",
      "2020-02-26 17:46:49,253 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-26 17:46:49,253 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-26 17:46:49,254 cfg.model.encoder.hidden_size      : 5\n",
      "2020-02-26 17:46:49,254 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-26 17:46:49,256 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-26 17:46:49,256 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-26 17:46:49,257 cfg.model.encoder.freeze           : False\n",
      "2020-02-26 17:46:49,257 cfg.model.decoder.type             : recurrent\n",
      "2020-02-26 17:46:49,258 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-26 17:46:49,259 cfg.model.decoder.embeddings.embedding_dim : 2\n",
      "2020-02-26 17:46:49,260 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-26 17:46:49,261 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-26 17:46:49,261 cfg.model.decoder.hidden_size      : 5\n",
      "2020-02-26 17:46:49,262 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-26 17:46:49,262 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-26 17:46:49,263 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-26 17:46:49,264 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-26 17:46:49,265 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-26 17:46:49,265 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-26 17:46:49,266 cfg.model.decoder.freeze           : False\n",
      "2020-02-26 17:46:49,266 Data set sizes: \n",
      "\ttrain 5996,\n",
      "\tvalid 749,\n",
      "\ttest 749\n",
      "2020-02-26 17:46:49,267 First training example:\n",
      "\t[SRC] ikaxbi ja non ibon joira jawetianbi keyóyamai iki\n",
      "\t[TRG] pero la palabra del señor permanece eternamente esta palabra es el evangelio que se les ha anunciado a ustedes\n",
      "2020-02-26 17:46:49,267 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) iki (5) ja (6) jato (7) mato (8) ea (9) en\n",
      "2020-02-26 17:46:49,269 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) que (5) de (6) y (7) a (8) la (9) el\n",
      "2020-02-26 17:46:49,270 Number of Src words (types): 11507\n",
      "2020-02-26 17:46:49,270 Number of Trg words (types): 9005\n",
      "2020-02-26 17:46:49,271 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(2, 5, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(7, 5, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=2, vocab_size=11507),\n",
      "\ttrg_embed=Embeddings(embedding_dim=2, vocab_size=9005))\n",
      "2020-02-26 17:46:49,287 EPOCH 1\n",
      "2020-02-26 17:46:55,984 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-26 17:46:55,985 Saving new checkpoint.\n",
      "2020-02-26 17:46:55,988 Validation result (greedy) at epoch   1, step        2: bleu:   0.00, loss: 104958.9922, ppl: 9004.5986, duration: 6.3867s\n",
      "2020-02-26 17:47:01,895 Validation result (greedy) at epoch   1, step        4: bleu:   0.00, loss: 104956.1016, ppl: 9002.3398, duration: 5.6601s\n",
      "2020-02-26 17:47:07,791 Validation result (greedy) at epoch   1, step        6: bleu:   0.00, loss: 104952.2188, ppl: 8999.3105, duration: 5.6594s\n",
      "2020-02-26 17:47:13,710 Validation result (greedy) at epoch   1, step        8: bleu:   0.00, loss: 104947.4844, ppl: 8995.6123, duration: 5.6642s\n",
      "2020-02-26 17:47:19,675 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 104941.5547, ppl: 8990.9893, duration: 5.7220s\n",
      "2020-02-26 17:47:26,165 Validation result (greedy) at epoch   1, step       12: bleu:   0.00, loss: 104934.3672, ppl: 8985.3828, duration: 6.1371s\n",
      "2020-02-26 17:47:32,270 Validation result (greedy) at epoch   1, step       14: bleu:   0.00, loss: 104925.5156, ppl: 8978.4873, duration: 5.7959s\n",
      "2020-02-26 17:47:38,054 Validation result (greedy) at epoch   1, step       16: bleu:   0.00, loss: 104916.2344, ppl: 8971.2637, duration: 5.6429s\n",
      "2020-02-26 17:47:42,973 Validation result (greedy) at epoch   1, step       18: bleu:   0.00, loss: 104905.2031, ppl: 8962.6777, duration: 4.6434s\n",
      "2020-02-26 17:47:46,763 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 104892.5469, ppl: 8952.8457, duration: 3.5218s\n",
      "2020-02-26 17:47:50,069 Validation result (greedy) at epoch   1, step       22: bleu:   0.00, loss: 104877.8672, ppl: 8941.4453, duration: 3.0452s\n",
      "2020-02-26 17:47:53,460 Validation result (greedy) at epoch   1, step       24: bleu:   0.00, loss: 104861.7812, ppl: 8928.9795, duration: 3.1472s\n",
      "2020-02-26 17:47:56,195 Validation result (greedy) at epoch   1, step       26: bleu:   0.00, loss: 104843.5469, ppl: 8914.8633, duration: 2.5185s\n",
      "2020-02-26 17:47:59,075 Validation result (greedy) at epoch   1, step       28: bleu:   0.00, loss: 104823.7500, ppl: 8899.5732, duration: 2.6944s\n",
      "2020-02-26 17:48:01,770 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 104801.7969, ppl: 8882.6396, duration: 2.4443s\n",
      "2020-02-26 17:48:04,427 Validation result (greedy) at epoch   1, step       32: bleu:   0.00, loss: 104776.9375, ppl: 8863.4990, duration: 2.3944s\n",
      "2020-02-26 17:48:07,257 Validation result (greedy) at epoch   1, step       34: bleu:   0.00, loss: 104750.1328, ppl: 8842.9150, duration: 2.6465s\n",
      "2020-02-26 17:48:10,316 Validation result (greedy) at epoch   1, step       36: bleu:   0.00, loss: 104720.8516, ppl: 8820.4775, duration: 2.7901s\n",
      "2020-02-26 17:48:13,035 Validation result (greedy) at epoch   1, step       38: bleu:   0.00, loss: 104687.8906, ppl: 8795.2939, duration: 2.4511s\n",
      "2020-02-26 17:48:15,699 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 104652.2031, ppl: 8768.1016, duration: 2.3949s\n",
      "2020-02-26 17:48:18,547 Validation result (greedy) at epoch   1, step       42: bleu:   0.00, loss: 104613.2500, ppl: 8738.5254, duration: 2.5891s\n",
      "2020-02-26 17:48:21,225 Validation result (greedy) at epoch   1, step       44: bleu:   0.00, loss: 104570.8906, ppl: 8706.4736, duration: 2.3879s\n",
      "2020-02-26 17:48:23,859 Validation result (greedy) at epoch   1, step       46: bleu:   0.00, loss: 104524.0078, ppl: 8671.1328, duration: 2.3795s\n",
      "2020-02-26 17:48:26,603 Validation result (greedy) at epoch   1, step       48: bleu:   0.00, loss: 104472.8203, ppl: 8632.7158, duration: 2.3783s\n",
      "2020-02-26 17:48:29,172 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 104419.7344, ppl: 8593.0430, duration: 2.3781s\n",
      "2020-02-26 17:48:31,818 Validation result (greedy) at epoch   1, step       52: bleu:   0.00, loss: 104365.0938, ppl: 8552.4102, duration: 2.3853s\n",
      "2020-02-26 17:48:34,482 Validation result (greedy) at epoch   1, step       54: bleu:   0.00, loss: 104308.4844, ppl: 8510.5088, duration: 2.3702s\n",
      "2020-02-26 17:48:37,112 Validation result (greedy) at epoch   1, step       56: bleu:   0.00, loss: 104248.5234, ppl: 8466.3574, duration: 2.3721s\n",
      "2020-02-26 17:48:39,760 Validation result (greedy) at epoch   1, step       58: bleu:   0.00, loss: 104185.8828, ppl: 8420.4756, duration: 2.3736s\n",
      "2020-02-26 17:48:42,204 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 104124.4062, ppl: 8375.6816, duration: 2.3063s\n",
      "2020-02-26 17:48:44,731 Validation result (greedy) at epoch   1, step       62: bleu:   0.00, loss: 104060.4766, ppl: 8329.3613, duration: 2.2963s\n",
      "2020-02-26 17:48:47,315 Validation result (greedy) at epoch   1, step       64: bleu:   0.00, loss: 103993.4141, ppl: 8281.0391, duration: 2.2800s\n",
      "2020-02-26 17:48:49,932 Validation result (greedy) at epoch   1, step       66: bleu:   0.00, loss: 103919.7344, ppl: 8228.2793, duration: 2.2873s\n",
      "2020-02-26 17:48:52,501 Validation result (greedy) at epoch   1, step       68: bleu:   0.00, loss: 103841.2031, ppl: 8172.4102, duration: 2.2918s\n",
      "2020-02-26 17:48:55,022 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 103759.8594, ppl: 8114.9468, duration: 2.3053s\n",
      "2020-02-26 17:48:57,554 Validation result (greedy) at epoch   1, step       72: bleu:   0.00, loss: 103679.7578, ppl: 8058.7480, duration: 2.3074s\n",
      "2020-02-26 17:49:00,047 Validation result (greedy) at epoch   1, step       74: bleu:   0.00, loss: 103597.8906, ppl: 8001.7178, duration: 2.2935s\n",
      "2020-02-26 17:49:02,561 Validation result (greedy) at epoch   1, step       76: bleu:   0.00, loss: 103513.6875, ppl: 7943.4771, duration: 2.2880s\n",
      "2020-02-26 17:49:05,150 Validation result (greedy) at epoch   1, step       78: bleu:   0.00, loss: 103425.7031, ppl: 7883.0732, duration: 2.2830s\n",
      "2020-02-26 17:49:07,807 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 103337.2656, ppl: 7822.8311, duration: 2.4495s\n",
      "2020-02-26 17:49:10,540 Validation result (greedy) at epoch   1, step       82: bleu:   0.00, loss: 103244.8516, ppl: 7760.3618, duration: 2.4411s\n",
      "2020-02-26 17:49:13,336 Validation result (greedy) at epoch   1, step       84: bleu:   0.00, loss: 103145.1016, ppl: 7693.4961, duration: 2.3936s\n",
      "2020-02-26 17:49:15,956 Validation result (greedy) at epoch   1, step       86: bleu:   0.00, loss: 103040.1797, ppl: 7623.7886, duration: 2.3698s\n",
      "2020-02-26 17:49:18,644 Validation result (greedy) at epoch   1, step       88: bleu:   0.00, loss: 102938.3203, ppl: 7556.7158, duration: 2.4255s\n",
      "2020-02-26 17:49:21,329 Validation result (greedy) at epoch   1, step       90: bleu:   0.00, loss: 102831.9531, ppl: 7487.3052, duration: 2.3975s\n",
      "2020-02-26 17:49:23,995 Validation result (greedy) at epoch   1, step       92: bleu:   0.00, loss: 102725.1328, ppl: 7418.2422, duration: 2.3722s\n",
      "2020-02-26 17:49:26,601 Validation result (greedy) at epoch   1, step       94: bleu:   0.00, loss: 102620.2578, ppl: 7351.0566, duration: 2.4554s\n",
      "2020-02-26 17:49:29,785 Validation result (greedy) at epoch   1, step       96: bleu:   0.00, loss: 102513.4453, ppl: 7283.2500, duration: 2.8624s\n",
      "2020-02-26 17:49:32,623 Validation result (greedy) at epoch   1, step       98: bleu:   0.00, loss: 102411.4219, ppl: 7219.0703, duration: 2.6306s\n",
      "2020-02-26 17:49:35,353 Validation result (greedy) at epoch   1, step      100: bleu:   0.00, loss: 102306.8438, ppl: 7153.8726, duration: 2.4328s\n",
      "2020-02-26 17:49:38,286 Validation result (greedy) at epoch   1, step      102: bleu:   0.00, loss: 102203.5469, ppl: 7090.0552, duration: 2.6905s\n",
      "2020-02-26 17:49:41,119 Validation result (greedy) at epoch   1, step      104: bleu:   0.00, loss: 102094.5703, ppl: 7023.3364, duration: 2.5333s\n",
      "2020-02-26 17:49:43,790 Validation result (greedy) at epoch   1, step      106: bleu:   0.00, loss: 101985.9531, ppl: 6957.4712, duration: 2.3929s\n",
      "2020-02-26 17:49:46,487 Validation result (greedy) at epoch   1, step      108: bleu:   0.00, loss: 101882.0234, ppl: 6895.0244, duration: 2.4472s\n",
      "2020-02-26 17:49:49,185 Validation result (greedy) at epoch   1, step      110: bleu:   0.00, loss: 101776.7656, ppl: 6832.3496, duration: 2.4155s\n",
      "2020-02-26 17:49:51,780 Validation result (greedy) at epoch   1, step      112: bleu:   0.00, loss: 101676.1875, ppl: 6772.9893, duration: 2.4376s\n",
      "2020-02-26 17:49:54,535 Validation result (greedy) at epoch   1, step      114: bleu:   0.00, loss: 101573.9688, ppl: 6713.1968, duration: 2.4249s\n",
      "2020-02-26 17:49:57,520 Validation result (greedy) at epoch   1, step      116: bleu:   0.00, loss: 101469.0625, ppl: 6652.3774, duration: 2.7551s\n",
      "2020-02-26 17:50:00,189 Validation result (greedy) at epoch   1, step      118: bleu:   0.00, loss: 101372.8125, ppl: 6597.0591, duration: 2.4799s\n",
      "2020-02-26 17:50:02,952 Validation result (greedy) at epoch   1, step      120: bleu:   0.00, loss: 101268.1953, ppl: 6537.4604, duration: 2.3752s\n",
      "2020-02-26 17:50:05,542 Validation result (greedy) at epoch   1, step      122: bleu:   0.00, loss: 101165.4453, ppl: 6479.4443, duration: 2.3752s\n",
      "2020-02-26 17:50:08,231 Validation result (greedy) at epoch   1, step      124: bleu:   0.00, loss: 101064.3047, ppl: 6422.8379, duration: 2.4821s\n",
      "2020-02-26 17:50:08,344 Epoch   1: total training loss 17677.92\n",
      "2020-02-26 17:50:08,344 Training ended after   1 epochs.\n",
      "2020-02-26 17:50:08,345 Best validation result (greedy) at step        2:   0.00 eval_metric.\n",
      "2020-02-26 17:50:36,875  dev bleu:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-02-26 17:50:36,877 Translations saved to: results/temp/00000002.hyps.dev\n",
      "2020-02-26 17:51:03,510 test bleu:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-02-26 17:51:03,511 Translations saved to: results/temp/00000002.hyps.test\n"
     ]
    }
   ],
   "source": [
    "train(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
