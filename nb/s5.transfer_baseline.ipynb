{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘./data/transfer/preprocessed/splits.en/en-shp/word/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./nb/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./results/translate/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./joeynmt/joeynmt/.ipynb_checkpoints’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!find . -name .ipynb* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 500                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 30                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: True                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/transfer/preprocessed'\n",
    "datas = os.listdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/translate/data/transfer/preprocessed/splits.es/es-shp/char\n",
      "2020-02-27 17:56:00,305 Hello! This is Joey-NMT.\n",
      "2020-02-27 17:56:00.856830: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-02-27 17:56:00.856890: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-02-27 17:56:00.856902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2020-02-27 17:56:01,381 Total params: 12466180\n",
      "2020-02-27 17:56:01,381 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-27 17:56:03,161 cfg.name                           : my_experiment\n",
      "2020-02-27 17:56:03,162 cfg.data.src                       : es\n",
      "2020-02-27 17:56:03,162 cfg.data.trg                       : shp\n",
      "2020-02-27 17:56:03,162 cfg.data.train                     : data/transfer/preprocessed/splits.es/es-shp/char/train\n",
      "2020-02-27 17:56:03,162 cfg.data.dev                       : data/transfer/preprocessed/splits.es/es-shp/char/valid\n",
      "2020-02-27 17:56:03,162 cfg.data.test                      : data/transfer/preprocessed/splits.es/es-shp/char/test\n",
      "2020-02-27 17:56:03,162 cfg.data.level                     : char\n",
      "2020-02-27 17:56:03,162 cfg.data.lowercase                 : True\n",
      "2020-02-27 17:56:03,162 cfg.data.max_sent_length           : 150\n",
      "2020-02-27 17:56:03,162 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-27 17:56:03,162 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-27 17:56:03,162 cfg.testing.beam_size              : 5\n",
      "2020-02-27 17:56:03,162 cfg.testing.alpha                  : 1.0\n",
      "2020-02-27 17:56:03,162 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-27 17:56:03,162 cfg.training.reset_scheduler       : False\n",
      "2020-02-27 17:56:03,162 cfg.training.reset_optimizer       : False\n",
      "2020-02-27 17:56:03,162 cfg.training.random_seed           : 42\n",
      "2020-02-27 17:56:03,162 cfg.training.optimizer             : adam\n",
      "2020-02-27 17:56:03,162 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-27 17:56:03,162 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-27 17:56:03,162 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-27 17:56:03,162 cfg.training.weight_decay          : 0.0\n",
      "2020-02-27 17:56:03,162 cfg.training.batch_size            : 48\n",
      "2020-02-27 17:56:03,162 cfg.training.batch_type            : sentence\n",
      "2020-02-27 17:56:03,162 cfg.training.eval_batch_size       : 10\n",
      "2020-02-27 17:56:03,162 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-27 17:56:03,162 cfg.training.batch_multiplier      : 1\n",
      "2020-02-27 17:56:03,163 cfg.training.scheduling            : plateau\n",
      "2020-02-27 17:56:03,163 cfg.training.patience              : 500\n",
      "2020-02-27 17:56:03,163 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-27 17:56:03,163 cfg.training.epochs                : 30\n",
      "2020-02-27 17:56:03,163 cfg.training.validation_freq       : 10\n",
      "2020-02-27 17:56:03,163 cfg.training.logging_freq          : 1000\n",
      "2020-02-27 17:56:03,163 cfg.training.eval_metric           : bleu\n",
      "2020-02-27 17:56:03,163 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-27 17:56:03,163 cfg.training.model_dir             : results/transfer_baseline/es-shp_300_512/char\n",
      "2020-02-27 17:56:03,163 cfg.training.overwrite             : True\n",
      "2020-02-27 17:56:03,163 cfg.training.shuffle               : True\n",
      "2020-02-27 17:56:03,163 cfg.training.use_cuda              : True\n",
      "2020-02-27 17:56:03,163 cfg.training.max_output_length     : 60\n",
      "2020-02-27 17:56:03,163 cfg.training.print_valid_sents     : []\n",
      "2020-02-27 17:56:03,163 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-27 17:56:03,163 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-27 17:56:03,163 cfg.model.initializer              : xavier\n",
      "2020-02-27 17:56:03,163 cfg.model.init_weight              : 0.01\n",
      "2020-02-27 17:56:03,163 cfg.model.init_gain                : 1.0\n",
      "2020-02-27 17:56:03,163 cfg.model.bias_initializer         : zeros\n",
      "2020-02-27 17:56:03,163 cfg.model.embed_initializer        : normal\n",
      "2020-02-27 17:56:03,163 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-27 17:56:03,163 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-27 17:56:03,163 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-27 17:56:03,163 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-27 17:56:03,163 cfg.model.tied_embeddings          : False\n",
      "2020-02-27 17:56:03,163 cfg.model.tied_softmax             : False\n",
      "2020-02-27 17:56:03,163 cfg.model.encoder.type             : recurrent\n",
      "2020-02-27 17:56:03,163 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-27 17:56:03,164 cfg.model.encoder.embeddings.embedding_dim : 300\n",
      "2020-02-27 17:56:03,164 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-27 17:56:03,164 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-27 17:56:03,164 cfg.model.encoder.hidden_size      : 512\n",
      "2020-02-27 17:56:03,164 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-27 17:56:03,164 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-27 17:56:03,164 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-27 17:56:03,164 cfg.model.encoder.freeze           : False\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.type             : recurrent\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.embeddings.embedding_dim : 300\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.hidden_size      : 512\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-27 17:56:03,164 cfg.model.decoder.freeze           : False\n",
      "2020-02-27 17:56:03,164 Data set sizes: \n",
      "\ttrain 6906,\n",
      "\tvalid 388,\n",
      "\ttest 388\n",
      "2020-02-27 17:56:03,164 First training example:\n",
      "\t[SRC] @@ ! @@ p ó n g a n s e @@ a l g o @@ d e @@ r o p a @@ !\n",
      "\t[TRG] @@ ¡ @@ c h o p a t a n i @@ s a w e w e @@ !\n",
      "2020-02-27 17:56:03,164 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) @@ (5) e (6) a (7) o (8) s (9) r\n",
      "2020-02-27 17:56:03,165 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) @@ (5) a (6) i (7) n (8) e (9) k\n",
      "2020-02-27 17:56:03,165 Number of Src words (types): 51\n",
      "2020-02-27 17:56:03,165 Number of Trg words (types): 48\n",
      "2020-02-27 17:56:03,165 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(300, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(812, 512, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=300, vocab_size=51),\n",
      "\ttrg_embed=Embeddings(embedding_dim=300, vocab_size=48))\n",
      "2020-02-27 17:56:03,165 EPOCH 1\n",
      "2020-02-27 17:56:08,779 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 17:56:08,779 Saving new checkpoint.\n",
      "2020-02-27 17:56:08,959 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 23157.7188, ppl:  14.8560, duration: 4.2844s\n",
      "2020-02-27 17:56:14,627 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 21927.1738, ppl:  12.8715, duration: 4.1841s\n",
      "2020-02-27 17:56:18,364 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 21600.7129, ppl:  12.3911, duration: 2.0395s\n",
      "2020-02-27 17:56:24,105 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 20081.9238, ppl:  10.3813, duration: 4.2208s\n",
      "2020-02-27 17:56:28,943 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 18038.4609, ppl:   8.1817, duration: 3.1106s\n",
      "2020-02-27 17:56:32,314 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 16854.0371, ppl:   7.1269, duration: 1.8577s\n",
      "2020-02-27 17:56:35,893 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 16252.5010, ppl:   6.6445, duration: 1.9330s\n",
      "2020-02-27 17:56:39,877 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 15137.4316, ppl:   5.8349, duration: 2.4609s\n",
      "2020-02-27 17:56:43,692 Validation result (greedy) at epoch   1, step       90: bleu:   0.00, loss: 14536.6670, ppl:   5.4404, duration: 2.2213s\n",
      "2020-02-27 17:56:47,660 Validation result (greedy) at epoch   1, step      100: bleu:   0.00, loss: 14218.3594, ppl:   5.2423, duration: 2.4017s\n",
      "2020-02-27 17:56:51,487 Validation result (greedy) at epoch   1, step      110: bleu:   0.00, loss: 13650.2402, ppl:   4.9065, duration: 2.2899s\n",
      "2020-02-27 17:56:55,100 Validation result (greedy) at epoch   1, step      120: bleu:   0.00, loss: 13472.2900, ppl:   4.8058, duration: 2.0138s\n",
      "2020-02-27 17:56:58,853 Validation result (greedy) at epoch   1, step      130: bleu:   0.00, loss: 13265.0547, ppl:   4.6912, duration: 2.1258s\n",
      "2020-02-27 17:57:02,786 Validation result (greedy) at epoch   1, step      140: bleu:   0.00, loss: 12713.6260, ppl:   4.3992, duration: 2.3520s\n",
      "2020-02-27 17:57:03,487 Epoch   1: total training loss 6669.62\n",
      "2020-02-27 17:57:03,487 EPOCH 2\n",
      "2020-02-27 17:57:06,762 Validation result (greedy) at epoch   2, step      150: bleu:   0.00, loss: 12579.8203, ppl:   4.3312, duration: 2.3138s\n",
      "2020-02-27 17:57:10,738 Validation result (greedy) at epoch   2, step      160: bleu:   0.00, loss: 12170.4219, ppl:   4.1294, duration: 2.3091s\n",
      "2020-02-27 17:57:14,554 Validation result (greedy) at epoch   2, step      170: bleu:   0.00, loss: 11918.2900, ppl:   4.0099, duration: 2.2138s\n",
      "2020-02-27 17:57:18,429 Validation result (greedy) at epoch   2, step      180: bleu:   0.00, loss: 11748.9678, ppl:   3.9315, duration: 2.1730s\n",
      "2020-02-27 17:57:22,537 Validation result (greedy) at epoch   2, step      190: bleu:   0.00, loss: 11266.4590, ppl:   3.7166, duration: 2.5289s\n",
      "2020-02-27 17:57:26,208 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 17:57:26,208 Saving new checkpoint.\n",
      "2020-02-27 17:57:26,379 Validation result (greedy) at epoch   2, step      200: bleu:   0.26, loss: 11129.2646, ppl:   3.6576, duration: 2.4511s\n",
      "2020-02-27 17:57:30,572 Validation result (greedy) at epoch   2, step      210: bleu:   0.26, loss: 10842.6025, ppl:   3.5375, duration: 2.4962s\n",
      "2020-02-27 17:57:34,570 Validation result (greedy) at epoch   2, step      220: bleu:   0.00, loss: 10775.6904, ppl:   3.5100, duration: 2.5221s\n",
      "2020-02-27 17:57:38,811 Validation result (greedy) at epoch   2, step      230: bleu:   0.00, loss: 10640.7666, ppl:   3.4552, duration: 2.5296s\n",
      "2020-02-27 17:57:43,550 Validation result (greedy) at epoch   2, step      240: bleu:   0.00, loss: 10502.2168, ppl:   3.3999, duration: 3.1453s\n",
      "2020-02-27 17:57:47,181 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 17:57:47,182 Saving new checkpoint.\n",
      "2020-02-27 17:57:47,294 Validation result (greedy) at epoch   2, step      250: bleu:   0.52, loss: 10409.6475, ppl:   3.3634, duration: 2.5351s\n",
      "2020-02-27 17:57:51,372 Validation result (greedy) at epoch   2, step      260: bleu:   0.00, loss: 10311.1074, ppl:   3.3250, duration: 2.4579s\n",
      "2020-02-27 17:57:55,545 Validation result (greedy) at epoch   2, step      270: bleu:   0.00, loss: 10132.6699, ppl:   3.2566, duration: 2.6144s\n",
      "2020-02-27 17:57:59,653 Validation result (greedy) at epoch   2, step      280: bleu:   0.00, loss: 9918.1328, ppl:   3.1762, duration: 2.6379s\n",
      "2020-02-27 17:58:00,892 Epoch   2: total training loss 4364.27\n",
      "2020-02-27 17:58:00,892 EPOCH 3\n",
      "2020-02-27 17:58:04,122 Validation result (greedy) at epoch   3, step      290: bleu:   0.00, loss: 9716.8564, ppl:   3.1026, duration: 2.9706s\n",
      "2020-02-27 17:58:07,989 Validation result (greedy) at epoch   3, step      300: bleu:   0.26, loss: 9640.4395, ppl:   3.0751, duration: 2.7549s\n",
      "2020-02-27 17:58:12,516 Validation result (greedy) at epoch   3, step      310: bleu:   0.00, loss: 9574.8760, ppl:   3.0517, duration: 2.8171s\n",
      "2020-02-27 17:58:17,033 Validation result (greedy) at epoch   3, step      320: bleu:   0.00, loss: 9477.5762, ppl:   3.0173, duration: 3.1936s\n",
      "2020-02-27 17:58:21,164 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 17:58:21,164 Saving new checkpoint.\n",
      "2020-02-27 17:58:21,294 Validation result (greedy) at epoch   3, step      330: bleu:   0.77, loss: 9274.2812, ppl:   2.9466, duration: 2.7313s\n",
      "2020-02-27 17:58:25,445 Validation result (greedy) at epoch   3, step      340: bleu:   0.52, loss: 9217.3574, ppl:   2.9272, duration: 2.5767s\n",
      "2020-02-27 17:58:30,111 Validation result (greedy) at epoch   3, step      350: bleu:   0.00, loss: 9133.6768, ppl:   2.8988, duration: 3.0447s\n",
      "2020-02-27 17:58:34,254 Validation result (greedy) at epoch   3, step      360: bleu:   0.00, loss: 8996.3799, ppl:   2.8528, duration: 2.5426s\n",
      "2020-02-27 17:58:38,431 Validation result (greedy) at epoch   3, step      370: bleu:   0.26, loss: 8860.9512, ppl:   2.8081, duration: 2.5414s\n",
      "2020-02-27 17:58:42,386 Validation result (greedy) at epoch   3, step      380: bleu:   0.00, loss: 9007.4658, ppl:   2.8564, duration: 2.4556s\n",
      "2020-02-27 17:58:46,254 Validation result (greedy) at epoch   3, step      390: bleu:   0.00, loss: 8799.9209, ppl:   2.7882, duration: 2.5568s\n",
      "2020-02-27 17:58:49,918 Validation result (greedy) at epoch   3, step      400: bleu:   0.26, loss: 8716.9492, ppl:   2.7614, duration: 2.4426s\n",
      "2020-02-27 17:58:53,647 Validation result (greedy) at epoch   3, step      410: bleu:   0.00, loss: 8644.4307, ppl:   2.7381, duration: 2.3253s\n",
      "2020-02-27 17:58:57,832 Validation result (greedy) at epoch   3, step      420: bleu:   0.00, loss: 8434.9131, ppl:   2.6721, duration: 2.3641s\n",
      "2020-02-27 17:59:01,420 Validation result (greedy) at epoch   3, step      430: bleu:   0.26, loss: 8303.9111, ppl:   2.6316, duration: 2.5037s\n",
      "2020-02-27 17:59:01,687 Epoch   3: total training loss 3643.09\n",
      "2020-02-27 17:59:01,687 EPOCH 4\n",
      "2020-02-27 17:59:05,633 Validation result (greedy) at epoch   4, step      440: bleu:   0.26, loss: 8198.6631, ppl:   2.5995, duration: 2.7440s\n",
      "2020-02-27 17:59:09,739 Validation result (greedy) at epoch   4, step      450: bleu:   0.26, loss: 8247.7598, ppl:   2.6144, duration: 2.5871s\n",
      "2020-02-27 17:59:14,127 Validation result (greedy) at epoch   4, step      460: bleu:   0.26, loss: 8099.5073, ppl:   2.5697, duration: 2.8430s\n",
      "2020-02-27 17:59:18,126 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 17:59:18,127 Saving new checkpoint.\n",
      "2020-02-27 17:59:18,289 Validation result (greedy) at epoch   4, step      470: bleu:   1.03, loss: 8044.0747, ppl:   2.5531, duration: 2.7142s\n",
      "2020-02-27 17:59:22,644 Validation result (greedy) at epoch   4, step      480: bleu:   0.00, loss: 7961.5703, ppl:   2.5287, duration: 2.6519s\n",
      "2020-02-27 17:59:27,034 Validation result (greedy) at epoch   4, step      490: bleu:   0.52, loss: 7884.4312, ppl:   2.5061, duration: 2.8836s\n",
      "2020-02-27 17:59:31,489 Validation result (greedy) at epoch   4, step      500: bleu:   0.77, loss: 7762.3506, ppl:   2.4707, duration: 2.6949s\n",
      "2020-02-27 17:59:35,703 Validation result (greedy) at epoch   4, step      510: bleu:   0.52, loss: 7720.6357, ppl:   2.4587, duration: 2.7446s\n",
      "2020-02-27 17:59:39,666 Validation result (greedy) at epoch   4, step      520: bleu:   0.26, loss: 7638.7222, ppl:   2.4353, duration: 2.5316s\n",
      "2020-02-27 17:59:43,769 Validation result (greedy) at epoch   4, step      530: bleu:   0.77, loss: 7750.8135, ppl:   2.4674, duration: 2.5330s\n",
      "2020-02-27 17:59:48,150 Validation result (greedy) at epoch   4, step      540: bleu:   0.77, loss: 7545.7139, ppl:   2.4091, duration: 2.6411s\n",
      "2020-02-27 17:59:52,040 Validation result (greedy) at epoch   4, step      550: bleu:   0.52, loss: 7559.0000, ppl:   2.4128, duration: 2.4429s\n",
      "2020-02-27 17:59:55,865 Validation result (greedy) at epoch   4, step      560: bleu:   0.77, loss: 7604.5488, ppl:   2.4257, duration: 2.5209s\n",
      "2020-02-27 17:59:59,737 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 17:59:59,737 Saving new checkpoint.\n",
      "2020-02-27 17:59:59,913 Validation result (greedy) at epoch   4, step      570: bleu:   1.29, loss: 7307.4561, ppl:   2.3431, duration: 2.5950s\n",
      "2020-02-27 18:00:00,833 Epoch   4: total training loss 3174.31\n",
      "2020-02-27 18:00:00,833 EPOCH 5\n",
      "2020-02-27 18:00:04,176 Validation result (greedy) at epoch   5, step      580: bleu:   1.03, loss: 7303.5356, ppl:   2.3421, duration: 2.7135s\n",
      "2020-02-27 18:00:08,394 Validation result (greedy) at epoch   5, step      590: bleu:   0.52, loss: 7286.3574, ppl:   2.3374, duration: 2.6082s\n",
      "2020-02-27 18:00:12,039 Validation result (greedy) at epoch   5, step      600: bleu:   1.29, loss: 7245.7061, ppl:   2.3263, duration: 2.5721s\n",
      "2020-02-27 18:00:15,488 Validation result (greedy) at epoch   5, step      610: bleu:   0.52, loss: 7158.0933, ppl:   2.3027, duration: 2.4577s\n",
      "2020-02-27 18:00:19,187 Validation result (greedy) at epoch   5, step      620: bleu:   0.52, loss: 7162.9849, ppl:   2.3040, duration: 2.8172s\n",
      "2020-02-27 18:00:23,076 Validation result (greedy) at epoch   5, step      630: bleu:   1.29, loss: 7002.1445, ppl:   2.2612, duration: 2.9509s\n",
      "2020-02-27 18:00:26,700 Validation result (greedy) at epoch   5, step      640: bleu:   1.03, loss: 6982.6553, ppl:   2.2561, duration: 2.6432s\n",
      "2020-02-27 18:00:30,133 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:00:30,134 Saving new checkpoint.\n",
      "2020-02-27 18:00:30,272 Validation result (greedy) at epoch   5, step      650: bleu:   1.55, loss: 6951.4155, ppl:   2.2479, duration: 2.6620s\n",
      "2020-02-27 18:00:34,392 Validation result (greedy) at epoch   5, step      660: bleu:   1.29, loss: 6805.4541, ppl:   2.2100, duration: 2.7015s\n",
      "2020-02-27 18:00:38,695 Validation result (greedy) at epoch   5, step      670: bleu:   1.29, loss: 6804.4917, ppl:   2.2098, duration: 2.7872s\n",
      "2020-02-27 18:00:43,028 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:00:43,028 Saving new checkpoint.\n",
      "2020-02-27 18:00:43,171 Validation result (greedy) at epoch   5, step      680: bleu:   1.80, loss: 6716.8701, ppl:   2.1873, duration: 2.7982s\n",
      "2020-02-27 18:00:47,295 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:00:47,295 Saving new checkpoint.\n",
      "2020-02-27 18:00:47,418 Validation result (greedy) at epoch   5, step      690: bleu:   2.06, loss: 6839.4131, ppl:   2.2188, duration: 2.6836s\n",
      "2020-02-27 18:00:51,345 Validation result (greedy) at epoch   5, step      700: bleu:   1.55, loss: 6740.2314, ppl:   2.1933, duration: 2.4949s\n",
      "2020-02-27 18:00:55,637 Validation result (greedy) at epoch   5, step      710: bleu:   2.06, loss: 6651.1484, ppl:   2.1706, duration: 2.6205s\n",
      "2020-02-27 18:00:59,839 Validation result (greedy) at epoch   5, step      720: bleu:   2.06, loss: 6688.0249, ppl:   2.1800, duration: 2.6349s\n",
      "2020-02-27 18:00:59,839 Epoch   5: total training loss 2826.21\n",
      "2020-02-27 18:00:59,839 EPOCH 6\n",
      "2020-02-27 18:01:03,853 Validation result (greedy) at epoch   6, step      730: bleu:   2.06, loss: 6643.6147, ppl:   2.1687, duration: 2.5544s\n",
      "2020-02-27 18:01:07,989 Validation result (greedy) at epoch   6, step      740: bleu:   1.29, loss: 6532.3066, ppl:   2.1408, duration: 2.7935s\n",
      "2020-02-27 18:01:12,022 Validation result (greedy) at epoch   6, step      750: bleu:   1.80, loss: 6454.8140, ppl:   2.1215, duration: 2.6894s\n",
      "2020-02-27 18:01:16,207 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:01:16,207 Saving new checkpoint.\n",
      "2020-02-27 18:01:16,374 Validation result (greedy) at epoch   6, step      760: bleu:   2.32, loss: 6457.4102, ppl:   2.1222, duration: 2.8686s\n",
      "2020-02-27 18:01:20,457 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:01:20,457 Saving new checkpoint.\n",
      "2020-02-27 18:01:20,579 Validation result (greedy) at epoch   6, step      770: bleu:   2.58, loss: 6489.6567, ppl:   2.1302, duration: 2.6360s\n",
      "2020-02-27 18:01:24,822 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:01:24,823 Saving new checkpoint.\n",
      "2020-02-27 18:01:25,013 Validation result (greedy) at epoch   6, step      780: bleu:   2.84, loss: 6409.2109, ppl:   2.1103, duration: 2.8553s\n",
      "2020-02-27 18:01:29,225 Validation result (greedy) at epoch   6, step      790: bleu:   2.84, loss: 6313.7090, ppl:   2.0869, duration: 2.5879s\n",
      "2020-02-27 18:01:33,693 Validation result (greedy) at epoch   6, step      800: bleu:   2.58, loss: 6386.8223, ppl:   2.1048, duration: 2.7165s\n",
      "2020-02-27 18:01:37,886 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:01:37,887 Saving new checkpoint.\n",
      "2020-02-27 18:01:38,052 Validation result (greedy) at epoch   6, step      810: bleu:   3.35, loss: 6333.2285, ppl:   2.0917, duration: 2.7974s\n",
      "2020-02-27 18:01:42,287 Validation result (greedy) at epoch   6, step      820: bleu:   2.84, loss: 6294.5508, ppl:   2.0823, duration: 2.7262s\n",
      "2020-02-27 18:01:46,633 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:01:46,634 Saving new checkpoint.\n",
      "2020-02-27 18:01:46,761 Validation result (greedy) at epoch   6, step      830: bleu:   3.61, loss: 6189.6587, ppl:   2.0570, duration: 2.8018s\n",
      "2020-02-27 18:01:50,970 Validation result (greedy) at epoch   6, step      840: bleu:   2.32, loss: 6131.3096, ppl:   2.0430, duration: 2.6465s\n",
      "2020-02-27 18:01:55,302 Validation result (greedy) at epoch   6, step      850: bleu:   3.61, loss: 6029.7095, ppl:   2.0190, duration: 2.6286s\n",
      "2020-02-27 18:01:59,317 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:01:59,317 Saving new checkpoint.\n",
      "2020-02-27 18:01:59,444 Validation result (greedy) at epoch   6, step      860: bleu:   4.38, loss: 6042.0708, ppl:   2.0219, duration: 2.6966s\n",
      "2020-02-27 18:01:59,981 Epoch   6: total training loss 2543.98\n",
      "2020-02-27 18:01:59,981 EPOCH 7\n",
      "2020-02-27 18:02:03,646 Validation result (greedy) at epoch   7, step      870: bleu:   3.61, loss: 5953.4502, ppl:   2.0011, duration: 2.8103s\n",
      "2020-02-27 18:02:07,522 Validation result (greedy) at epoch   7, step      880: bleu:   4.12, loss: 5997.4238, ppl:   2.0114, duration: 2.6242s\n",
      "2020-02-27 18:02:11,975 Validation result (greedy) at epoch   7, step      890: bleu:   4.12, loss: 5872.1533, ppl:   1.9823, duration: 2.8826s\n",
      "2020-02-27 18:02:16,356 Validation result (greedy) at epoch   7, step      900: bleu:   4.38, loss: 5924.9956, ppl:   1.9945, duration: 2.7036s\n",
      "2020-02-27 18:02:20,442 Validation result (greedy) at epoch   7, step      910: bleu:   4.38, loss: 5948.4717, ppl:   2.0000, duration: 2.8450s\n",
      "2020-02-27 18:02:24,563 Validation result (greedy) at epoch   7, step      920: bleu:   3.87, loss: 5977.3330, ppl:   2.0067, duration: 2.6848s\n",
      "2020-02-27 18:02:28,760 Validation result (greedy) at epoch   7, step      930: bleu:   4.12, loss: 5831.5620, ppl:   1.9729, duration: 2.7234s\n",
      "2020-02-27 18:02:32,942 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:02:32,942 Saving new checkpoint.\n",
      "2020-02-27 18:02:33,118 Validation result (greedy) at epoch   7, step      940: bleu:   4.64, loss: 5788.7563, ppl:   1.9631, duration: 3.0737s\n",
      "2020-02-27 18:02:37,453 Validation result (greedy) at epoch   7, step      950: bleu:   3.61, loss: 5791.4565, ppl:   1.9637, duration: 2.6536s\n",
      "2020-02-27 18:02:41,783 Validation result (greedy) at epoch   7, step      960: bleu:   3.87, loss: 5726.1660, ppl:   1.9488, duration: 2.8132s\n",
      "2020-02-27 18:02:46,326 Validation result (greedy) at epoch   7, step      970: bleu:   4.38, loss: 5739.8740, ppl:   1.9519, duration: 2.8056s\n",
      "2020-02-27 18:02:51,046 Validation result (greedy) at epoch   7, step      980: bleu:   3.61, loss: 5645.4341, ppl:   1.9306, duration: 3.1719s\n",
      "2020-02-27 18:02:55,227 Validation result (greedy) at epoch   7, step      990: bleu:   3.61, loss: 5616.9858, ppl:   1.9242, duration: 2.6646s\n",
      "2020-02-27 18:02:56,762 Epoch   7 Step:     1000 Batch Loss:    15.754902 Tokens per Sec:     7254, Lr: 0.000500\n",
      "2020-02-27 18:02:59,492 Validation result (greedy) at epoch   7, step     1000: bleu:   4.38, loss: 5665.3911, ppl:   1.9351, duration: 2.7294s\n",
      "2020-02-27 18:03:00,804 Epoch   7: total training loss 2282.51\n",
      "2020-02-27 18:03:00,804 EPOCH 8\n",
      "2020-02-27 18:03:03,958 Validation result (greedy) at epoch   8, step     1010: bleu:   4.64, loss: 5526.8540, ppl:   1.9041, duration: 2.7868s\n",
      "2020-02-27 18:03:08,214 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:03:08,214 Saving new checkpoint.\n",
      "2020-02-27 18:03:08,338 Validation result (greedy) at epoch   8, step     1020: bleu:   4.90, loss: 5538.4810, ppl:   1.9067, duration: 2.9372s\n",
      "2020-02-27 18:03:12,557 Validation result (greedy) at epoch   8, step     1030: bleu:   3.35, loss: 5522.5435, ppl:   1.9031, duration: 2.6976s\n",
      "2020-02-27 18:03:16,844 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:03:16,845 Saving new checkpoint.\n",
      "2020-02-27 18:03:16,968 Validation result (greedy) at epoch   8, step     1040: bleu:   5.15, loss: 5532.7759, ppl:   1.9054, duration: 2.8200s\n",
      "2020-02-27 18:03:21,400 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:03:21,400 Saving new checkpoint.\n",
      "2020-02-27 18:03:21,516 Validation result (greedy) at epoch   8, step     1050: bleu:   5.41, loss: 5545.3232, ppl:   1.9082, duration: 2.7916s\n",
      "2020-02-27 18:03:25,962 Validation result (greedy) at epoch   8, step     1060: bleu:   4.90, loss: 5417.0122, ppl:   1.8799, duration: 2.8653s\n",
      "2020-02-27 18:03:30,079 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:03:30,080 Saving new checkpoint.\n",
      "2020-02-27 18:03:30,258 Validation result (greedy) at epoch   8, step     1070: bleu:   5.67, loss: 5383.5298, ppl:   1.8726, duration: 2.8497s\n",
      "2020-02-27 18:03:34,728 Validation result (greedy) at epoch   8, step     1080: bleu:   4.64, loss: 5444.5801, ppl:   1.8859, duration: 2.7991s\n",
      "2020-02-27 18:03:38,654 Validation result (greedy) at epoch   8, step     1090: bleu:   5.41, loss: 5356.5913, ppl:   1.8667, duration: 2.5547s\n",
      "2020-02-27 18:03:42,978 Validation result (greedy) at epoch   8, step     1100: bleu:   5.41, loss: 5344.6606, ppl:   1.8641, duration: 2.6648s\n",
      "2020-02-27 18:03:47,280 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:03:47,281 Saving new checkpoint.\n",
      "2020-02-27 18:03:47,406 Validation result (greedy) at epoch   8, step     1110: bleu:   6.19, loss: 5376.1650, ppl:   1.8710, duration: 2.7445s\n",
      "2020-02-27 18:03:51,631 Validation result (greedy) at epoch   8, step     1120: bleu:   4.64, loss: 5394.6436, ppl:   1.8750, duration: 2.6170s\n",
      "2020-02-27 18:03:55,869 Validation result (greedy) at epoch   8, step     1130: bleu:   5.67, loss: 5302.6187, ppl:   1.8550, duration: 2.7743s\n",
      "2020-02-27 18:04:00,064 Validation result (greedy) at epoch   8, step     1140: bleu:   5.41, loss: 5257.2461, ppl:   1.8452, duration: 2.6905s\n",
      "2020-02-27 18:04:04,161 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:04:04,161 Saving new checkpoint.\n",
      "2020-02-27 18:04:04,278 Validation result (greedy) at epoch   8, step     1150: bleu:   6.44, loss: 5236.5781, ppl:   1.8408, duration: 2.8041s\n",
      "2020-02-27 18:04:04,589 Epoch   8: total training loss 2032.73\n",
      "2020-02-27 18:04:04,590 EPOCH 9\n",
      "2020-02-27 18:04:08,512 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:04:08,512 Saving new checkpoint.\n",
      "2020-02-27 18:04:08,628 Validation result (greedy) at epoch   9, step     1160: bleu:   6.70, loss: 5267.8145, ppl:   1.8475, duration: 2.8102s\n",
      "2020-02-27 18:04:13,203 Validation result (greedy) at epoch   9, step     1170: bleu:   6.19, loss: 5167.6821, ppl:   1.8260, duration: 2.7586s\n",
      "2020-02-27 18:04:17,459 Validation result (greedy) at epoch   9, step     1180: bleu:   6.70, loss: 5274.6597, ppl:   1.8490, duration: 2.6745s\n",
      "2020-02-27 18:04:22,118 Validation result (greedy) at epoch   9, step     1190: bleu:   6.70, loss: 5239.2505, ppl:   1.8413, duration: 2.7703s\n",
      "2020-02-27 18:04:26,471 Validation result (greedy) at epoch   9, step     1200: bleu:   4.90, loss: 5180.4238, ppl:   1.8288, duration: 2.8190s\n",
      "2020-02-27 18:04:30,645 Validation result (greedy) at epoch   9, step     1210: bleu:   5.41, loss: 5154.8511, ppl:   1.8233, duration: 2.7047s\n",
      "2020-02-27 18:04:35,135 Validation result (greedy) at epoch   9, step     1220: bleu:   5.41, loss: 5226.5688, ppl:   1.8386, duration: 2.7913s\n",
      "2020-02-27 18:04:39,617 Validation result (greedy) at epoch   9, step     1230: bleu:   6.44, loss: 5140.2310, ppl:   1.8202, duration: 2.8145s\n",
      "2020-02-27 18:04:43,952 Validation result (greedy) at epoch   9, step     1240: bleu:   6.19, loss: 5158.9370, ppl:   1.8242, duration: 2.8046s\n",
      "2020-02-27 18:04:48,202 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-27 18:04:48,203 Saving new checkpoint.\n",
      "2020-02-27 18:04:48,389 Validation result (greedy) at epoch   9, step     1250: bleu:   6.96, loss: 5110.5635, ppl:   1.8139, duration: 2.8606s\n",
      "2020-02-27 18:04:52,729 Validation result (greedy) at epoch   9, step     1260: bleu:   4.64, loss: 5118.4448, ppl:   1.8156, duration: 2.8206s\n"
     ]
    }
   ],
   "source": [
    "base_path = Path('data/transfer/preprocessed')\n",
    "datas = os.listdir(base_path)\n",
    "emb_size = 300\n",
    "hidden_size = 512\n",
    "for data in datas:\n",
    "    data_path = base_path / data\n",
    "    for lang in os.listdir(data_path):\n",
    "        if 'shp' in lang:\n",
    "            lang_path = data_path / lang\n",
    "            for segment in os.listdir(lang_path):\n",
    "                segment_path = lang_path / segment\n",
    "                print(os.path.join('results/translate', segment_path))\n",
    "                if 'bpe_drop' in segment:\n",
    "                    level = 'bpe'\n",
    "                elif 'bpe' in segment:\n",
    "                    level = 'bpe'\n",
    "                elif 'char' in segment:\n",
    "                    level = 'char'\n",
    "                elif 'word' in segment:\n",
    "                    level = 'word'\n",
    "                elif 'syl' in segment:\n",
    "                    level = 'syl'\n",
    "                else:\n",
    "                    level = None\n",
    "\n",
    "                val_freq = 10\n",
    "                lang_in, lang_out = lang.split('-')\n",
    "                f_config = config.format(lang_src=lang_in, lang_tgt=lang_out, \n",
    "                                         train_path=os.path.join(segment_path, 'train'),\n",
    "                                         test_path=os.path.join(segment_path, 'test'),\n",
    "                                         dev_path=os.path.join(segment_path, 'valid'),\n",
    "                                         level=level,\n",
    "                                         emb_size=emb_size,\n",
    "                                         hidden_size=hidden_size,\n",
    "                                         val_freq=val_freq,\n",
    "                                         model_dir=os.path.join('results/rnn/transfer_baseline',\\\n",
    "                                                                f'{lang_in}-{lang_out}_{emb_size}_{hidden_size}', segment))\n",
    "\n",
    "                with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=\"baseline\"),'w') as f:\n",
    "                    f.write(f_config)\n",
    "\n",
    "                !python3 joeynmt/joeynmt train \"joeynmt/configs/transformer_baseline.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es-shp'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
