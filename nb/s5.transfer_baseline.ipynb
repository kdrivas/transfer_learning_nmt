{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘./nb/.ipynb_checkpoints’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!find . -name .ipynb* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 500                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 17                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: True                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/transfer/preprocessed'\n",
    "datas = os.listdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en-de'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/translate/data/transfer/preprocessed/splits.es/es-shp/word\n",
      "2020-03-04 16:36:33,388 Hello! This is Joey-NMT.\n",
      "2020-03-04 16:36:34.079067: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-04 16:36:34.079126: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-03-04 16:36:34.079136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2020-03-04 16:36:34,681 Total params: 16649136\n",
      "2020-03-04 16:36:34,682 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-03-04 16:36:36,648 cfg.name                           : my_experiment\n",
      "2020-03-04 16:36:36,648 cfg.data.src                       : es\n",
      "2020-03-04 16:36:36,648 cfg.data.trg                       : shp\n",
      "2020-03-04 16:36:36,648 cfg.data.train                     : data/transfer/preprocessed/splits.es/es-shp/word/train\n",
      "2020-03-04 16:36:36,648 cfg.data.dev                       : data/transfer/preprocessed/splits.es/es-shp/word/valid\n",
      "2020-03-04 16:36:36,649 cfg.data.test                      : data/transfer/preprocessed/splits.es/es-shp/word/test\n",
      "2020-03-04 16:36:36,649 cfg.data.level                     : word\n",
      "2020-03-04 16:36:36,649 cfg.data.lowercase                 : True\n",
      "2020-03-04 16:36:36,649 cfg.data.max_sent_length           : 150\n",
      "2020-03-04 16:36:36,649 cfg.data.src_voc_min_freq          : 1\n",
      "2020-03-04 16:36:36,649 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-03-04 16:36:36,649 cfg.testing.beam_size              : 5\n",
      "2020-03-04 16:36:36,649 cfg.testing.alpha                  : 1.0\n",
      "2020-03-04 16:36:36,649 cfg.training.reset_best_ckpt       : False\n",
      "2020-03-04 16:36:36,649 cfg.training.reset_scheduler       : False\n",
      "2020-03-04 16:36:36,649 cfg.training.reset_optimizer       : False\n",
      "2020-03-04 16:36:36,649 cfg.training.random_seed           : 42\n",
      "2020-03-04 16:36:36,649 cfg.training.optimizer             : adam\n",
      "2020-03-04 16:36:36,649 cfg.training.learning_rate         : 0.0005\n",
      "2020-03-04 16:36:36,649 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-03-04 16:36:36,649 cfg.training.clip_grad_val         : 1.0\n",
      "2020-03-04 16:36:36,649 cfg.training.weight_decay          : 0.0\n",
      "2020-03-04 16:36:36,649 cfg.training.batch_size            : 48\n",
      "2020-03-04 16:36:36,649 cfg.training.batch_type            : sentence\n",
      "2020-03-04 16:36:36,649 cfg.training.eval_batch_size       : 10\n",
      "2020-03-04 16:36:36,649 cfg.training.eval_batch_type       : sentence\n",
      "2020-03-04 16:36:36,649 cfg.training.batch_multiplier      : 1\n",
      "2020-03-04 16:36:36,649 cfg.training.scheduling            : plateau\n",
      "2020-03-04 16:36:36,649 cfg.training.patience              : 500\n",
      "2020-03-04 16:36:36,649 cfg.training.decrease_factor       : 0.5\n",
      "2020-03-04 16:36:36,649 cfg.training.epochs                : 17\n",
      "2020-03-04 16:36:36,649 cfg.training.validation_freq       : 50\n",
      "2020-03-04 16:36:36,649 cfg.training.logging_freq          : 1000\n",
      "2020-03-04 16:36:36,650 cfg.training.eval_metric           : bleu\n",
      "2020-03-04 16:36:36,650 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-03-04 16:36:36,650 cfg.training.model_dir             : results/rnn/transfer_baseline/es-shp_300_512/word\n",
      "2020-03-04 16:36:36,650 cfg.training.overwrite             : True\n",
      "2020-03-04 16:36:36,650 cfg.training.shuffle               : True\n",
      "2020-03-04 16:36:36,650 cfg.training.use_cuda              : True\n",
      "2020-03-04 16:36:36,650 cfg.training.max_output_length     : 60\n",
      "2020-03-04 16:36:36,650 cfg.training.print_valid_sents     : []\n",
      "2020-03-04 16:36:36,650 cfg.training.keep_last_ckpts       : 3\n",
      "2020-03-04 16:36:36,650 cfg.training.label_smoothing       : 0.0\n",
      "2020-03-04 16:36:36,650 cfg.model.initializer              : xavier\n",
      "2020-03-04 16:36:36,650 cfg.model.init_weight              : 0.01\n",
      "2020-03-04 16:36:36,650 cfg.model.init_gain                : 1.0\n",
      "2020-03-04 16:36:36,650 cfg.model.bias_initializer         : zeros\n",
      "2020-03-04 16:36:36,650 cfg.model.embed_initializer        : normal\n",
      "2020-03-04 16:36:36,650 cfg.model.embed_init_weight        : 0.1\n",
      "2020-03-04 16:36:36,650 cfg.model.embed_init_gain          : 1.0\n",
      "2020-03-04 16:36:36,650 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-03-04 16:36:36,650 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-03-04 16:36:36,650 cfg.model.tied_embeddings          : False\n",
      "2020-03-04 16:36:36,650 cfg.model.tied_softmax             : False\n",
      "2020-03-04 16:36:36,650 cfg.model.encoder.type             : recurrent\n",
      "2020-03-04 16:36:36,650 cfg.model.encoder.rnn_type         : gru\n",
      "2020-03-04 16:36:36,650 cfg.model.encoder.embeddings.embedding_dim : 300\n",
      "2020-03-04 16:36:36,650 cfg.model.encoder.embeddings.scale : False\n",
      "2020-03-04 16:36:36,650 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-03-04 16:36:36,650 cfg.model.encoder.hidden_size      : 512\n",
      "2020-03-04 16:36:36,650 cfg.model.encoder.bidirectional    : True\n",
      "2020-03-04 16:36:36,651 cfg.model.encoder.dropout          : 0.3\n",
      "2020-03-04 16:36:36,651 cfg.model.encoder.num_layers       : 2\n",
      "2020-03-04 16:36:36,651 cfg.model.encoder.freeze           : False\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.type             : recurrent\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.rnn_type         : gru\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.embeddings.embedding_dim : 300\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.embeddings.scale : False\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.hidden_size      : 512\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.dropout          : 0.3\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.num_layers       : 2\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.input_feeding    : True\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.init_hidden      : last\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.attention        : bahdanau\n",
      "2020-03-04 16:36:36,651 cfg.model.decoder.freeze           : False\n",
      "2020-03-04 16:36:36,651 Data set sizes: \n",
      "\ttrain 6906,\n",
      "\tvalid 388,\n",
      "\ttest 388\n",
      "2020-03-04 16:36:36,651 First training example:\n",
      "\t[SRC] ! pónganse algo de ropa !\n",
      "\t[TRG] ¡ chopatani sawewe !\n",
      "2020-03-04 16:36:36,651 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ? (6) ¿ (7) no (8) puedo (9) tom\n",
      "2020-03-04 16:36:36,651 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) ? (6) ¿ (7) mia (8) ea (9) riki\n",
      "2020-03-04 16:36:36,651 Number of Src words (types): 4215\n",
      "2020-03-04 16:36:36,651 Number of Trg words (types): 3661\n",
      "2020-03-04 16:36:36,652 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(300, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(812, 512, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=300, vocab_size=4215),\n",
      "\ttrg_embed=Embeddings(embedding_dim=300, vocab_size=3661))\n",
      "2020-03-04 16:36:36,655 EPOCH 1\n",
      "2020-03-04 16:36:39,064 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:36:39,065 Saving new checkpoint.\n",
      "2020-03-04 16:36:39,300 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 8982.5049, ppl:  82.7793, duration: 0.8435s\n",
      "2020-03-04 16:36:41,868 Validation result (greedy) at epoch   1, step      100: bleu:   0.00, loss: 8183.0972, ppl:  55.8771, duration: 0.7702s\n",
      "2020-03-04 16:36:43,892 Epoch   1: total training loss 3411.80\n",
      "2020-03-04 16:36:43,892 EPOCH 2\n",
      "2020-03-04 16:36:45,013 Validation result (greedy) at epoch   2, step      150: bleu:   0.00, loss: 7682.3433, ppl:  43.6832, duration: 0.8495s\n",
      "2020-03-04 16:36:48,273 Validation result (greedy) at epoch   2, step      200: bleu:   0.00, loss: 7496.9775, ppl:  39.8782, duration: 0.8314s\n",
      "2020-03-04 16:36:51,095 Validation result (greedy) at epoch   2, step      250: bleu:   0.00, loss: 7407.5796, ppl:  38.1634, duration: 0.7290s\n",
      "2020-03-04 16:36:52,224 Epoch   2: total training loss 2734.18\n",
      "2020-03-04 16:36:52,224 EPOCH 3\n",
      "2020-03-04 16:36:53,325 Validation result (greedy) at epoch   3, step      300: bleu:   0.00, loss: 7172.9258, ppl:  34.0052, duration: 0.7034s\n",
      "2020-03-04 16:36:55,712 Validation result (greedy) at epoch   3, step      350: bleu:   0.00, loss: 7152.4868, ppl:  33.6652, duration: 0.7358s\n",
      "2020-03-04 16:36:58,105 Validation result (greedy) at epoch   3, step      400: bleu:   0.00, loss: 6970.8940, ppl:  30.7898, duration: 0.7473s\n",
      "2020-03-04 16:36:59,258 Epoch   3: total training loss 2518.79\n",
      "2020-03-04 16:36:59,258 EPOCH 4\n",
      "2020-03-04 16:37:00,655 Validation result (greedy) at epoch   4, step      450: bleu:   0.00, loss: 6951.9360, ppl:  30.5042, duration: 0.7787s\n",
      "2020-03-04 16:37:03,909 Validation result (greedy) at epoch   4, step      500: bleu:   0.00, loss: 6768.8237, ppl:  27.8780, duration: 0.8567s\n",
      "2020-03-04 16:37:07,505 Validation result (greedy) at epoch   4, step      550: bleu:   0.00, loss: 6645.1465, ppl:  26.2334, duration: 1.1148s\n",
      "2020-03-04 16:37:08,659 Epoch   4: total training loss 2328.78\n",
      "2020-03-04 16:37:08,659 EPOCH 5\n",
      "2020-03-04 16:37:10,510 Validation result (greedy) at epoch   5, step      600: bleu:   0.00, loss: 6649.3960, ppl:  26.2883, duration: 0.7634s\n",
      "2020-03-04 16:37:13,324 Validation result (greedy) at epoch   5, step      650: bleu:   0.00, loss: 6482.8574, ppl:  24.2216, duration: 0.8224s\n",
      "2020-03-04 16:37:15,958 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:37:15,959 Saving new checkpoint.\n",
      "2020-03-04 16:37:16,182 Validation result (greedy) at epoch   5, step      700: bleu:   4.39, loss: 6348.4683, ppl:  22.6730, duration: 0.9788s\n",
      "2020-03-04 16:37:17,061 Epoch   5: total training loss 2144.91\n",
      "2020-03-04 16:37:17,061 EPOCH 6\n",
      "2020-03-04 16:37:18,870 Validation result (greedy) at epoch   6, step      750: bleu:   4.33, loss: 6325.1074, ppl:  22.4141, duration: 0.7990s\n",
      "2020-03-04 16:37:21,776 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:37:21,776 Saving new checkpoint.\n",
      "2020-03-04 16:37:21,979 Validation result (greedy) at epoch   6, step      800: bleu:   4.60, loss: 6263.6670, ppl:  21.7471, duration: 1.2529s\n",
      "2020-03-04 16:37:25,405 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:37:25,405 Saving new checkpoint.\n",
      "2020-03-04 16:37:25,651 Validation result (greedy) at epoch   6, step      850: bleu:   6.24, loss: 6121.6772, ppl:  20.2808, duration: 1.3521s\n",
      "2020-03-04 16:37:26,148 Epoch   6: total training loss 1974.49\n",
      "2020-03-04 16:37:26,148 EPOCH 7\n",
      "2020-03-04 16:37:28,940 Validation result (greedy) at epoch   7, step      900: bleu:   5.71, loss: 6104.9487, ppl:  20.1147, duration: 0.9958s\n",
      "2020-03-04 16:37:32,106 Validation result (greedy) at epoch   7, step      950: bleu:   5.62, loss: 6036.5991, ppl:  19.4500, duration: 0.7923s\n",
      "2020-03-04 16:37:34,354 Epoch   7 Step:     1000 Batch Loss:    10.137936 Tokens per Sec:     5322, Lr: 0.000500\n",
      "2020-03-04 16:37:35,137 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:37:35,137 Saving new checkpoint.\n",
      "2020-03-04 16:37:35,308 Validation result (greedy) at epoch   7, step     1000: bleu:   7.66, loss: 5987.1548, ppl:  18.9829, duration: 0.9541s\n",
      "2020-03-04 16:37:35,626 Epoch   7: total training loss 1821.09\n",
      "2020-03-04 16:37:35,626 EPOCH 8\n",
      "2020-03-04 16:37:37,979 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:37:37,980 Saving new checkpoint.\n",
      "2020-03-04 16:37:38,222 Validation result (greedy) at epoch   8, step     1050: bleu:   8.05, loss: 5949.3501, ppl:  18.6333, duration: 1.0804s\n",
      "2020-03-04 16:37:40,648 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:37:40,648 Saving new checkpoint.\n",
      "2020-03-04 16:37:40,845 Validation result (greedy) at epoch   8, step     1100: bleu:   8.81, loss: 5909.6201, ppl:  18.2729, duration: 0.9708s\n",
      "2020-03-04 16:37:43,702 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:37:43,702 Saving new checkpoint.\n",
      "2020-03-04 16:37:43,876 Validation result (greedy) at epoch   8, step     1150: bleu:   9.30, loss: 5838.0991, ppl:  17.6415, duration: 1.0292s\n",
      "2020-03-04 16:37:43,978 Epoch   8: total training loss 1672.92\n",
      "2020-03-04 16:37:43,978 EPOCH 9\n",
      "2020-03-04 16:37:47,127 Validation result (greedy) at epoch   9, step     1200: bleu:   8.87, loss: 5879.6699, ppl:  18.0058, duration: 0.9801s\n",
      "2020-03-04 16:37:50,354 Validation result (greedy) at epoch   9, step     1250: bleu:   7.71, loss: 5883.2339, ppl:  18.0374, duration: 1.0074s\n",
      "2020-03-04 16:37:51,941 Epoch   9: total training loss 1539.89\n",
      "2020-03-04 16:37:51,941 EPOCH 10\n",
      "2020-03-04 16:37:52,836 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:37:52,836 Saving new checkpoint.\n",
      "2020-03-04 16:37:53,007 Validation result (greedy) at epoch  10, step     1300: bleu:  11.98, loss: 5796.2339, ppl:  17.2821, duration: 0.9532s\n",
      "2020-03-04 16:37:55,382 Validation result (greedy) at epoch  10, step     1350: bleu:  10.42, loss: 5783.5269, ppl:  17.1745, duration: 0.7737s\n",
      "2020-03-04 16:37:57,736 Validation result (greedy) at epoch  10, step     1400: bleu:  10.89, loss: 5697.5249, ppl:  16.4635, duration: 0.7826s\n",
      "2020-03-04 16:37:58,904 Epoch  10: total training loss 1409.38\n",
      "2020-03-04 16:37:58,904 EPOCH 11\n",
      "2020-03-04 16:38:00,002 Validation result (greedy) at epoch  11, step     1450: bleu:  10.12, loss: 5716.4009, ppl:  16.6170, duration: 0.7933s\n",
      "2020-03-04 16:38:02,269 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:38:02,270 Saving new checkpoint.\n",
      "2020-03-04 16:38:02,428 Validation result (greedy) at epoch  11, step     1500: bleu:  12.14, loss: 5716.2090, ppl:  16.6154, duration: 0.8710s\n",
      "2020-03-04 16:38:05,631 Validation result (greedy) at epoch  11, step     1550: bleu:  11.82, loss: 5663.2910, ppl:  16.1887, duration: 1.0614s\n",
      "2020-03-04 16:38:07,050 Epoch  11: total training loss 1292.73\n",
      "2020-03-04 16:38:07,050 EPOCH 12\n",
      "2020-03-04 16:38:08,691 Validation result (greedy) at epoch  12, step     1600: bleu:  11.10, loss: 5658.4922, ppl:  16.1505, duration: 0.9245s\n",
      "2020-03-04 16:38:11,940 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:38:11,941 Saving new checkpoint.\n",
      "2020-03-04 16:38:12,113 Validation result (greedy) at epoch  12, step     1650: bleu:  12.39, loss: 5678.6489, ppl:  16.3114, duration: 1.0347s\n",
      "2020-03-04 16:38:14,857 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:38:14,858 Saving new checkpoint.\n",
      "2020-03-04 16:38:15,092 Validation result (greedy) at epoch  12, step     1700: bleu:  14.27, loss: 5624.7437, ppl:  15.8848, duration: 1.0416s\n",
      "2020-03-04 16:38:16,113 Epoch  12: total training loss 1176.53\n",
      "2020-03-04 16:38:16,113 EPOCH 13\n",
      "2020-03-04 16:38:17,754 Validation result (greedy) at epoch  13, step     1750: bleu:  12.67, loss: 5639.1855, ppl:  15.9980, duration: 0.8092s\n",
      "2020-03-04 16:38:20,339 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:38:20,339 Saving new checkpoint.\n",
      "2020-03-04 16:38:20,607 Validation result (greedy) at epoch  13, step     1800: bleu:  14.32, loss: 5661.9717, ppl:  16.1782, duration: 1.1071s\n",
      "2020-03-04 16:38:24,380 Validation result (greedy) at epoch  13, step     1850: bleu:  11.81, loss: 5637.1084, ppl:  15.9816, duration: 1.1295s\n",
      "2020-03-04 16:38:25,462 Epoch  13: total training loss 1069.91\n",
      "2020-03-04 16:38:25,462 EPOCH 14\n",
      "2020-03-04 16:38:27,883 Validation result (greedy) at epoch  14, step     1900: bleu:  13.82, loss: 5619.6094, ppl:  15.8447, duration: 0.8981s\n",
      "2020-03-04 16:38:30,763 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:38:30,764 Saving new checkpoint.\n",
      "2020-03-04 16:38:31,010 Validation result (greedy) at epoch  14, step     1950: bleu:  14.78, loss: 5624.3276, ppl:  15.8815, duration: 0.9998s\n",
      "2020-03-04 16:38:32,681 Epoch  14 Step:     2000 Batch Loss:     5.950390 Tokens per Sec:     6043, Lr: 0.000500\n",
      "2020-03-04 16:38:33,496 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:38:33,497 Saving new checkpoint.\n",
      "2020-03-04 16:38:33,688 Validation result (greedy) at epoch  14, step     2000: bleu:  16.08, loss: 5561.0303, ppl:  15.3949, duration: 1.0066s\n",
      "2020-03-04 16:38:34,376 Epoch  14: total training loss 975.30\n",
      "2020-03-04 16:38:34,376 EPOCH 15\n",
      "2020-03-04 16:38:36,634 Validation result (greedy) at epoch  15, step     2050: bleu:  15.28, loss: 5653.1328, ppl:  16.1080, duration: 0.7877s\n",
      "2020-03-04 16:38:39,172 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:38:39,172 Saving new checkpoint.\n",
      "2020-03-04 16:38:39,387 Validation result (greedy) at epoch  15, step     2100: bleu:  16.24, loss: 5656.7437, ppl:  16.1367, duration: 0.9530s\n",
      "2020-03-04 16:38:42,672 Validation result (greedy) at epoch  15, step     2150: bleu:  16.16, loss: 5611.6074, ppl:  15.7825, duration: 0.8102s\n",
      "2020-03-04 16:38:43,051 Epoch  15: total training loss 870.36\n",
      "2020-03-04 16:38:43,051 EPOCH 16\n",
      "2020-03-04 16:38:46,340 Validation result (greedy) at epoch  16, step     2200: bleu:  15.53, loss: 5620.3008, ppl:  15.8501, duration: 1.2719s\n",
      "2020-03-04 16:38:49,415 Hooray! New best validation result [eval_metric]!\n",
      "2020-03-04 16:38:49,415 Saving new checkpoint.\n",
      "2020-03-04 16:38:49,681 Validation result (greedy) at epoch  16, step     2250: bleu:  17.35, loss: 5670.7051, ppl:  16.2478, duration: 1.0415s\n",
      "2020-03-04 16:38:52,034 Validation result (greedy) at epoch  16, step     2300: bleu:  16.39, loss: 5528.5474, ppl:  15.1510, duration: 0.7704s\n",
      "2020-03-04 16:38:52,167 Epoch  16: total training loss 786.44\n",
      "2020-03-04 16:38:52,167 EPOCH 17\n",
      "2020-03-04 16:38:54,524 Validation result (greedy) at epoch  17, step     2350: bleu:  17.06, loss: 5608.2554, ppl:  15.7565, duration: 0.7869s\n",
      "2020-03-04 16:38:57,140 Validation result (greedy) at epoch  17, step     2400: bleu:  15.94, loss: 5579.0864, ppl:  15.5322, duration: 0.7834s\n",
      "2020-03-04 16:38:58,646 Epoch  17: total training loss 700.76\n",
      "2020-03-04 16:38:58,646 Training ended after  17 epochs.\n",
      "2020-03-04 16:38:58,646 Best validation result (greedy) at step     2250:  17.35 eval_metric.\n",
      "2020-03-04 16:39:00,860  dev bleu:  19.19 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-03-04 16:39:00,861 Translations saved to: results/rnn/transfer_baseline/es-shp_300_512/word/00002250.hyps.dev\n",
      "2020-03-04 16:39:02,324 test bleu:  20.97 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-03-04 16:39:02,324 Translations saved to: results/rnn/transfer_baseline/es-shp_300_512/word/00002250.hyps.test\n"
     ]
    }
   ],
   "source": [
    "base_path = Path('data/transfer/preprocessed')\n",
    "datas = os.listdir(base_path)\n",
    "emb_size = 300\n",
    "hidden_size = 512\n",
    "for data in datas:\n",
    "    data_path = base_path / data\n",
    "    for lang in os.listdir(data_path):\n",
    "        if 'es-shp' in lang:\n",
    "            lang_path = data_path / lang\n",
    "            lang_in_temp, lang_out_temp = lang.split('-')\n",
    "            for lang_in, lang_out in [[lang_in_temp, lang_out_temp]]:\n",
    "                for segment in os.listdir(lang_path):\n",
    "                    if 'word' in segment:\n",
    "                        segment_path = lang_path / segment\n",
    "                        print(os.path.join('results/translate', segment_path))\n",
    "                        if 'bpe_drop' in segment:\n",
    "                            level = 'bpe'\n",
    "                        elif 'bpe' in segment:\n",
    "                            level = 'bpe'\n",
    "                        elif 'char' in segment:\n",
    "                            level = 'char'\n",
    "                        elif 'word' in segment:\n",
    "                            level = 'word'\n",
    "                        elif 'syl' in segment:\n",
    "                            level = 'syl'\n",
    "                        else:\n",
    "                            level = None\n",
    "\n",
    "                        val_freq = 50\n",
    "                        f_config = config.format(lang_src=lang_in, lang_tgt=lang_out, \n",
    "                                                 train_path=os.path.join(segment_path, 'train'),\n",
    "                                                 test_path=os.path.join(segment_path, 'test'),\n",
    "                                                 dev_path=os.path.join(segment_path, 'valid'),\n",
    "                                                 level=level,\n",
    "                                                 emb_size=emb_size,\n",
    "                                                 hidden_size=hidden_size,\n",
    "                                                 val_freq=val_freq,\n",
    "                                                 model_dir=os.path.join('results/rnn/transfer_baseline',\\\n",
    "                                                                        f'{lang_in}-{lang_out}_{emb_size}_{hidden_size}', segment))\n",
    "\n",
    "                        with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=\"baseline\"),'w') as f:\n",
    "                            f.write(f_config)\n",
    "\n",
    "                        !python3 joeynmt/joeynmt train \"joeynmt/configs/transformer_baseline.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es-shp'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
