{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "from typing import List\n",
    "import os\n",
    "import queue\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.data import Dataset\n",
    "\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import log_data_info, load_config, log_cfg, \\\n",
    "    store_attention_plots, load_checkpoint, make_model_dir, \\\n",
    "    make_logger, set_seed, symlink_update, ConfigurationError\n",
    "from joeynmt.model import Model\n",
    "from joeynmt.prediction import validate_on_data\n",
    "from joeynmt.loss import XentLoss\n",
    "from joeynmt.data import load_data, make_data_iter\n",
    "from joeynmt.builders import build_optimizer, build_scheduler, \\\n",
    "    build_gradient_clipper\n",
    "from joeynmt.training import TrainManager\n",
    "from joeynmt.prediction import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joeynmt.model import Model\n",
    "from joeynmt.initialization import initialize_model\n",
    "from joeynmt.embeddings import Embeddings\n",
    "from joeynmt.encoders import Encoder, RecurrentEncoder, TransformerEncoder\n",
    "from joeynmt.decoders import Decoder, RecurrentDecoder, TransformerDecoder\n",
    "from joeynmt.constants import PAD_TOKEN, EOS_TOKEN, BOS_TOKEN\n",
    "from joeynmt.search import beam_search, greedy\n",
    "from joeynmt.vocabulary import Vocabulary\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(module, val):\n",
    "    for p in module.parameters():\n",
    "        p = p.detach()\n",
    "        p.requires_grad = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "def build_pretrained_model(cfg: dict = None,\n",
    "                pretrained_model: Model = None,\n",
    "                pretrained_src_vocab: Vocabulary = None,\n",
    "                src_vocab: Vocabulary = None,\n",
    "                trg_vocab: Vocabulary = None) -> Model:\n",
    "    \"\"\"\n",
    "    Build and initialize the model according to the configuration.\n",
    "\n",
    "    :param cfg: dictionary configuration containing model specifications\n",
    "    :param src_vocab: source vocabulary\n",
    "    :param trg_vocab: target vocabulary\n",
    "    :return: built and initialized model\n",
    "    \"\"\"\n",
    "    src_padding_idx = src_vocab.stoi[PAD_TOKEN]\n",
    "    trg_padding_idx = trg_vocab.stoi[PAD_TOKEN]\n",
    "\n",
    "    src_embed = Embeddings(\n",
    "        **cfg[\"encoder\"][\"embeddings\"], vocab_size=len(src_vocab),\n",
    "        padding_idx=src_padding_idx)\n",
    "\n",
    "    embedding_matrix = np.zeros((len(src_vocab), src_embed.embedding_dim))\n",
    "    unknown_words = []\n",
    "    for w in pretrained_src_vocab.itos:\n",
    "        try:\n",
    "            ix = pretrained_src_vocab.stoi[w]\n",
    "            embedding_matrix[ix] = pretrained_model.src_embed.lut.weight[ix].cpu().detach().numpy()\n",
    "        except KeyError:\n",
    "            unknown_words.append(w)\n",
    "    \n",
    "    src_embed.lut.weight = torch.nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "\n",
    "    trg_embed = Embeddings(\n",
    "            **cfg[\"decoder\"][\"embeddings\"], vocab_size=len(trg_vocab),\n",
    "            padding_idx=trg_padding_idx)\n",
    "\n",
    "    # build decoder\n",
    "    dec_dropout = cfg[\"decoder\"].get(\"dropout\", 0.)\n",
    "    dec_emb_dropout = cfg[\"decoder\"][\"embeddings\"].get(\"dropout\", dec_dropout)\n",
    "    \n",
    "    encoder = pretrained_model.encoder\n",
    "    encoder.train()\n",
    "    set_requires_grad(encoder, True)\n",
    "\n",
    "    # build encoder\n",
    "    #enc_dropout = cfg[\"encoder\"].get(\"dropout\", 0.)\n",
    "    #enc_emb_dropout = cfg[\"encoder\"][\"embeddings\"].get(\"dropout\", enc_dropout)\n",
    "    #if cfg[\"encoder\"].get(\"type\", \"recurrent\") == \"transformer\":\n",
    "    #    assert cfg[\"encoder\"][\"embeddings\"][\"embedding_dim\"] == \\\n",
    "    #           cfg[\"encoder\"][\"hidden_size\"], \\\n",
    "    #           \"for transformer, emb_size must be hidden_size\"\n",
    "\n",
    "    #    encoder = TransformerEncoder(**cfg[\"encoder\"],\n",
    "    #                                 emb_size=src_embed.embedding_dim,\n",
    "    #                                 emb_dropout=enc_emb_dropout)\n",
    "    #else:\n",
    "    #    encoder = RecurrentEncoder(**cfg[\"encoder\"],\n",
    "    #                               emb_size=src_embed.embedding_dim,\n",
    "    #                               emb_dropout=enc_emb_dropout)\n",
    "    \n",
    "    if cfg[\"decoder\"].get(\"type\", \"recurrent\") == \"transformer\":\n",
    "        decoder = TransformerDecoder(\n",
    "            **cfg[\"decoder\"], encoder=encoder, vocab_size=len(trg_vocab),\n",
    "            emb_size=trg_embed.embedding_dim, emb_dropout=dec_emb_dropout)\n",
    "    else:\n",
    "        decoder = RecurrentDecoder(\n",
    "            **cfg[\"decoder\"], encoder=encoder, vocab_size=len(trg_vocab),\n",
    "            emb_size=trg_embed.embedding_dim, emb_dropout=dec_emb_dropout)\n",
    "\n",
    "    model = Model(encoder=encoder, decoder=decoder,\n",
    "                  src_embed=src_embed, trg_embed=trg_embed,\n",
    "                  src_vocab=pretrained_model.src_vocab, trg_vocab=trg_vocab)\n",
    "\n",
    "    # tie softmax layer with trg embeddings\n",
    "    if cfg.get(\"tied_softmax\", False):\n",
    "        if trg_embed.lut.weight.shape == \\\n",
    "                model.decoder.output_layer.weight.shape:\n",
    "            # (also) share trg embeddings and softmax layer:\n",
    "            model.decoder.output_layer.weight = trg_embed.lut.weight\n",
    "        else:\n",
    "            raise ConfigurationError(\n",
    "                \"For tied_softmax, the decoder embedding_dim and decoder \"\n",
    "                \"hidden_size must be the same.\"\n",
    "                \"The decoder must be a Transformer.\")\n",
    "\n",
    "    # custom initialization of model parameters\n",
    "    initialize_model(model, cfg, src_padding_idx, trg_padding_idx)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager:\n",
    "    \"\"\" Manages training loop, validations, learning rate scheduling\n",
    "    and early stopping.\"\"\"\n",
    "\n",
    "    def __init__(self, model: Model, config: dict, training_key: str=\"training\") -> None:\n",
    "        \"\"\"\n",
    "        Creates a new TrainManager for a model, specified as in configuration.\n",
    "\n",
    "        :param model: torch module defining the model\n",
    "        :param config: dictionary containing the training configurations\n",
    "        \"\"\"\n",
    "        train_config = config[training_key]\n",
    "\n",
    "        # files for logging and storing\n",
    "        self.model_dir = make_model_dir(train_config[\"model_dir\"],\n",
    "                                        overwrite=train_config.get(\n",
    "                                            \"overwrite\", False))\n",
    "        self.logger = make_logger(\"{}/train.log\".format(self.model_dir))\n",
    "        self.logging_freq = train_config.get(\"logging_freq\", 100)\n",
    "        self.valid_report_file = \"{}/validations.txt\".format(self.model_dir)\n",
    "        self.tb_writer = SummaryWriter(log_dir=self.model_dir+\"/tensorboard/\")\n",
    "\n",
    "        # model\n",
    "        self.model = model\n",
    "        self.pad_index = self.model.pad_index\n",
    "        self.bos_index = self.model.bos_index\n",
    "        self._log_parameters_list()\n",
    "\n",
    "        # objective\n",
    "        self.label_smoothing = train_config.get(\"label_smoothing\", 0.0)\n",
    "        self.loss = XentLoss(pad_index=self.pad_index,\n",
    "                             smoothing=self.label_smoothing)\n",
    "        self.normalization = train_config.get(\"normalization\", \"batch\")\n",
    "        if self.normalization not in [\"batch\", \"tokens\"]:\n",
    "            raise ConfigurationError(\"Invalid normalization. \"\n",
    "                                     \"Valid options: 'batch', 'tokens'.\")\n",
    "\n",
    "        # optimization\n",
    "        self.learning_rate_min = train_config.get(\"learning_rate_min\", 1.0e-8)\n",
    "\n",
    "        self.clip_grad_fun = build_gradient_clipper(config=train_config)\n",
    "        self.optimizer = build_optimizer(config=train_config,\n",
    "                                         parameters=model.parameters())\n",
    "\n",
    "        # validation & early stopping\n",
    "        self.validation_freq = train_config.get(\"validation_freq\", 1000)\n",
    "        self.log_valid_sents = train_config.get(\"print_valid_sents\", [0, 1, 2])\n",
    "        self.ckpt_queue = queue.Queue(\n",
    "            maxsize=train_config.get(\"keep_last_ckpts\", 5))\n",
    "        self.eval_metric = train_config.get(\"eval_metric\", \"bleu\")\n",
    "        if self.eval_metric not in ['bleu', 'chrf']:\n",
    "            raise ConfigurationError(\"Invalid setting for 'eval_metric', \"\n",
    "                                     \"valid options: 'bleu', 'chrf'.\")\n",
    "        self.early_stopping_metric = train_config.get(\"early_stopping_metric\",\n",
    "                                                      \"eval_metric\")\n",
    "\n",
    "        # if we schedule after BLEU/chrf, we want to maximize it, else minimize\n",
    "        # early_stopping_metric decides on how to find the early stopping point:\n",
    "        # ckpts are written when there's a new high/low score for this metric\n",
    "        if self.early_stopping_metric in [\"ppl\", \"loss\"]:\n",
    "            self.minimize_metric = True\n",
    "        elif self.early_stopping_metric == \"eval_metric\":\n",
    "            if self.eval_metric in [\"bleu\", \"chrf\"]:\n",
    "                self.minimize_metric = False\n",
    "            else:  # eval metric that has to get minimized (not yet implemented)\n",
    "                self.minimize_metric = True\n",
    "        else:\n",
    "            raise ConfigurationError(\n",
    "                \"Invalid setting for 'early_stopping_metric', \"\n",
    "                \"valid options: 'loss', 'ppl', 'eval_metric'.\")\n",
    "\n",
    "        # learning rate scheduling\n",
    "        self.scheduler, self.scheduler_step_at = build_scheduler(\n",
    "            config=train_config,\n",
    "            scheduler_mode=\"min\" if self.minimize_metric else \"max\",\n",
    "            optimizer=self.optimizer,\n",
    "            hidden_size=config[\"model\"][\"encoder\"][\"hidden_size\"])\n",
    "\n",
    "        # data & batch handling\n",
    "        self.level = config[\"data\"][\"level\"]\n",
    "        if self.level not in [\"word\", \"bpe\", \"char\", \"syl\", \"bpe_drop\"]:\n",
    "            raise ConfigurationError(\"Invalid segmentation level. \"\n",
    "                                     \"Valid options: 'word', 'bpe', 'char'.\")\n",
    "        self.shuffle = train_config.get(\"shuffle\", True)\n",
    "        self.epochs = train_config[\"epochs\"]\n",
    "        self.batch_size = train_config[\"batch_size\"]\n",
    "        self.batch_type = train_config.get(\"batch_type\", \"sentence\")\n",
    "        self.eval_batch_size = train_config.get(\"eval_batch_size\",\n",
    "                                                self.batch_size)\n",
    "        self.eval_batch_type = train_config.get(\"eval_batch_type\",\n",
    "                                                self.batch_type)\n",
    "\n",
    "        self.batch_multiplier = train_config.get(\"batch_multiplier\", 1)\n",
    "\n",
    "        # generation\n",
    "        self.max_output_length = train_config.get(\"max_output_length\", None)\n",
    "\n",
    "        # CPU / GPU\n",
    "        self.use_cuda = train_config[\"use_cuda\"]\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "            self.loss.cuda()\n",
    "\n",
    "        # initialize training statistics\n",
    "        self.steps = 0\n",
    "        # stop training if this flag is True by reaching learning rate minimum\n",
    "        self.stop = False\n",
    "        self.total_tokens = 0\n",
    "        self.best_ckpt_iteration = 0\n",
    "        # initial values for best scores\n",
    "        self.best_ckpt_score = np.inf if self.minimize_metric else -np.inf\n",
    "        # comparison function for scores\n",
    "        self.is_best = lambda score: score < self.best_ckpt_score \\\n",
    "            if self.minimize_metric else score > self.best_ckpt_score\n",
    "\n",
    "        # model parameters\n",
    "        if \"load_model\" in train_config.keys():\n",
    "            model_load_path = train_config[\"load_model\"]\n",
    "            self.logger.info(\"Loading model from %s\", model_load_path)\n",
    "            reset_best_ckpt = train_config.get(\"reset_best_ckpt\", False)\n",
    "            reset_scheduler = train_config.get(\"reset_scheduler\", False)\n",
    "            reset_optimizer = train_config.get(\"reset_optimizer\", False)\n",
    "            self.init_from_checkpoint(model_load_path,\n",
    "                                      reset_best_ckpt=reset_best_ckpt,\n",
    "                                      reset_scheduler=reset_scheduler,\n",
    "                                      reset_optimizer=reset_optimizer)\n",
    "\n",
    "    def _save_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "        Save the model's current parameters and the training state to a\n",
    "        checkpoint.\n",
    "\n",
    "        The training state contains the total number of training steps,\n",
    "        the total number of training tokens,\n",
    "        the best checkpoint score and iteration so far,\n",
    "        and optimizer and scheduler states.\n",
    "\n",
    "        \"\"\"\n",
    "        model_path = \"{}/{}.ckpt\".format(self.model_dir, self.steps)\n",
    "        state = {\n",
    "            \"steps\": self.steps,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"best_ckpt_score\": self.best_ckpt_score,\n",
    "            \"best_ckpt_iteration\": self.best_ckpt_iteration,\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optimizer_state\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state\": self.scheduler.state_dict() if \\\n",
    "            self.scheduler is not None else None,\n",
    "        }\n",
    "        torch.save(state, model_path)\n",
    "        if self.ckpt_queue.full():\n",
    "            to_delete = self.ckpt_queue.get()  # delete oldest ckpt\n",
    "            try:\n",
    "                os.remove(to_delete)\n",
    "            except FileNotFoundError:\n",
    "                self.logger.warning(\"Wanted to delete old checkpoint %s but \"\n",
    "                                    \"file does not exist.\", to_delete)\n",
    "\n",
    "        self.ckpt_queue.put(model_path)\n",
    "\n",
    "        best_path = \"{}/best.ckpt\".format(self.model_dir)\n",
    "        try:\n",
    "            # create/modify symbolic link for best checkpoint\n",
    "            symlink_update(\"{}.ckpt\".format(self.steps), best_path)\n",
    "        except OSError:\n",
    "            # overwrite best.ckpt\n",
    "            torch.save(state, best_path)\n",
    "\n",
    "    def init_from_checkpoint(self, path: str,\n",
    "                             reset_best_ckpt: bool = False,\n",
    "                             reset_scheduler: bool = False,\n",
    "                             reset_optimizer: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the trainer from a given checkpoint file.\n",
    "\n",
    "        This checkpoint file contains not only model parameters, but also\n",
    "        scheduler and optimizer states, see `self._save_checkpoint`.\n",
    "\n",
    "        :param path: path to checkpoint\n",
    "        :param reset_best_ckpt: reset tracking of the best checkpoint,\n",
    "                                use for domain adaptation with a new dev\n",
    "                                set or when using a new metric for fine-tuning.\n",
    "        :param reset_scheduler: reset the learning rate scheduler, and do not\n",
    "                                use the one stored in the checkpoint.\n",
    "        :param reset_optimizer: reset the optimizer, and do not use the one\n",
    "                                stored in the checkpoint.\n",
    "        \"\"\"\n",
    "        model_checkpoint = load_checkpoint(path=path, use_cuda=self.use_cuda)\n",
    "\n",
    "        # restore model and optimizer parameters\n",
    "        self.model.load_state_dict(model_checkpoint[\"model_state\"])\n",
    "\n",
    "        if not reset_optimizer:\n",
    "            self.optimizer.load_state_dict(model_checkpoint[\"optimizer_state\"])\n",
    "        else:\n",
    "            self.logger.info(\"Reset optimizer.\")\n",
    "\n",
    "        if not reset_scheduler:\n",
    "            if model_checkpoint[\"scheduler_state\"] is not None and \\\n",
    "                    self.scheduler is not None:\n",
    "                self.scheduler.load_state_dict(\n",
    "                    model_checkpoint[\"scheduler_state\"])\n",
    "        else:\n",
    "            self.logger.info(\"Reset scheduler.\")\n",
    "\n",
    "        # restore counts\n",
    "        self.steps = model_checkpoint[\"steps\"]\n",
    "        self.total_tokens = model_checkpoint[\"total_tokens\"]\n",
    "\n",
    "        if not reset_best_ckpt:\n",
    "            self.best_ckpt_score = model_checkpoint[\"best_ckpt_score\"]\n",
    "            self.best_ckpt_iteration = model_checkpoint[\"best_ckpt_iteration\"]\n",
    "        else:\n",
    "            self.logger.info(\"Reset tracking of the best checkpoint.\")\n",
    "\n",
    "        # move parameters to cuda\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "    # pylint: disable=unnecessary-comprehension\n",
    "    def train_and_validate(self, train_data: Dataset, valid_data: Dataset) \\\n",
    "            -> None:\n",
    "        \"\"\"\n",
    "        Train the model and validate it from time to time on the validation set.\n",
    "\n",
    "        :param train_data: training data\n",
    "        :param valid_data: validation data\n",
    "        \"\"\"\n",
    "        train_iter = make_data_iter(train_data,\n",
    "                                    batch_size=self.batch_size,\n",
    "                                    batch_type=self.batch_type,\n",
    "                                    train=True, shuffle=self.shuffle)\n",
    "        for epoch_no in range(self.epochs):\n",
    "            self.logger.info(\"EPOCH %d\", epoch_no + 1)\n",
    "\n",
    "            if self.scheduler is not None and self.scheduler_step_at == \"epoch\":\n",
    "                self.scheduler.step(epoch=epoch_no)\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            # Reset statistics for each epoch.\n",
    "            start = time.time()\n",
    "            total_valid_duration = 0\n",
    "            start_tokens = self.total_tokens\n",
    "            count = self.batch_multiplier - 1\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for batch in iter(train_iter):\n",
    "                # reactivate training\n",
    "                self.model.train()\n",
    "                # create a Batch object from torchtext batch\n",
    "                batch = Batch(batch, self.pad_index, use_cuda=self.use_cuda)\n",
    "\n",
    "                # only update every batch_multiplier batches\n",
    "                # see https://medium.com/@davidlmorton/\n",
    "                # increasing-mini-batch-size-without-increasing-\n",
    "                # memory-6794e10db672\n",
    "                update = count == 0\n",
    "                # print(count, update, self.steps)\n",
    "                batch_loss = self._train_batch(batch, update=update)\n",
    "                self.tb_writer.add_scalar(\"train/train_batch_loss\", batch_loss,\n",
    "                                          self.steps)\n",
    "                count = self.batch_multiplier if update else count\n",
    "                count -= 1\n",
    "                epoch_loss += batch_loss.detach().cpu().numpy()\n",
    "\n",
    "                if self.scheduler is not None and \\\n",
    "                        self.scheduler_step_at == \"step\" and update:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                # log learning progress\n",
    "                if self.steps % self.logging_freq == 0 and update:\n",
    "                    elapsed = time.time() - start - total_valid_duration\n",
    "                    elapsed_tokens = self.total_tokens - start_tokens\n",
    "                    self.logger.info(\n",
    "                        \"Epoch %3d Step: %8d Batch Loss: %12.6f \"\n",
    "                        \"Tokens per Sec: %8.0f, Lr: %.6f\",\n",
    "                        epoch_no + 1, self.steps, batch_loss,\n",
    "                        elapsed_tokens / elapsed,\n",
    "                        self.optimizer.param_groups[0][\"lr\"])\n",
    "                    start = time.time()\n",
    "                    total_valid_duration = 0\n",
    "                    start_tokens = self.total_tokens\n",
    "\n",
    "                # validate on the entire dev set\n",
    "                if self.steps % self.validation_freq == 0 and update:\n",
    "                    valid_start_time = time.time()\n",
    "\n",
    "                    valid_score, valid_loss, valid_ppl, valid_sources, \\\n",
    "                        valid_sources_raw, valid_references, valid_hypotheses, \\\n",
    "                        valid_hypotheses_raw, valid_attention_scores = \\\n",
    "                        validate_on_data(\n",
    "                            logger=self.logger,\n",
    "                            batch_size=self.eval_batch_size,\n",
    "                            data=valid_data,\n",
    "                            eval_metric=self.eval_metric,\n",
    "                            level=self.level, model=self.model,\n",
    "                            use_cuda=self.use_cuda,\n",
    "                            max_output_length=self.max_output_length,\n",
    "                            loss_function=self.loss,\n",
    "                            beam_size=1,  # greedy validations\n",
    "                            batch_type=self.eval_batch_type\n",
    "                        )\n",
    "\n",
    "                    self.tb_writer.add_scalar(\"valid/valid_loss\",\n",
    "                                              valid_loss, self.steps)\n",
    "                    self.tb_writer.add_scalar(\"valid/valid_score\",\n",
    "                                              valid_score, self.steps)\n",
    "                    self.tb_writer.add_scalar(\"valid/valid_ppl\",\n",
    "                                              valid_ppl, self.steps)\n",
    "\n",
    "                    if self.early_stopping_metric == \"loss\":\n",
    "                        ckpt_score = valid_loss\n",
    "                    elif self.early_stopping_metric in [\"ppl\", \"perplexity\"]:\n",
    "                        ckpt_score = valid_ppl\n",
    "                    else:\n",
    "                        ckpt_score = valid_score\n",
    "\n",
    "                    new_best = False\n",
    "                    if self.is_best(ckpt_score):\n",
    "                        self.best_ckpt_score = ckpt_score\n",
    "                        self.best_ckpt_iteration = self.steps\n",
    "                        self.logger.info(\n",
    "                            'Hooray! New best validation result [%s]!',\n",
    "                            self.early_stopping_metric)\n",
    "                        if self.ckpt_queue.maxsize > 0:\n",
    "                            self.logger.info(\"Saving new checkpoint.\")\n",
    "                            new_best = True\n",
    "                            self._save_checkpoint()\n",
    "\n",
    "                    if self.scheduler is not None \\\n",
    "                            and self.scheduler_step_at == \"validation\":\n",
    "                        self.scheduler.step(ckpt_score)\n",
    "\n",
    "                    # append to validation report\n",
    "                    self._add_report(\n",
    "                        valid_score=valid_score, valid_loss=valid_loss,\n",
    "                        valid_ppl=valid_ppl, eval_metric=self.eval_metric,\n",
    "                        new_best=new_best)\n",
    "\n",
    "                    self._log_examples(\n",
    "                        sources_raw=[v for v in valid_sources_raw],\n",
    "                        sources=valid_sources,\n",
    "                        hypotheses_raw=valid_hypotheses_raw,\n",
    "                        hypotheses=valid_hypotheses,\n",
    "                        references=valid_references\n",
    "                    )\n",
    "\n",
    "                    valid_duration = time.time() - valid_start_time\n",
    "                    total_valid_duration += valid_duration\n",
    "                    self.logger.info(\n",
    "                        'Validation result (greedy) at epoch %3d, '\n",
    "                        'step %8d: %s: %6.2f, loss: %8.4f, ppl: %8.4f, '\n",
    "                        'duration: %.4fs', epoch_no+1, self.steps,\n",
    "                        self.eval_metric, valid_score, valid_loss,\n",
    "                        valid_ppl, valid_duration)\n",
    "\n",
    "                    # store validation set outputs\n",
    "                    self._store_outputs(valid_hypotheses)\n",
    "\n",
    "                    # store attention plots for selected valid sentences\n",
    "                    if valid_attention_scores:\n",
    "                        store_attention_plots(\n",
    "                            attentions=valid_attention_scores,\n",
    "                            targets=valid_hypotheses_raw,\n",
    "                            sources=[s for s in valid_data.src],\n",
    "                            indices=self.log_valid_sents,\n",
    "                            output_prefix=\"{}/att.{}\".format(\n",
    "                                self.model_dir, self.steps),\n",
    "                            tb_writer=self.tb_writer, steps=self.steps)\n",
    "\n",
    "                if self.stop:\n",
    "                    break\n",
    "            if self.stop:\n",
    "                self.logger.info(\n",
    "                    'Training ended since minimum lr %f was reached.',\n",
    "                     self.learning_rate_min)\n",
    "                break\n",
    "\n",
    "            self.logger.info('Epoch %3d: total training loss %.2f', epoch_no+1,\n",
    "                             epoch_loss)\n",
    "        else:\n",
    "            self.logger.info('Training ended after %3d epochs.', epoch_no+1)\n",
    "        self.logger.info('Best validation result (greedy) at step '\n",
    "                         '%8d: %6.2f %s.', self.best_ckpt_iteration,\n",
    "                         self.best_ckpt_score,\n",
    "                         self.early_stopping_metric)\n",
    "\n",
    "        self.tb_writer.close()  # close Tensorboard writer\n",
    "\n",
    "    def _train_batch(self, batch: Batch, update: bool = True) -> Tensor:\n",
    "        \"\"\"\n",
    "        Train the model on one batch: Compute the loss, make a gradient step.\n",
    "\n",
    "        :param batch: training batch\n",
    "        :param update: if False, only store gradient. if True also make update\n",
    "        :return: loss for batch (sum)\n",
    "        \"\"\"\n",
    "        batch_loss = self.model.get_loss_for_batch(\n",
    "            batch=batch, loss_function=self.loss)\n",
    "\n",
    "        # normalize batch loss\n",
    "        if self.normalization == \"batch\":\n",
    "            normalizer = batch.nseqs\n",
    "        elif self.normalization == \"tokens\":\n",
    "            normalizer = batch.ntokens\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only normalize by 'batch' or 'tokens'\")\n",
    "\n",
    "        norm_batch_loss = batch_loss / normalizer\n",
    "        # division needed since loss.backward sums the gradients until updated\n",
    "        norm_batch_multiply = norm_batch_loss / self.batch_multiplier\n",
    "\n",
    "        # compute gradients\n",
    "        norm_batch_multiply.backward()\n",
    "\n",
    "        if self.clip_grad_fun is not None:\n",
    "            # clip gradients (in-place)\n",
    "            self.clip_grad_fun(params=self.model.parameters())\n",
    "\n",
    "        if update:\n",
    "            # make gradient step\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # increment step counter\n",
    "            self.steps += 1\n",
    "\n",
    "        # increment token counter\n",
    "        self.total_tokens += batch.ntokens\n",
    "\n",
    "        return norm_batch_loss\n",
    "\n",
    "    def _add_report(self, valid_score: float, valid_ppl: float,\n",
    "                    valid_loss: float, eval_metric: str,\n",
    "                    new_best: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Append a one-line report to validation logging file.\n",
    "\n",
    "        :param valid_score: validation evaluation score [eval_metric]\n",
    "        :param valid_ppl: validation perplexity\n",
    "        :param valid_loss: validation loss (sum over whole validation set)\n",
    "        :param eval_metric: evaluation metric, e.g. \"bleu\"\n",
    "        :param new_best: whether this is a new best model\n",
    "        \"\"\"\n",
    "        current_lr = -1\n",
    "        # ignores other param groups for now\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "\n",
    "        if current_lr < self.learning_rate_min:\n",
    "            self.stop = True\n",
    "\n",
    "        with open(self.valid_report_file, 'a') as opened_file:\n",
    "            opened_file.write(\n",
    "                \"Steps: {}\\tLoss: {:.5f}\\tPPL: {:.5f}\\t{}: {:.5f}\\t\"\n",
    "                \"LR: {:.8f}\\t{}\\n\".format(\n",
    "                    self.steps, valid_loss, valid_ppl, eval_metric,\n",
    "                    valid_score, current_lr, \"*\" if new_best else \"\"))\n",
    "\n",
    "    def _log_parameters_list(self) -> None:\n",
    "        \"\"\"\n",
    "        Write all model parameters (name, shape) to the log.\n",
    "        \"\"\"\n",
    "        model_parameters = filter(lambda p: p.requires_grad,\n",
    "                                  self.model.parameters())\n",
    "        n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        self.logger.info(\"Total params: %d\", n_params)\n",
    "        trainable_params = [n for (n, p) in self.model.named_parameters()\n",
    "                            if p.requires_grad]\n",
    "        self.logger.info(\"Trainable parameters: %s\", sorted(trainable_params))\n",
    "        assert trainable_params\n",
    "\n",
    "    def _log_examples(self, sources: List[str], hypotheses: List[str],\n",
    "                      references: List[str],\n",
    "                      sources_raw: List[List[str]] = None,\n",
    "                      hypotheses_raw: List[List[str]] = None,\n",
    "                      references_raw: List[List[str]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Log a the first `self.log_valid_sents` sentences from given examples.\n",
    "\n",
    "        :param sources: decoded sources (list of strings)\n",
    "        :param hypotheses: decoded hypotheses (list of strings)\n",
    "        :param references: decoded references (list of strings)\n",
    "        :param sources_raw: raw sources (list of list of tokens)\n",
    "        :param hypotheses_raw: raw hypotheses (list of list of tokens)\n",
    "        :param references_raw: raw references (list of list of tokens)\n",
    "        \"\"\"\n",
    "        for p in self.log_valid_sents:\n",
    "\n",
    "            if p >= len(sources):\n",
    "                continue\n",
    "\n",
    "            self.logger.info(\"Example #%d\", p)\n",
    "\n",
    "            if sources_raw is not None:\n",
    "                self.logger.debug(\"\\tRaw source:     %s\", sources_raw[p])\n",
    "            if references_raw is not None:\n",
    "                self.logger.debug(\"\\tRaw reference:  %s\", references_raw[p])\n",
    "            if hypotheses_raw is not None:\n",
    "                self.logger.debug(\"\\tRaw hypothesis: %s\", hypotheses_raw[p])\n",
    "\n",
    "            self.logger.info(\"\\tSource:     %s\", sources[p])\n",
    "            self.logger.info(\"\\tReference:  %s\", references[p])\n",
    "            self.logger.info(\"\\tHypothesis: %s\", hypotheses[p])\n",
    "\n",
    "    def _store_outputs(self, hypotheses: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Write current validation outputs to file in `self.model_dir.`\n",
    "\n",
    "        :param hypotheses: list of strings\n",
    "        \"\"\"\n",
    "        current_valid_output_file = \"{}/{}.hyps\".format(self.model_dir,\n",
    "                                                        self.steps)\n",
    "        with open(current_valid_output_file, 'w') as opened_file:\n",
    "            for hyp in hypotheses:\n",
    "                opened_file.write(\"{}\\n\".format(hyp))\n",
    "\n",
    "def train(cfg_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Main training function. After training, also test on test data if given.\n",
    "\n",
    "    :param cfg_file: path to configuration yaml file\n",
    "    \"\"\"\n",
    "    cfg = load_config(cfg_file)\n",
    "\n",
    "    # set the random seed\n",
    "    set_seed(seed=cfg[\"pretraining\"].get(\"random_seed\", 42))\n",
    "\n",
    "    # load the data\n",
    "    pre_train_data, pre_dev_data, pre_test_data, pre_src_vocab, pre_trg_vocab = load_data(\n",
    "        data_cfg=cfg[\"pretrained_data\"])\n",
    "\n",
    "    # build an encoder-decoder model\n",
    "    pretrained_model = build_model(cfg[\"model\"], src_vocab=pre_src_vocab, trg_vocab=pre_trg_vocab)\n",
    "\n",
    "    # for training management, e.g. early stopping and model selection\n",
    "    trainer = TrainManager(model=pretrained_model, config=cfg, training_key=\"pretraining\")\n",
    "\n",
    "    # store copy of original training config in model dir\n",
    "    shutil.copy2(cfg_file, trainer.model_dir+\"/config.yaml\")\n",
    "\n",
    "    # log all entries of config\n",
    "    log_cfg(cfg, trainer.logger)\n",
    "\n",
    "    log_data_info(train_data=pre_train_data, valid_data=pre_dev_data,\n",
    "                  test_data=pre_test_data, src_vocab=pre_src_vocab, trg_vocab=pre_trg_vocab,\n",
    "                  logging_function=trainer.logger.info)\n",
    "\n",
    "    trainer.logger.info(str(pretrained_model))\n",
    "\n",
    "    # store the vocabs\n",
    "    src_vocab_file = \"{}/src_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    pre_src_vocab.to_file(src_vocab_file)\n",
    "    trg_vocab_file = \"{}/trg_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    pre_trg_vocab.to_file(trg_vocab_file)\n",
    "\n",
    "    # train the model\n",
    "    trainer.train_and_validate(train_data=pre_train_data, valid_data=pre_dev_data)\n",
    "\n",
    "    # predict with the best model on validation and test\n",
    "    # (if test data is available)\n",
    "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
    "    output_name = \"{:08d}.hyps\".format(trainer.best_ckpt_iteration)\n",
    "    output_path = os.path.join(trainer.model_dir, output_name)\n",
    "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=trainer.logger)\n",
    "    \n",
    "    # set the random seed\n",
    "    set_seed(seed=cfg[\"pretraining\"].get(\"random_seed\", 42))\n",
    "\n",
    "    # load the data\n",
    "    train_data, dev_data, test_data, src_vocab, trg_vocab = load_data(\n",
    "        data_cfg=cfg[\"data\"])\n",
    "\n",
    "    # build an encoder-decoder model\n",
    "    model = build_pretrained_model(cfg[\"model\"], \n",
    "                                   pretrained_model=pretrained_model, \n",
    "                                   pretrained_src_vocab=pre_src_vocab,\n",
    "                                   src_vocab=src_vocab, \n",
    "                                   trg_vocab=trg_vocab)\n",
    "\n",
    "    # for training management, e.g. early stopping and model selection\n",
    "    trainer = TrainManager(model=model, config=cfg, training_key=\"training\")\n",
    "\n",
    "    # store copy of original training config in model dir\n",
    "    shutil.copy2(cfg_file, trainer.model_dir+\"/config.yaml\")\n",
    "\n",
    "    # log all entries of config\n",
    "    log_cfg(cfg, trainer.logger)\n",
    "\n",
    "    log_data_info(train_data=train_data, valid_data=dev_data,\n",
    "                  test_data=test_data, src_vocab=src_vocab, trg_vocab=trg_vocab,\n",
    "                  logging_function=trainer.logger.info)\n",
    "\n",
    "    trainer.logger.info(str(model))\n",
    "\n",
    "    # store the vocabs\n",
    "    src_vocab_file = \"{}/src_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    src_vocab.to_file(src_vocab_file)\n",
    "    trg_vocab_file = \"{}/trg_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    trg_vocab.to_file(trg_vocab_file)\n",
    "\n",
    "    # train the model\n",
    "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
    "\n",
    "    # predict with the best model on validation and test\n",
    "    # (if test data is available)\n",
    "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
    "    output_name = \"{:08d}.hyps\".format(trainer.best_ckpt_iteration)\n",
    "    output_path = os.path.join(trainer.model_dir, output_name)\n",
    "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=trainer.logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "pretrained_data: # specify your data here\n",
    "    src: {pretrained_lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {pretrained_lang_tgt}                       # trg language\n",
    "    train: {pretrained_train_path}     # training data\n",
    "    dev: {pretrained_dev_path}         # development data for validation\n",
    "    test: {pretrained_test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 1                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: False                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "pretraining:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0002           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.00001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 1                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: False                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_config = config.format(lang_src='es', lang_tgt='shp', \n",
    "                    train_path=os.path.join('data/translate/preprocessed/Religioso/word', 'train'),\n",
    "                    test_path=os.path.join('data/translate/preprocessed/Religioso/word', 'test'),\n",
    "                    dev_path=os.path.join('data/translate/preprocessed/Religioso/word', 'valid'),\n",
    "                    pretrained_lang_src='es', pretrained_lang_tgt='shp', \n",
    "                    pretrained_train_path=os.path.join('data/translate/preprocessed/Educativo/word', 'train'),\n",
    "                    pretrained_test_path=os.path.join('data/translate/preprocessed/Educativo/word', 'test'),\n",
    "                    pretrained_dev_path=os.path.join('data/translate/preprocessed/Educativo/word', 'valid'),\n",
    "                    level='word',\n",
    "                    emb_size=2,\n",
    "                    hidden_size=5,\n",
    "                    val_freq=2,\n",
    "                    model_dir=os.path.join('results/temp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\"),'w') as f:\n",
    "    f.write(f_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-26 12:17:26,210 Hello! This is Joey-NMT.\n",
      "2020-02-26 12:17:27,364 Total params: 31891\n",
      "2020-02-26 12:17:27,365 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-26 12:17:27,366 cfg.name                           : my_experiment\n",
      "2020-02-26 12:17:27,367 cfg.data.src                       : es\n",
      "2020-02-26 12:17:27,368 cfg.data.trg                       : shp\n",
      "2020-02-26 12:17:27,368 cfg.data.train                     : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-26 12:17:27,369 cfg.data.dev                       : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-26 12:17:27,369 cfg.data.test                      : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-26 12:17:27,370 cfg.data.level                     : word\n",
      "2020-02-26 12:17:27,370 cfg.data.lowercase                 : True\n",
      "2020-02-26 12:17:27,371 cfg.data.max_sent_length           : 150\n",
      "2020-02-26 12:17:27,372 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-26 12:17:27,372 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-26 12:17:27,373 cfg.pretrained_data.src            : es\n",
      "2020-02-26 12:17:27,373 cfg.pretrained_data.trg            : shp\n",
      "2020-02-26 12:17:27,374 cfg.pretrained_data.train          : data/translate/preprocessed/Educativo/word/train\n",
      "2020-02-26 12:17:27,374 cfg.pretrained_data.dev            : data/translate/preprocessed/Educativo/word/valid\n",
      "2020-02-26 12:17:27,375 cfg.pretrained_data.test           : data/translate/preprocessed/Educativo/word/test\n",
      "2020-02-26 12:17:27,375 cfg.pretrained_data.level          : word\n",
      "2020-02-26 12:17:27,376 cfg.pretrained_data.lowercase      : True\n",
      "2020-02-26 12:17:27,376 cfg.pretrained_data.max_sent_length : 150\n",
      "2020-02-26 12:17:27,377 cfg.pretrained_data.src_voc_min_freq : 1\n",
      "2020-02-26 12:17:27,378 cfg.pretrained_data.trg_voc_min_freq : 1\n",
      "2020-02-26 12:17:27,378 cfg.testing.beam_size              : 5\n",
      "2020-02-26 12:17:27,379 cfg.testing.alpha                  : 1.0\n",
      "2020-02-26 12:17:27,379 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-26 12:17:27,380 cfg.training.reset_scheduler       : False\n",
      "2020-02-26 12:17:27,380 cfg.training.reset_optimizer       : False\n",
      "2020-02-26 12:17:27,381 cfg.training.random_seed           : 42\n",
      "2020-02-26 12:17:27,381 cfg.training.optimizer             : adam\n",
      "2020-02-26 12:17:27,382 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-26 12:17:27,382 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-26 12:17:27,383 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-26 12:17:27,383 cfg.training.weight_decay          : 0.0\n",
      "2020-02-26 12:17:27,384 cfg.training.batch_size            : 48\n",
      "2020-02-26 12:17:27,384 cfg.training.batch_type            : sentence\n",
      "2020-02-26 12:17:27,385 cfg.training.eval_batch_size       : 10\n",
      "2020-02-26 12:17:27,385 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-26 12:17:27,386 cfg.training.batch_multiplier      : 1\n",
      "2020-02-26 12:17:27,386 cfg.training.scheduling            : plateau\n",
      "2020-02-26 12:17:27,387 cfg.training.patience              : 600\n",
      "2020-02-26 12:17:27,387 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-26 12:17:27,388 cfg.training.epochs                : 1\n",
      "2020-02-26 12:17:27,388 cfg.training.validation_freq       : 2\n",
      "2020-02-26 12:17:27,389 cfg.training.logging_freq          : 1000\n",
      "2020-02-26 12:17:27,389 cfg.training.eval_metric           : bleu\n",
      "2020-02-26 12:17:27,390 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-26 12:17:27,390 cfg.training.model_dir             : results/temp\n",
      "2020-02-26 12:17:27,391 cfg.training.overwrite             : True\n",
      "2020-02-26 12:17:27,391 cfg.training.shuffle               : True\n",
      "2020-02-26 12:17:27,392 cfg.training.use_cuda              : False\n",
      "2020-02-26 12:17:27,392 cfg.training.max_output_length     : 60\n",
      "2020-02-26 12:17:27,393 cfg.training.print_valid_sents     : []\n",
      "2020-02-26 12:17:27,393 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-26 12:17:27,394 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-26 12:17:27,394 cfg.pretraining.reset_best_ckpt    : False\n",
      "2020-02-26 12:17:27,395 cfg.pretraining.reset_scheduler    : False\n",
      "2020-02-26 12:17:27,396 cfg.pretraining.reset_optimizer    : False\n",
      "2020-02-26 12:17:27,396 cfg.pretraining.random_seed        : 42\n",
      "2020-02-26 12:17:27,397 cfg.pretraining.optimizer          : adam\n",
      "2020-02-26 12:17:27,397 cfg.pretraining.learning_rate      : 0.0002\n",
      "2020-02-26 12:17:27,398 cfg.pretraining.learning_rate_min  : 1e-05\n",
      "2020-02-26 12:17:27,398 cfg.pretraining.clip_grad_val      : 1.0\n",
      "2020-02-26 12:17:27,399 cfg.pretraining.weight_decay       : 0.0\n",
      "2020-02-26 12:17:27,399 cfg.pretraining.batch_size         : 48\n",
      "2020-02-26 12:17:27,399 cfg.pretraining.batch_type         : sentence\n",
      "2020-02-26 12:17:27,400 cfg.pretraining.eval_batch_size    : 10\n",
      "2020-02-26 12:17:27,400 cfg.pretraining.eval_batch_type    : sentence\n",
      "2020-02-26 12:17:27,401 cfg.pretraining.batch_multiplier   : 1\n",
      "2020-02-26 12:17:27,401 cfg.pretraining.scheduling         : plateau\n",
      "2020-02-26 12:17:27,402 cfg.pretraining.patience           : 600\n",
      "2020-02-26 12:17:27,402 cfg.pretraining.decrease_factor    : 0.5\n",
      "2020-02-26 12:17:27,403 cfg.pretraining.epochs             : 1\n",
      "2020-02-26 12:17:27,403 cfg.pretraining.validation_freq    : 2\n",
      "2020-02-26 12:17:27,404 cfg.pretraining.logging_freq       : 1000\n",
      "2020-02-26 12:17:27,404 cfg.pretraining.eval_metric        : bleu\n",
      "2020-02-26 12:17:27,405 cfg.pretraining.early_stopping_metric : eval_metric\n",
      "2020-02-26 12:17:27,405 cfg.pretraining.model_dir          : results/temp\n",
      "2020-02-26 12:17:27,406 cfg.pretraining.overwrite          : True\n",
      "2020-02-26 12:17:27,406 cfg.pretraining.shuffle            : True\n",
      "2020-02-26 12:17:27,407 cfg.pretraining.use_cuda           : False\n",
      "2020-02-26 12:17:27,407 cfg.pretraining.max_output_length  : 60\n",
      "2020-02-26 12:17:27,408 cfg.pretraining.print_valid_sents  : []\n",
      "2020-02-26 12:17:27,408 cfg.pretraining.keep_last_ckpts    : 3\n",
      "2020-02-26 12:17:27,409 cfg.pretraining.label_smoothing    : 0.0\n",
      "2020-02-26 12:17:27,409 cfg.model.initializer              : xavier\n",
      "2020-02-26 12:17:27,410 cfg.model.init_weight              : 0.01\n",
      "2020-02-26 12:17:27,410 cfg.model.init_gain                : 1.0\n",
      "2020-02-26 12:17:27,411 cfg.model.bias_initializer         : zeros\n",
      "2020-02-26 12:17:27,412 cfg.model.embed_initializer        : normal\n",
      "2020-02-26 12:17:27,412 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-26 12:17:27,413 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-26 12:17:27,413 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-26 12:17:27,425 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-26 12:17:27,425 cfg.model.tied_embeddings          : False\n",
      "2020-02-26 12:17:27,426 cfg.model.tied_softmax             : False\n",
      "2020-02-26 12:17:27,426 cfg.model.encoder.type             : recurrent\n",
      "2020-02-26 12:17:27,427 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-26 12:17:27,427 cfg.model.encoder.embeddings.embedding_dim : 2\n",
      "2020-02-26 12:17:27,428 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-26 12:17:27,428 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-26 12:17:27,428 cfg.model.encoder.hidden_size      : 5\n",
      "2020-02-26 12:17:27,429 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-26 12:17:27,429 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-26 12:17:27,430 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-26 12:17:27,432 cfg.model.encoder.freeze           : False\n",
      "2020-02-26 12:17:27,432 cfg.model.decoder.type             : recurrent\n",
      "2020-02-26 12:17:27,433 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-26 12:17:27,433 cfg.model.decoder.embeddings.embedding_dim : 2\n",
      "2020-02-26 12:17:27,434 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-26 12:17:27,434 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-26 12:17:27,435 cfg.model.decoder.hidden_size      : 5\n",
      "2020-02-26 12:17:27,435 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-26 12:17:27,435 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-26 12:17:27,436 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-26 12:17:27,436 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-26 12:17:27,437 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-26 12:17:27,437 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-26 12:17:27,438 cfg.model.decoder.freeze           : False\n",
      "2020-02-26 12:17:27,438 Data set sizes: \n",
      "\ttrain 3979,\n",
      "\tvalid 498,\n",
      "\ttest 498\n",
      "2020-02-26 12:17:27,439 First training example:\n",
      "\t[SRC] nenora non aki kai tanakin westiorabo kirika wishamaxon onankiakana ixon\n",
      "\t[TRG] se evaluará mediante problemas propuestos en una hoja de aplicación individual\n",
      "2020-02-26 12:17:27,439 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) itan (5) ja (6) kopi (7) ? (8) ¿ (9) yoyo\n",
      "2020-02-26 12:17:27,440 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) de (5) y (6) que (7) los (8) la (9) en\n",
      "2020-02-26 12:17:27,440 Number of Src words (types): 4056\n",
      "2020-02-26 12:17:27,441 Number of Trg words (types): 3207\n",
      "2020-02-26 12:17:27,441 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(2, 5, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(7, 5, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=2, vocab_size=4056),\n",
      "\ttrg_embed=Embeddings(embedding_dim=2, vocab_size=3207))\n",
      "2020-02-26 12:17:27,447 EPOCH 1\n",
      "2020-02-26 12:17:30,745 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-26 12:17:30,746 Saving new checkpoint.\n",
      "2020-02-26 12:17:30,749 Validation result (greedy) at epoch   1, step        2: bleu:   0.00, loss: 39097.5117, ppl: 3206.6890, duration: 3.1857s\n",
      "2020-02-26 12:17:33,960 Validation result (greedy) at epoch   1, step        4: bleu:   0.00, loss: 39097.0977, ppl: 3206.4138, duration: 3.0696s\n",
      "2020-02-26 12:17:37,524 Validation result (greedy) at epoch   1, step        6: bleu:   0.00, loss: 39096.5742, ppl: 3206.0684, duration: 3.4476s\n",
      "2020-02-26 12:17:40,860 Validation result (greedy) at epoch   1, step        8: bleu:   0.00, loss: 39095.9648, ppl: 3205.6648, duration: 3.2118s\n",
      "2020-02-26 12:17:44,158 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 39095.2500, ppl: 3205.1909, duration: 3.2095s\n",
      "2020-02-26 12:17:47,661 Validation result (greedy) at epoch   1, step       12: bleu:   0.00, loss: 39094.4492, ppl: 3204.6621, duration: 3.3071s\n",
      "2020-02-26 12:17:51,078 Validation result (greedy) at epoch   1, step       14: bleu:   0.00, loss: 39093.5078, ppl: 3204.0388, duration: 3.2599s\n",
      "2020-02-26 12:17:54,438 Validation result (greedy) at epoch   1, step       16: bleu:   0.00, loss: 39092.4375, ppl: 3203.3330, duration: 3.2169s\n",
      "2020-02-26 12:17:57,806 Validation result (greedy) at epoch   1, step       18: bleu:   0.00, loss: 39091.3320, ppl: 3202.5999, duration: 3.2438s\n",
      "2020-02-26 12:18:02,085 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 39090.2031, ppl: 3201.8547, duration: 4.1941s\n",
      "2020-02-26 12:18:05,285 Validation result (greedy) at epoch   1, step       22: bleu:   0.00, loss: 39088.8945, ppl: 3200.9875, duration: 3.0762s\n",
      "2020-02-26 12:18:08,458 Validation result (greedy) at epoch   1, step       24: bleu:   0.00, loss: 39087.3945, ppl: 3199.9988, duration: 3.0653s\n",
      "2020-02-26 12:18:11,676 Validation result (greedy) at epoch   1, step       26: bleu:   0.00, loss: 39085.6328, ppl: 3198.8330, duration: 3.0708s\n",
      "2020-02-26 12:18:14,875 Validation result (greedy) at epoch   1, step       28: bleu:   0.00, loss: 39083.6055, ppl: 3197.4941, duration: 3.0688s\n",
      "2020-02-26 12:18:18,379 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 39081.5312, ppl: 3196.1252, duration: 3.4258s\n",
      "2020-02-26 12:18:21,647 Validation result (greedy) at epoch   1, step       32: bleu:   0.00, loss: 39079.4414, ppl: 3194.7478, duration: 3.1916s\n",
      "2020-02-26 12:18:24,892 Validation result (greedy) at epoch   1, step       34: bleu:   0.00, loss: 39077.3125, ppl: 3193.3435, duration: 3.1552s\n",
      "2020-02-26 12:18:28,077 Validation result (greedy) at epoch   1, step       36: bleu:   0.00, loss: 39074.9531, ppl: 3191.7878, duration: 3.0882s\n",
      "2020-02-26 12:18:31,225 Validation result (greedy) at epoch   1, step       38: bleu:   0.00, loss: 39072.3320, ppl: 3190.0593, duration: 3.0453s\n",
      "2020-02-26 12:18:34,397 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 39069.5703, ppl: 3188.2405, duration: 3.0744s\n",
      "2020-02-26 12:18:37,591 Validation result (greedy) at epoch   1, step       42: bleu:   0.00, loss: 39066.5273, ppl: 3186.2405, duration: 3.0868s\n",
      "2020-02-26 12:18:40,754 Validation result (greedy) at epoch   1, step       44: bleu:   0.00, loss: 39063.3750, ppl: 3184.1658, duration: 3.0579s\n",
      "2020-02-26 12:18:44,303 Validation result (greedy) at epoch   1, step       46: bleu:   0.00, loss: 39060.2656, ppl: 3182.1228, duration: 3.4460s\n",
      "2020-02-26 12:18:47,599 Validation result (greedy) at epoch   1, step       48: bleu:   0.00, loss: 39057.0234, ppl: 3179.9932, duration: 3.1751s\n",
      "2020-02-26 12:18:50,903 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 39053.3125, ppl: 3177.5559, duration: 3.1957s\n",
      "2020-02-26 12:18:54,142 Validation result (greedy) at epoch   1, step       52: bleu:   0.00, loss: 39049.4336, ppl: 3175.0144, duration: 3.1309s\n",
      "2020-02-26 12:18:57,919 Validation result (greedy) at epoch   1, step       54: bleu:   0.00, loss: 39044.9336, ppl: 3172.0635, duration: 3.5410s\n",
      "2020-02-26 12:19:00,878 Validation result (greedy) at epoch   1, step       56: bleu:   0.00, loss: 39039.7734, ppl: 3168.6863, duration: 2.7985s\n",
      "2020-02-26 12:19:03,452 Validation result (greedy) at epoch   1, step       58: bleu:   0.00, loss: 39034.8086, ppl: 3165.4395, duration: 2.5207s\n",
      "2020-02-26 12:19:05,893 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 39029.7148, ppl: 3162.1113, duration: 2.3268s\n",
      "2020-02-26 12:19:08,433 Validation result (greedy) at epoch   1, step       62: bleu:   0.00, loss: 39023.9609, ppl: 3158.3562, duration: 2.4099s\n",
      "2020-02-26 12:19:10,341 Validation result (greedy) at epoch   1, step       64: bleu:   0.00, loss: 39017.9062, ppl: 3154.4099, duration: 1.7660s\n",
      "2020-02-26 12:19:12,056 Validation result (greedy) at epoch   1, step       66: bleu:   0.00, loss: 39011.9453, ppl: 3150.5315, duration: 1.6423s\n",
      "2020-02-26 12:19:13,707 Validation result (greedy) at epoch   1, step       68: bleu:   0.00, loss: 39005.9922, ppl: 3146.6611, duration: 1.5634s\n",
      "2020-02-26 12:19:15,244 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 38999.6836, ppl: 3142.5645, duration: 1.3983s\n",
      "2020-02-26 12:19:16,665 Validation result (greedy) at epoch   1, step       72: bleu:   0.00, loss: 38992.6797, ppl: 3138.0244, duration: 1.2434s\n",
      "2020-02-26 12:19:18,195 Validation result (greedy) at epoch   1, step       74: bleu:   0.00, loss: 38985.0820, ppl: 3133.1052, duration: 1.3314s\n",
      "2020-02-26 12:19:19,370 Validation result (greedy) at epoch   1, step       76: bleu:   0.00, loss: 38977.4336, ppl: 3128.1611, duration: 1.0714s\n",
      "2020-02-26 12:19:20,616 Validation result (greedy) at epoch   1, step       78: bleu:   0.00, loss: 38969.8281, ppl: 3123.2515, duration: 1.1201s\n",
      "2020-02-26 12:19:21,908 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 38962.1992, ppl: 3118.3350, duration: 1.1619s\n",
      "2020-02-26 12:19:23,337 Validation result (greedy) at epoch   1, step       82: bleu:   0.00, loss: 38954.6211, ppl: 3113.4585, duration: 1.3223s\n",
      "2020-02-26 12:19:23,388 Epoch   1: total training loss 6565.01\n",
      "2020-02-26 12:19:23,389 Training ended after   1 epochs.\n",
      "2020-02-26 12:19:23,389 Best validation result (greedy) at step        2:   0.00 eval_metric.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tsize mismatch for src_embed.lut.weight: copying a param with shape torch.Size([4056, 2]) from checkpoint, the shape in current model is torch.Size([11507, 2]).\n\tsize mismatch for trg_embed.lut.weight: copying a param with shape torch.Size([3207, 2]) from checkpoint, the shape in current model is torch.Size([9005, 2]).\n\tsize mismatch for decoder.output_layer.weight: copying a param with shape torch.Size([3207, 5]) from checkpoint, the shape in current model is torch.Size([9005, 5]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a822bc7ccb9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"joeynmt/configs/sample_{name}.yaml\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9c274c525599>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cfg_file)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0moutput_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:08d}.hyps\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_ckpt_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;31m# set the random seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joeynmt-0.0.1-py3.7.egg/joeynmt/prediction.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(cfg_file, ckpt, output_path, save_attention, logger)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;31m# build model and load parameters into it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 777\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tsize mismatch for src_embed.lut.weight: copying a param with shape torch.Size([4056, 2]) from checkpoint, the shape in current model is torch.Size([11507, 2]).\n\tsize mismatch for trg_embed.lut.weight: copying a param with shape torch.Size([3207, 2]) from checkpoint, the shape in current model is torch.Size([9005, 2]).\n\tsize mismatch for decoder.output_layer.weight: copying a param with shape torch.Size([3207, 5]) from checkpoint, the shape in current model is torch.Size([9005, 5])."
     ]
    }
   ],
   "source": [
    "train(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
