{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "from typing import List\n",
    "import os\n",
    "import queue\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.data import Dataset\n",
    "\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import log_data_info, load_config, log_cfg, \\\n",
    "    store_attention_plots, load_checkpoint, make_model_dir, \\\n",
    "    make_logger, set_seed, symlink_update, ConfigurationError\n",
    "from joeynmt.model import Model\n",
    "from joeynmt.prediction import validate_on_data\n",
    "from joeynmt.loss import XentLoss\n",
    "from joeynmt.data import load_data, make_data_iter\n",
    "from joeynmt.builders import build_optimizer, build_scheduler, \\\n",
    "    build_gradient_clipper\n",
    "from joeynmt.training import TrainManager\n",
    "from joeynmt.prediction import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joeynmt.model import Model\n",
    "from joeynmt.initialization import initialize_model\n",
    "from joeynmt.embeddings import Embeddings\n",
    "from joeynmt.encoders import Encoder, RecurrentEncoder, TransformerEncoder\n",
    "from joeynmt.decoders import Decoder, RecurrentDecoder, TransformerDecoder\n",
    "from joeynmt.constants import PAD_TOKEN, EOS_TOKEN, BOS_TOKEN\n",
    "from joeynmt.search import beam_search, greedy\n",
    "from joeynmt.vocabulary import Vocabulary\n",
    "from joeynmt.batch import Batch\n",
    "from joeynmt.helpers import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(module, val):\n",
    "    for p in module.parameters():\n",
    "        p = p.detach()\n",
    "        p.requires_grad = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "def build_pretrained_model(cfg: dict = None,\n",
    "                pretrained_model: Model = None,\n",
    "                pretrained_src_vocab: Vocabulary = None,\n",
    "                src_vocab: Vocabulary = None,\n",
    "                trg_vocab: Vocabulary = None) -> Model:\n",
    "    \"\"\"\n",
    "    Build and initialize the model according to the configuration.\n",
    "\n",
    "    :param cfg: dictionary configuration containing model specifications\n",
    "    :param src_vocab: source vocabulary\n",
    "    :param trg_vocab: target vocabulary\n",
    "    :return: built and initialized model\n",
    "    \"\"\"\n",
    "    src_padding_idx = src_vocab.stoi[PAD_TOKEN]\n",
    "    trg_padding_idx = trg_vocab.stoi[PAD_TOKEN]\n",
    "\n",
    "    src_embed = Embeddings(\n",
    "        **cfg[\"encoder\"][\"embeddings\"], vocab_size=len(src_vocab),\n",
    "        padding_idx=src_padding_idx)\n",
    "\n",
    "    embedding_matrix = np.zeros((len(pretrained_src_vocab.itos), src_embed.embedding_dim))\n",
    "    unknown_words = []\n",
    "    for w in pretrained_src_vocab.itos:\n",
    "        try:\n",
    "            ix = pretrained_src_vocab.stoi[w]\n",
    "            embedding_matrix[ix] = pretrained_model.src_embed.lut.weight[ix].cpu().detach().numpy()\n",
    "        except KeyError:\n",
    "            unknown_words.append(w)\n",
    "    \n",
    "    src_embed.lut.weight = torch.nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "\n",
    "    trg_embed = Embeddings(\n",
    "            **cfg[\"decoder\"][\"embeddings\"], vocab_size=len(trg_vocab),\n",
    "            padding_idx=trg_padding_idx)\n",
    "\n",
    "    # build decoder\n",
    "    dec_dropout = cfg[\"decoder\"].get(\"dropout\", 0.)\n",
    "    dec_emb_dropout = cfg[\"decoder\"][\"embeddings\"].get(\"dropout\", dec_dropout)\n",
    "    \n",
    "    encoder = pretrained_model.encoder\n",
    "    encoder.train()\n",
    "    set_requires_grad(encoder, True)\n",
    "\n",
    "    # build encoder\n",
    "    #enc_dropout = cfg[\"encoder\"].get(\"dropout\", 0.)\n",
    "    #enc_emb_dropout = cfg[\"encoder\"][\"embeddings\"].get(\"dropout\", enc_dropout)\n",
    "    #if cfg[\"encoder\"].get(\"type\", \"recurrent\") == \"transformer\":\n",
    "    #    assert cfg[\"encoder\"][\"embeddings\"][\"embedding_dim\"] == \\\n",
    "    #           cfg[\"encoder\"][\"hidden_size\"], \\\n",
    "    #           \"for transformer, emb_size must be hidden_size\"\n",
    "\n",
    "    #    encoder = TransformerEncoder(**cfg[\"encoder\"],\n",
    "    #                                 emb_size=src_embed.embedding_dim,\n",
    "    #                                 emb_dropout=enc_emb_dropout)\n",
    "    #else:\n",
    "    #    encoder = RecurrentEncoder(**cfg[\"encoder\"],\n",
    "    #                               emb_size=src_embed.embedding_dim,\n",
    "    #                               emb_dropout=enc_emb_dropout)\n",
    "    \n",
    "    if cfg[\"decoder\"].get(\"type\", \"recurrent\") == \"transformer\":\n",
    "        decoder = TransformerDecoder(\n",
    "            **cfg[\"decoder\"], encoder=encoder, vocab_size=len(trg_vocab),\n",
    "            emb_size=trg_embed.embedding_dim, emb_dropout=dec_emb_dropout)\n",
    "    else:\n",
    "        decoder = RecurrentDecoder(\n",
    "            **cfg[\"decoder\"], encoder=encoder, vocab_size=len(trg_vocab),\n",
    "            emb_size=trg_embed.embedding_dim, emb_dropout=dec_emb_dropout)\n",
    "\n",
    "    model = Model(encoder=encoder, decoder=decoder,\n",
    "                  src_embed=src_embed, trg_embed=trg_embed,\n",
    "                  src_vocab=pretrained_model.src_vocab, trg_vocab=trg_vocab)\n",
    "\n",
    "    # tie softmax layer with trg embeddings\n",
    "    if cfg.get(\"tied_softmax\", False):\n",
    "        if trg_embed.lut.weight.shape == \\\n",
    "                model.decoder.output_layer.weight.shape:\n",
    "            # (also) share trg embeddings and softmax layer:\n",
    "            model.decoder.output_layer.weight = trg_embed.lut.weight\n",
    "        else:\n",
    "            raise ConfigurationError(\n",
    "                \"For tied_softmax, the decoder embedding_dim and decoder \"\n",
    "                \"hidden_size must be the same.\"\n",
    "                \"The decoder must be a Transformer.\")\n",
    "\n",
    "    # custom initialization of model parameters\n",
    "    initialize_model(model, cfg, src_padding_idx, trg_padding_idx)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager:\n",
    "    \"\"\" Manages training loop, validations, learning rate scheduling\n",
    "    and early stopping.\"\"\"\n",
    "\n",
    "    def __init__(self, model: Model, config: dict, training_key: str=\"training\") -> None:\n",
    "        \"\"\"\n",
    "        Creates a new TrainManager for a model, specified as in configuration.\n",
    "\n",
    "        :param model: torch module defining the model\n",
    "        :param config: dictionary containing the training configurations\n",
    "        \"\"\"\n",
    "        train_config = config[training_key]\n",
    "\n",
    "        # files for logging and storing\n",
    "        self.model_dir = make_model_dir(train_config[\"model_dir\"],\n",
    "                                        overwrite=train_config.get(\n",
    "                                            \"overwrite\", False))\n",
    "        self.logger = make_logger(\"{}/train.log\".format(self.model_dir))\n",
    "        self.logging_freq = train_config.get(\"logging_freq\", 100)\n",
    "        self.valid_report_file = \"{}/validations.txt\".format(self.model_dir)\n",
    "        self.tb_writer = SummaryWriter(log_dir=self.model_dir+\"/tensorboard/\")\n",
    "\n",
    "        # model\n",
    "        self.model = model\n",
    "        self.pad_index = self.model.pad_index\n",
    "        self.bos_index = self.model.bos_index\n",
    "        self._log_parameters_list()\n",
    "\n",
    "        # objective\n",
    "        self.label_smoothing = train_config.get(\"label_smoothing\", 0.0)\n",
    "        self.loss = XentLoss(pad_index=self.pad_index,\n",
    "                             smoothing=self.label_smoothing)\n",
    "        self.normalization = train_config.get(\"normalization\", \"batch\")\n",
    "        if self.normalization not in [\"batch\", \"tokens\"]:\n",
    "            raise ConfigurationError(\"Invalid normalization. \"\n",
    "                                     \"Valid options: 'batch', 'tokens'.\")\n",
    "\n",
    "        # optimization\n",
    "        self.learning_rate_min = train_config.get(\"learning_rate_min\", 1.0e-8)\n",
    "\n",
    "        self.clip_grad_fun = build_gradient_clipper(config=train_config)\n",
    "        self.optimizer = build_optimizer(config=train_config,\n",
    "                                         parameters=model.parameters())\n",
    "\n",
    "        # validation & early stopping\n",
    "        self.validation_freq = train_config.get(\"validation_freq\", 1000)\n",
    "        self.log_valid_sents = train_config.get(\"print_valid_sents\", [0, 1, 2])\n",
    "        self.ckpt_queue = queue.Queue(\n",
    "            maxsize=train_config.get(\"keep_last_ckpts\", 5))\n",
    "        self.eval_metric = train_config.get(\"eval_metric\", \"bleu\")\n",
    "        if self.eval_metric not in ['bleu', 'chrf']:\n",
    "            raise ConfigurationError(\"Invalid setting for 'eval_metric', \"\n",
    "                                     \"valid options: 'bleu', 'chrf'.\")\n",
    "        self.early_stopping_metric = train_config.get(\"early_stopping_metric\",\n",
    "                                                      \"eval_metric\")\n",
    "\n",
    "        # if we schedule after BLEU/chrf, we want to maximize it, else minimize\n",
    "        # early_stopping_metric decides on how to find the early stopping point:\n",
    "        # ckpts are written when there's a new high/low score for this metric\n",
    "        if self.early_stopping_metric in [\"ppl\", \"loss\"]:\n",
    "            self.minimize_metric = True\n",
    "        elif self.early_stopping_metric == \"eval_metric\":\n",
    "            if self.eval_metric in [\"bleu\", \"chrf\"]:\n",
    "                self.minimize_metric = False\n",
    "            else:  # eval metric that has to get minimized (not yet implemented)\n",
    "                self.minimize_metric = True\n",
    "        else:\n",
    "            raise ConfigurationError(\n",
    "                \"Invalid setting for 'early_stopping_metric', \"\n",
    "                \"valid options: 'loss', 'ppl', 'eval_metric'.\")\n",
    "\n",
    "        # learning rate scheduling\n",
    "        self.scheduler, self.scheduler_step_at = build_scheduler(\n",
    "            config=train_config,\n",
    "            scheduler_mode=\"min\" if self.minimize_metric else \"max\",\n",
    "            optimizer=self.optimizer,\n",
    "            hidden_size=config[\"model\"][\"encoder\"][\"hidden_size\"])\n",
    "\n",
    "        # data & batch handling\n",
    "        self.level = config[\"data\"][\"level\"]\n",
    "        if self.level not in [\"word\", \"bpe\", \"char\", \"syl\", \"bpe_drop\"]:\n",
    "            raise ConfigurationError(\"Invalid segmentation level. \"\n",
    "                                     \"Valid options: 'word', 'bpe', 'char'.\")\n",
    "        self.shuffle = train_config.get(\"shuffle\", True)\n",
    "        self.epochs = train_config[\"epochs\"]\n",
    "        self.batch_size = train_config[\"batch_size\"]\n",
    "        self.batch_type = train_config.get(\"batch_type\", \"sentence\")\n",
    "        self.eval_batch_size = train_config.get(\"eval_batch_size\",\n",
    "                                                self.batch_size)\n",
    "        self.eval_batch_type = train_config.get(\"eval_batch_type\",\n",
    "                                                self.batch_type)\n",
    "\n",
    "        self.batch_multiplier = train_config.get(\"batch_multiplier\", 1)\n",
    "\n",
    "        # generation\n",
    "        self.max_output_length = train_config.get(\"max_output_length\", None)\n",
    "\n",
    "        # CPU / GPU\n",
    "        self.use_cuda = train_config[\"use_cuda\"]\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "            self.loss.cuda()\n",
    "\n",
    "        # initialize training statistics\n",
    "        self.steps = 0\n",
    "        # stop training if this flag is True by reaching learning rate minimum\n",
    "        self.stop = False\n",
    "        self.total_tokens = 0\n",
    "        self.best_ckpt_iteration = 0\n",
    "        # initial values for best scores\n",
    "        self.best_ckpt_score = np.inf if self.minimize_metric else -np.inf\n",
    "        # comparison function for scores\n",
    "        self.is_best = lambda score: score < self.best_ckpt_score \\\n",
    "            if self.minimize_metric else score > self.best_ckpt_score\n",
    "\n",
    "        # model parameters\n",
    "        if \"load_model\" in train_config.keys():\n",
    "            model_load_path = train_config[\"load_model\"]\n",
    "            self.logger.info(\"Loading model from %s\", model_load_path)\n",
    "            reset_best_ckpt = train_config.get(\"reset_best_ckpt\", False)\n",
    "            reset_scheduler = train_config.get(\"reset_scheduler\", False)\n",
    "            reset_optimizer = train_config.get(\"reset_optimizer\", False)\n",
    "            self.init_from_checkpoint(model_load_path,\n",
    "                                      reset_best_ckpt=reset_best_ckpt,\n",
    "                                      reset_scheduler=reset_scheduler,\n",
    "                                      reset_optimizer=reset_optimizer)\n",
    "\n",
    "    def _save_checkpoint(self) -> None:\n",
    "        \"\"\"\n",
    "        Save the model's current parameters and the training state to a\n",
    "        checkpoint.\n",
    "\n",
    "        The training state contains the total number of training steps,\n",
    "        the total number of training tokens,\n",
    "        the best checkpoint score and iteration so far,\n",
    "        and optimizer and scheduler states.\n",
    "\n",
    "        \"\"\"\n",
    "        model_path = \"{}/{}.ckpt\".format(self.model_dir, self.steps)\n",
    "        state = {\n",
    "            \"steps\": self.steps,\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"best_ckpt_score\": self.best_ckpt_score,\n",
    "            \"best_ckpt_iteration\": self.best_ckpt_iteration,\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optimizer_state\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state\": self.scheduler.state_dict() if \\\n",
    "            self.scheduler is not None else None,\n",
    "        }\n",
    "        torch.save(state, model_path)\n",
    "        if self.ckpt_queue.full():\n",
    "            to_delete = self.ckpt_queue.get()  # delete oldest ckpt\n",
    "            try:\n",
    "                os.remove(to_delete)\n",
    "            except FileNotFoundError:\n",
    "                self.logger.warning(\"Wanted to delete old checkpoint %s but \"\n",
    "                                    \"file does not exist.\", to_delete)\n",
    "\n",
    "        self.ckpt_queue.put(model_path)\n",
    "\n",
    "        best_path = \"{}/best.ckpt\".format(self.model_dir)\n",
    "        try:\n",
    "            # create/modify symbolic link for best checkpoint\n",
    "            symlink_update(\"{}.ckpt\".format(self.steps), best_path)\n",
    "        except OSError:\n",
    "            # overwrite best.ckpt\n",
    "            torch.save(state, best_path)\n",
    "\n",
    "    def init_from_checkpoint(self, path: str,\n",
    "                             reset_best_ckpt: bool = False,\n",
    "                             reset_scheduler: bool = False,\n",
    "                             reset_optimizer: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the trainer from a given checkpoint file.\n",
    "\n",
    "        This checkpoint file contains not only model parameters, but also\n",
    "        scheduler and optimizer states, see `self._save_checkpoint`.\n",
    "\n",
    "        :param path: path to checkpoint\n",
    "        :param reset_best_ckpt: reset tracking of the best checkpoint,\n",
    "                                use for domain adaptation with a new dev\n",
    "                                set or when using a new metric for fine-tuning.\n",
    "        :param reset_scheduler: reset the learning rate scheduler, and do not\n",
    "                                use the one stored in the checkpoint.\n",
    "        :param reset_optimizer: reset the optimizer, and do not use the one\n",
    "                                stored in the checkpoint.\n",
    "        \"\"\"\n",
    "        model_checkpoint = load_checkpoint(path=path, use_cuda=self.use_cuda)\n",
    "\n",
    "        # restore model and optimizer parameters\n",
    "        self.model.load_state_dict(model_checkpoint[\"model_state\"])\n",
    "\n",
    "        if not reset_optimizer:\n",
    "            self.optimizer.load_state_dict(model_checkpoint[\"optimizer_state\"])\n",
    "        else:\n",
    "            self.logger.info(\"Reset optimizer.\")\n",
    "\n",
    "        if not reset_scheduler:\n",
    "            if model_checkpoint[\"scheduler_state\"] is not None and \\\n",
    "                    self.scheduler is not None:\n",
    "                self.scheduler.load_state_dict(\n",
    "                    model_checkpoint[\"scheduler_state\"])\n",
    "        else:\n",
    "            self.logger.info(\"Reset scheduler.\")\n",
    "\n",
    "        # restore counts\n",
    "        self.steps = model_checkpoint[\"steps\"]\n",
    "        self.total_tokens = model_checkpoint[\"total_tokens\"]\n",
    "\n",
    "        if not reset_best_ckpt:\n",
    "            self.best_ckpt_score = model_checkpoint[\"best_ckpt_score\"]\n",
    "            self.best_ckpt_iteration = model_checkpoint[\"best_ckpt_iteration\"]\n",
    "        else:\n",
    "            self.logger.info(\"Reset tracking of the best checkpoint.\")\n",
    "\n",
    "        # move parameters to cuda\n",
    "        if self.use_cuda:\n",
    "            self.model.cuda()\n",
    "\n",
    "    # pylint: disable=unnecessary-comprehension\n",
    "    def train_and_validate(self, train_data: Dataset, valid_data: Dataset) \\\n",
    "            -> None:\n",
    "        \"\"\"\n",
    "        Train the model and validate it from time to time on the validation set.\n",
    "\n",
    "        :param train_data: training data\n",
    "        :param valid_data: validation data\n",
    "        \"\"\"\n",
    "        train_iter = make_data_iter(train_data,\n",
    "                                    batch_size=self.batch_size,\n",
    "                                    batch_type=self.batch_type,\n",
    "                                    train=True, shuffle=self.shuffle)\n",
    "        for epoch_no in range(self.epochs):\n",
    "            self.logger.info(\"EPOCH %d\", epoch_no + 1)\n",
    "\n",
    "            if self.scheduler is not None and self.scheduler_step_at == \"epoch\":\n",
    "                self.scheduler.step(epoch=epoch_no)\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            # Reset statistics for each epoch.\n",
    "            start = time.time()\n",
    "            total_valid_duration = 0\n",
    "            start_tokens = self.total_tokens\n",
    "            count = self.batch_multiplier - 1\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for batch in iter(train_iter):\n",
    "                # reactivate training\n",
    "                self.model.train()\n",
    "                # create a Batch object from torchtext batch\n",
    "                batch = Batch(batch, self.pad_index, use_cuda=self.use_cuda)\n",
    "\n",
    "                # only update every batch_multiplier batches\n",
    "                # see https://medium.com/@davidlmorton/\n",
    "                # increasing-mini-batch-size-without-increasing-\n",
    "                # memory-6794e10db672\n",
    "                update = count == 0\n",
    "                # print(count, update, self.steps)\n",
    "                batch_loss = self._train_batch(batch, update=update)\n",
    "                self.tb_writer.add_scalar(\"train/train_batch_loss\", batch_loss,\n",
    "                                          self.steps)\n",
    "                count = self.batch_multiplier if update else count\n",
    "                count -= 1\n",
    "                epoch_loss += batch_loss.detach().cpu().numpy()\n",
    "\n",
    "                if self.scheduler is not None and \\\n",
    "                        self.scheduler_step_at == \"step\" and update:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                # log learning progress\n",
    "                if self.steps % self.logging_freq == 0 and update:\n",
    "                    elapsed = time.time() - start - total_valid_duration\n",
    "                    elapsed_tokens = self.total_tokens - start_tokens\n",
    "                    self.logger.info(\n",
    "                        \"Epoch %3d Step: %8d Batch Loss: %12.6f \"\n",
    "                        \"Tokens per Sec: %8.0f, Lr: %.6f\",\n",
    "                        epoch_no + 1, self.steps, batch_loss,\n",
    "                        elapsed_tokens / elapsed,\n",
    "                        self.optimizer.param_groups[0][\"lr\"])\n",
    "                    start = time.time()\n",
    "                    total_valid_duration = 0\n",
    "                    start_tokens = self.total_tokens\n",
    "\n",
    "                # validate on the entire dev set\n",
    "                if self.steps % self.validation_freq == 0 and update:\n",
    "                    valid_start_time = time.time()\n",
    "\n",
    "                    valid_score, valid_loss, valid_ppl, valid_sources, \\\n",
    "                        valid_sources_raw, valid_references, valid_hypotheses, \\\n",
    "                        valid_hypotheses_raw, valid_attention_scores = \\\n",
    "                        validate_on_data(\n",
    "                            logger=self.logger,\n",
    "                            batch_size=self.eval_batch_size,\n",
    "                            data=valid_data,\n",
    "                            eval_metric=self.eval_metric,\n",
    "                            level=self.level, model=self.model,\n",
    "                            use_cuda=self.use_cuda,\n",
    "                            max_output_length=self.max_output_length,\n",
    "                            loss_function=self.loss,\n",
    "                            beam_size=1,  # greedy validations\n",
    "                            batch_type=self.eval_batch_type\n",
    "                        )\n",
    "\n",
    "                    self.tb_writer.add_scalar(\"valid/valid_loss\",\n",
    "                                              valid_loss, self.steps)\n",
    "                    self.tb_writer.add_scalar(\"valid/valid_score\",\n",
    "                                              valid_score, self.steps)\n",
    "                    self.tb_writer.add_scalar(\"valid/valid_ppl\",\n",
    "                                              valid_ppl, self.steps)\n",
    "\n",
    "                    if self.early_stopping_metric == \"loss\":\n",
    "                        ckpt_score = valid_loss\n",
    "                    elif self.early_stopping_metric in [\"ppl\", \"perplexity\"]:\n",
    "                        ckpt_score = valid_ppl\n",
    "                    else:\n",
    "                        ckpt_score = valid_score\n",
    "\n",
    "                    new_best = False\n",
    "                    if self.is_best(ckpt_score):\n",
    "                        self.best_ckpt_score = ckpt_score\n",
    "                        self.best_ckpt_iteration = self.steps\n",
    "                        self.logger.info(\n",
    "                            'Hooray! New best validation result [%s]!',\n",
    "                            self.early_stopping_metric)\n",
    "                        if self.ckpt_queue.maxsize > 0:\n",
    "                            self.logger.info(\"Saving new checkpoint.\")\n",
    "                            new_best = True\n",
    "                            self._save_checkpoint()\n",
    "\n",
    "                    if self.scheduler is not None \\\n",
    "                            and self.scheduler_step_at == \"validation\":\n",
    "                        self.scheduler.step(ckpt_score)\n",
    "\n",
    "                    # append to validation report\n",
    "                    self._add_report(\n",
    "                        valid_score=valid_score, valid_loss=valid_loss,\n",
    "                        valid_ppl=valid_ppl, eval_metric=self.eval_metric,\n",
    "                        new_best=new_best)\n",
    "\n",
    "                    self._log_examples(\n",
    "                        sources_raw=[v for v in valid_sources_raw],\n",
    "                        sources=valid_sources,\n",
    "                        hypotheses_raw=valid_hypotheses_raw,\n",
    "                        hypotheses=valid_hypotheses,\n",
    "                        references=valid_references\n",
    "                    )\n",
    "\n",
    "                    valid_duration = time.time() - valid_start_time\n",
    "                    total_valid_duration += valid_duration\n",
    "                    self.logger.info(\n",
    "                        'Validation result (greedy) at epoch %3d, '\n",
    "                        'step %8d: %s: %6.2f, loss: %8.4f, ppl: %8.4f, '\n",
    "                        'duration: %.4fs', epoch_no+1, self.steps,\n",
    "                        self.eval_metric, valid_score, valid_loss,\n",
    "                        valid_ppl, valid_duration)\n",
    "\n",
    "                    # store validation set outputs\n",
    "                    self._store_outputs(valid_hypotheses)\n",
    "\n",
    "                    # store attention plots for selected valid sentences\n",
    "                    if valid_attention_scores:\n",
    "                        store_attention_plots(\n",
    "                            attentions=valid_attention_scores,\n",
    "                            targets=valid_hypotheses_raw,\n",
    "                            sources=[s for s in valid_data.src],\n",
    "                            indices=self.log_valid_sents,\n",
    "                            output_prefix=\"{}/att.{}\".format(\n",
    "                                self.model_dir, self.steps),\n",
    "                            tb_writer=self.tb_writer, steps=self.steps)\n",
    "\n",
    "                if self.stop:\n",
    "                    break\n",
    "            if self.stop:\n",
    "                self.logger.info(\n",
    "                    'Training ended since minimum lr %f was reached.',\n",
    "                     self.learning_rate_min)\n",
    "                break\n",
    "\n",
    "            self.logger.info('Epoch %3d: total training loss %.2f', epoch_no+1,\n",
    "                             epoch_loss)\n",
    "        else:\n",
    "            self.logger.info('Training ended after %3d epochs.', epoch_no+1)\n",
    "        self.logger.info('Best validation result (greedy) at step '\n",
    "                         '%8d: %6.2f %s.', self.best_ckpt_iteration,\n",
    "                         self.best_ckpt_score,\n",
    "                         self.early_stopping_metric)\n",
    "\n",
    "        self.tb_writer.close()  # close Tensorboard writer\n",
    "\n",
    "    def _train_batch(self, batch: Batch, update: bool = True) -> Tensor:\n",
    "        \"\"\"\n",
    "        Train the model on one batch: Compute the loss, make a gradient step.\n",
    "\n",
    "        :param batch: training batch\n",
    "        :param update: if False, only store gradient. if True also make update\n",
    "        :return: loss for batch (sum)\n",
    "        \"\"\"\n",
    "        batch_loss = self.model.get_loss_for_batch(\n",
    "            batch=batch, loss_function=self.loss)\n",
    "\n",
    "        # normalize batch loss\n",
    "        if self.normalization == \"batch\":\n",
    "            normalizer = batch.nseqs\n",
    "        elif self.normalization == \"tokens\":\n",
    "            normalizer = batch.ntokens\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only normalize by 'batch' or 'tokens'\")\n",
    "\n",
    "        norm_batch_loss = batch_loss / normalizer\n",
    "        # division needed since loss.backward sums the gradients until updated\n",
    "        norm_batch_multiply = norm_batch_loss / self.batch_multiplier\n",
    "\n",
    "        # compute gradients\n",
    "        norm_batch_multiply.backward()\n",
    "\n",
    "        if self.clip_grad_fun is not None:\n",
    "            # clip gradients (in-place)\n",
    "            self.clip_grad_fun(params=self.model.parameters())\n",
    "\n",
    "        if update:\n",
    "            # make gradient step\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # increment step counter\n",
    "            self.steps += 1\n",
    "\n",
    "        # increment token counter\n",
    "        self.total_tokens += batch.ntokens\n",
    "\n",
    "        return norm_batch_loss\n",
    "\n",
    "    def _add_report(self, valid_score: float, valid_ppl: float,\n",
    "                    valid_loss: float, eval_metric: str,\n",
    "                    new_best: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Append a one-line report to validation logging file.\n",
    "\n",
    "        :param valid_score: validation evaluation score [eval_metric]\n",
    "        :param valid_ppl: validation perplexity\n",
    "        :param valid_loss: validation loss (sum over whole validation set)\n",
    "        :param eval_metric: evaluation metric, e.g. \"bleu\"\n",
    "        :param new_best: whether this is a new best model\n",
    "        \"\"\"\n",
    "        current_lr = -1\n",
    "        # ignores other param groups for now\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "\n",
    "        if current_lr < self.learning_rate_min:\n",
    "            self.stop = True\n",
    "\n",
    "        with open(self.valid_report_file, 'a') as opened_file:\n",
    "            opened_file.write(\n",
    "                \"Steps: {}\\tLoss: {:.5f}\\tPPL: {:.5f}\\t{}: {:.5f}\\t\"\n",
    "                \"LR: {:.8f}\\t{}\\n\".format(\n",
    "                    self.steps, valid_loss, valid_ppl, eval_metric,\n",
    "                    valid_score, current_lr, \"*\" if new_best else \"\"))\n",
    "\n",
    "    def _log_parameters_list(self) -> None:\n",
    "        \"\"\"\n",
    "        Write all model parameters (name, shape) to the log.\n",
    "        \"\"\"\n",
    "        model_parameters = filter(lambda p: p.requires_grad,\n",
    "                                  self.model.parameters())\n",
    "        n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        self.logger.info(\"Total params: %d\", n_params)\n",
    "        trainable_params = [n for (n, p) in self.model.named_parameters()\n",
    "                            if p.requires_grad]\n",
    "        self.logger.info(\"Trainable parameters: %s\", sorted(trainable_params))\n",
    "        assert trainable_params\n",
    "\n",
    "    def _log_examples(self, sources: List[str], hypotheses: List[str],\n",
    "                      references: List[str],\n",
    "                      sources_raw: List[List[str]] = None,\n",
    "                      hypotheses_raw: List[List[str]] = None,\n",
    "                      references_raw: List[List[str]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Log a the first `self.log_valid_sents` sentences from given examples.\n",
    "\n",
    "        :param sources: decoded sources (list of strings)\n",
    "        :param hypotheses: decoded hypotheses (list of strings)\n",
    "        :param references: decoded references (list of strings)\n",
    "        :param sources_raw: raw sources (list of list of tokens)\n",
    "        :param hypotheses_raw: raw hypotheses (list of list of tokens)\n",
    "        :param references_raw: raw references (list of list of tokens)\n",
    "        \"\"\"\n",
    "        for p in self.log_valid_sents:\n",
    "\n",
    "            if p >= len(sources):\n",
    "                continue\n",
    "\n",
    "            self.logger.info(\"Example #%d\", p)\n",
    "\n",
    "            if sources_raw is not None:\n",
    "                self.logger.debug(\"\\tRaw source:     %s\", sources_raw[p])\n",
    "            if references_raw is not None:\n",
    "                self.logger.debug(\"\\tRaw reference:  %s\", references_raw[p])\n",
    "            if hypotheses_raw is not None:\n",
    "                self.logger.debug(\"\\tRaw hypothesis: %s\", hypotheses_raw[p])\n",
    "\n",
    "            self.logger.info(\"\\tSource:     %s\", sources[p])\n",
    "            self.logger.info(\"\\tReference:  %s\", references[p])\n",
    "            self.logger.info(\"\\tHypothesis: %s\", hypotheses[p])\n",
    "\n",
    "    def _store_outputs(self, hypotheses: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Write current validation outputs to file in `self.model_dir.`\n",
    "\n",
    "        :param hypotheses: list of strings\n",
    "        \"\"\"\n",
    "        current_valid_output_file = \"{}/{}.hyps\".format(self.model_dir,\n",
    "                                                        self.steps)\n",
    "        with open(current_valid_output_file, 'w') as opened_file:\n",
    "            for hyp in hypotheses:\n",
    "                opened_file.write(\"{}\\n\".format(hyp))\n",
    "\n",
    "def train(cfg_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Main training function. After training, also test on test data if given.\n",
    "\n",
    "    :param cfg_file: path to configuration yaml file\n",
    "    \"\"\"\n",
    "    cfg = load_config(cfg_file)\n",
    "\n",
    "    # set the random seed\n",
    "    set_seed(seed=cfg[\"pretraining\"].get(\"random_seed\", 42))\n",
    "\n",
    "    # load the data\n",
    "    pre_train_data, pre_dev_data, pre_test_data, pre_src_vocab, pre_trg_vocab = load_data(\n",
    "        data_cfg=cfg[\"pretrained_data\"])\n",
    "\n",
    "    # build an encoder-decoder model\n",
    "    pretrained_model = build_model(cfg[\"model\"], src_vocab=pre_src_vocab, trg_vocab=pre_trg_vocab)\n",
    "\n",
    "    # for training management, e.g. early stopping and model selection\n",
    "    trainer = TrainManager(model=pretrained_model, config=cfg, training_key=\"pretraining\")\n",
    "\n",
    "    # store copy of original training config in model dir\n",
    "    shutil.copy2(cfg_file, trainer.model_dir+\"/config.yaml\")\n",
    "\n",
    "    # log all entries of config\n",
    "    log_cfg(cfg, trainer.logger)\n",
    "\n",
    "    log_data_info(train_data=pre_train_data, valid_data=pre_dev_data,\n",
    "                  test_data=pre_test_data, src_vocab=pre_src_vocab, trg_vocab=pre_trg_vocab,\n",
    "                  logging_function=trainer.logger.info)\n",
    "\n",
    "    trainer.logger.info(str(pretrained_model))\n",
    "\n",
    "    # store the vocabs\n",
    "    src_vocab_file = \"{}/src_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    pre_src_vocab.to_file(src_vocab_file)\n",
    "    trg_vocab_file = \"{}/trg_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    pre_trg_vocab.to_file(trg_vocab_file)\n",
    "\n",
    "    # train the model\n",
    "    trainer.train_and_validate(train_data=pre_train_data, valid_data=pre_dev_data)\n",
    "\n",
    "    # predict with the best model on validation and test\n",
    "    # (if test data is available)\n",
    "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
    "    output_name = \"{:08d}.hyps\".format(trainer.best_ckpt_iteration)\n",
    "    output_path = os.path.join(trainer.model_dir, output_name)\n",
    "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=trainer.logger)\n",
    "    \n",
    "    # set the random seed\n",
    "    set_seed(seed=cfg[\"pretraining\"].get(\"random_seed\", 42))\n",
    "\n",
    "    # load the data\n",
    "    train_data, dev_data, test_data, src_vocab, trg_vocab = load_data(\n",
    "        data_cfg=cfg[\"data\"])\n",
    "\n",
    "    # build an encoder-decoder model\n",
    "    model = build_pretrained_model(cfg[\"model\"], \n",
    "                                   pretrained_model=pretrained_model, \n",
    "                                   pretrained_src_vocab=pre_src_vocab,\n",
    "                                   src_vocab=src_vocab, \n",
    "                                   trg_vocab=trg_vocab)\n",
    "\n",
    "    # for training management, e.g. early stopping and model selection\n",
    "    trainer = TrainManager(model=model, config=cfg, training_key=\"training\")\n",
    "\n",
    "    # store copy of original training config in model dir\n",
    "    shutil.copy2(cfg_file, trainer.model_dir+\"/config.yaml\")\n",
    "\n",
    "    # log all entries of config\n",
    "    log_cfg(cfg, trainer.logger)\n",
    "\n",
    "    log_data_info(train_data=train_data, valid_data=dev_data,\n",
    "                  test_data=test_data, src_vocab=src_vocab, trg_vocab=trg_vocab,\n",
    "                  logging_function=trainer.logger.info)\n",
    "\n",
    "    trainer.logger.info(str(model))\n",
    "\n",
    "    # store the vocabs\n",
    "    src_vocab_file = \"{}/src_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    src_vocab.to_file(src_vocab_file)\n",
    "    trg_vocab_file = \"{}/trg_vocab.txt\".format(cfg[\"training\"][\"model_dir\"])\n",
    "    trg_vocab.to_file(trg_vocab_file)\n",
    "\n",
    "    # train the model\n",
    "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)\n",
    "\n",
    "    # predict with the best model on validation and test\n",
    "    # (if test data is available)\n",
    "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
    "    output_name = \"{:08d}.hyps\".format(trainer.best_ckpt_iteration)\n",
    "    output_path = os.path.join(trainer.model_dir, output_name)\n",
    "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=trainer.logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "pretrained_data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 1                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: False                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "pretraining:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0002           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.00001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 600                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 1                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: False                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_config = config.format(lang_src='shp', lang_tgt='es', \n",
    "                    train_path=os.path.join('data/translate/preprocessed/Religioso/word', 'train'),\n",
    "                    test_path=os.path.join('data/translate/preprocessed/Religioso/word', 'test'),\n",
    "                    dev_path=os.path.join('data/translate/preprocessed/Religioso/word', 'valid'),\n",
    "                    level='word',\n",
    "                    emb_size=2,\n",
    "                    hidden_size=5,\n",
    "                    val_freq=2,\n",
    "                    model_dir=os.path.join('results/temp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\"),'w') as f:\n",
    "    f.write(f_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-25 17:22:09,980 Hello! This is Joey-NMT.\n",
      "2020-02-25 17:22:11,119 Total params: 99889\n",
      "2020-02-25 17:22:11,120 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-25 17:22:11,121 cfg.name                           : my_experiment\n",
      "2020-02-25 17:22:11,122 cfg.data.src                       : shp\n",
      "2020-02-25 17:22:11,122 cfg.data.trg                       : es\n",
      "2020-02-25 17:22:11,123 cfg.data.train                     : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-25 17:22:11,123 cfg.data.dev                       : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-25 17:22:11,124 cfg.data.test                      : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-25 17:22:11,124 cfg.data.level                     : word\n",
      "2020-02-25 17:22:11,125 cfg.data.lowercase                 : True\n",
      "2020-02-25 17:22:11,125 cfg.data.max_sent_length           : 150\n",
      "2020-02-25 17:22:11,126 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-25 17:22:11,126 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-25 17:22:11,127 cfg.pretrained_data.src            : shp\n",
      "2020-02-25 17:22:11,127 cfg.pretrained_data.trg            : es\n",
      "2020-02-25 17:22:11,127 cfg.pretrained_data.train          : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-25 17:22:11,128 cfg.pretrained_data.dev            : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-25 17:22:11,128 cfg.pretrained_data.test           : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-25 17:22:11,129 cfg.pretrained_data.level          : word\n",
      "2020-02-25 17:22:11,129 cfg.pretrained_data.lowercase      : True\n",
      "2020-02-25 17:22:11,130 cfg.pretrained_data.max_sent_length : 150\n",
      "2020-02-25 17:22:11,130 cfg.pretrained_data.src_voc_min_freq : 1\n",
      "2020-02-25 17:22:11,131 cfg.pretrained_data.trg_voc_min_freq : 1\n",
      "2020-02-25 17:22:11,131 cfg.testing.beam_size              : 5\n",
      "2020-02-25 17:22:11,131 cfg.testing.alpha                  : 1.0\n",
      "2020-02-25 17:22:11,134 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-25 17:22:11,134 cfg.training.reset_scheduler       : False\n",
      "2020-02-25 17:22:11,135 cfg.training.reset_optimizer       : False\n",
      "2020-02-25 17:22:11,135 cfg.training.random_seed           : 42\n",
      "2020-02-25 17:22:11,136 cfg.training.optimizer             : adam\n",
      "2020-02-25 17:22:11,137 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-25 17:22:11,137 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-25 17:22:11,138 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-25 17:22:11,138 cfg.training.weight_decay          : 0.0\n",
      "2020-02-25 17:22:11,138 cfg.training.batch_size            : 48\n",
      "2020-02-25 17:22:11,139 cfg.training.batch_type            : sentence\n",
      "2020-02-25 17:22:11,139 cfg.training.eval_batch_size       : 10\n",
      "2020-02-25 17:22:11,140 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-25 17:22:11,140 cfg.training.batch_multiplier      : 1\n",
      "2020-02-25 17:22:11,141 cfg.training.scheduling            : plateau\n",
      "2020-02-25 17:22:11,141 cfg.training.patience              : 600\n",
      "2020-02-25 17:22:11,141 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-25 17:22:11,142 cfg.training.epochs                : 1\n",
      "2020-02-25 17:22:11,142 cfg.training.validation_freq       : 2\n",
      "2020-02-25 17:22:11,143 cfg.training.logging_freq          : 1000\n",
      "2020-02-25 17:22:11,143 cfg.training.eval_metric           : bleu\n",
      "2020-02-25 17:22:11,143 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-25 17:22:11,144 cfg.training.model_dir             : results/temp\n",
      "2020-02-25 17:22:11,144 cfg.training.overwrite             : True\n",
      "2020-02-25 17:22:11,145 cfg.training.shuffle               : True\n",
      "2020-02-25 17:22:11,145 cfg.training.use_cuda              : False\n",
      "2020-02-25 17:22:11,145 cfg.training.max_output_length     : 60\n",
      "2020-02-25 17:22:11,146 cfg.training.print_valid_sents     : []\n",
      "2020-02-25 17:22:11,146 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-25 17:22:11,147 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-25 17:22:11,147 cfg.pretraining.reset_best_ckpt    : False\n",
      "2020-02-25 17:22:11,147 cfg.pretraining.reset_scheduler    : False\n",
      "2020-02-25 17:22:11,148 cfg.pretraining.reset_optimizer    : False\n",
      "2020-02-25 17:22:11,148 cfg.pretraining.random_seed        : 42\n",
      "2020-02-25 17:22:11,149 cfg.pretraining.optimizer          : adam\n",
      "2020-02-25 17:22:11,149 cfg.pretraining.learning_rate      : 0.0002\n",
      "2020-02-25 17:22:11,149 cfg.pretraining.learning_rate_min  : 1e-05\n",
      "2020-02-25 17:22:11,150 cfg.pretraining.clip_grad_val      : 1.0\n",
      "2020-02-25 17:22:11,150 cfg.pretraining.weight_decay       : 0.0\n",
      "2020-02-25 17:22:11,151 cfg.pretraining.batch_size         : 48\n",
      "2020-02-25 17:22:11,151 cfg.pretraining.batch_type         : sentence\n",
      "2020-02-25 17:22:11,151 cfg.pretraining.eval_batch_size    : 10\n",
      "2020-02-25 17:22:11,152 cfg.pretraining.eval_batch_type    : sentence\n",
      "2020-02-25 17:22:11,152 cfg.pretraining.batch_multiplier   : 1\n",
      "2020-02-25 17:22:11,153 cfg.pretraining.scheduling         : plateau\n",
      "2020-02-25 17:22:11,153 cfg.pretraining.patience           : 600\n",
      "2020-02-25 17:22:11,154 cfg.pretraining.decrease_factor    : 0.5\n",
      "2020-02-25 17:22:11,158 cfg.pretraining.epochs             : 1\n",
      "2020-02-25 17:22:11,158 cfg.pretraining.validation_freq    : 2\n",
      "2020-02-25 17:22:11,159 cfg.pretraining.logging_freq       : 1000\n",
      "2020-02-25 17:22:11,159 cfg.pretraining.eval_metric        : bleu\n",
      "2020-02-25 17:22:11,160 cfg.pretraining.early_stopping_metric : eval_metric\n",
      "2020-02-25 17:22:11,160 cfg.pretraining.model_dir          : results/temp\n",
      "2020-02-25 17:22:11,161 cfg.pretraining.overwrite          : True\n",
      "2020-02-25 17:22:11,161 cfg.pretraining.shuffle            : True\n",
      "2020-02-25 17:22:11,161 cfg.pretraining.use_cuda           : False\n",
      "2020-02-25 17:22:11,162 cfg.pretraining.max_output_length  : 60\n",
      "2020-02-25 17:22:11,163 cfg.pretraining.print_valid_sents  : []\n",
      "2020-02-25 17:22:11,163 cfg.pretraining.keep_last_ckpts    : 3\n",
      "2020-02-25 17:22:11,164 cfg.pretraining.label_smoothing    : 0.0\n",
      "2020-02-25 17:22:11,164 cfg.model.initializer              : xavier\n",
      "2020-02-25 17:22:11,165 cfg.model.init_weight              : 0.01\n",
      "2020-02-25 17:22:11,165 cfg.model.init_gain                : 1.0\n",
      "2020-02-25 17:22:11,165 cfg.model.bias_initializer         : zeros\n",
      "2020-02-25 17:22:11,166 cfg.model.embed_initializer        : normal\n",
      "2020-02-25 17:22:11,166 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-25 17:22:11,166 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-25 17:22:11,167 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-25 17:22:11,167 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-25 17:22:11,168 cfg.model.tied_embeddings          : False\n",
      "2020-02-25 17:22:11,168 cfg.model.tied_softmax             : False\n",
      "2020-02-25 17:22:11,168 cfg.model.encoder.type             : recurrent\n",
      "2020-02-25 17:22:11,169 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-25 17:22:11,169 cfg.model.encoder.embeddings.embedding_dim : 2\n",
      "2020-02-25 17:22:11,171 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-25 17:22:11,171 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-25 17:22:11,172 cfg.model.encoder.hidden_size      : 5\n",
      "2020-02-25 17:22:11,172 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-25 17:22:11,172 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-25 17:22:11,173 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-25 17:22:11,173 cfg.model.encoder.freeze           : False\n",
      "2020-02-25 17:22:11,174 cfg.model.decoder.type             : recurrent\n",
      "2020-02-25 17:22:11,174 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-25 17:22:11,174 cfg.model.decoder.embeddings.embedding_dim : 2\n",
      "2020-02-25 17:22:11,175 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-25 17:22:11,175 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-25 17:22:11,175 cfg.model.decoder.hidden_size      : 5\n",
      "2020-02-25 17:22:11,176 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-25 17:22:11,176 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-25 17:22:11,177 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-25 17:22:11,177 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-25 17:22:11,177 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-25 17:22:11,178 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-25 17:22:11,178 cfg.model.decoder.freeze           : False\n",
      "2020-02-25 17:22:11,178 Data set sizes: \n",
      "\ttrain 5996,\n",
      "\tvalid 749,\n",
      "\ttest 749\n",
      "2020-02-25 17:22:11,179 First training example:\n",
      "\t[SRC] pero la palabra del seor permanece eternamente esta palabra es el evangelio que se les ha anunciado a ustedes\n",
      "\t[TRG] ikaxbi ja non ibon joira jawetianbi keyyamai iki\n",
      "2020-02-25 17:22:11,179 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) que (5) de (6) y (7) a (8) la (9) el\n",
      "2020-02-25 17:22:11,180 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) iki (5) ja (6) jato (7) mato (8) ea (9) en\n",
      "2020-02-25 17:22:11,180 Number of Src words (types): 9005\n",
      "2020-02-25 17:22:11,181 Number of Trg words (types): 11507\n",
      "2020-02-25 17:22:11,181 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(2, 5, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(7, 5, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=2, vocab_size=9005),\n",
      "\ttrg_embed=Embeddings(embedding_dim=2, vocab_size=11507))\n",
      "2020-02-25 17:22:11,193 EPOCH 1\n",
      "2020-02-25 17:22:17,692 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-25 17:22:17,693 Saving new checkpoint.\n",
      "2020-02-25 17:22:17,696 Validation result (greedy) at epoch   1, step        2: bleu:   0.00, loss: 102604.5703, ppl: 11506.1777, duration: 6.2124s\n",
      "2020-02-25 17:22:23,786 Validation result (greedy) at epoch   1, step        4: bleu:   0.00, loss: 102604.3828, ppl: 11505.9912, duration: 5.8077s\n",
      "2020-02-25 17:22:29,927 Validation result (greedy) at epoch   1, step        6: bleu:   0.00, loss: 102604.2656, ppl: 11505.8594, duration: 5.9063s\n",
      "2020-02-25 17:22:35,962 Validation result (greedy) at epoch   1, step        8: bleu:   0.00, loss: 102604.1484, ppl: 11505.7383, duration: 5.8103s\n",
      "2020-02-25 17:22:42,193 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 102603.9609, ppl: 11505.5410, duration: 6.0637s\n",
      "2020-02-25 17:22:48,451 Validation result (greedy) at epoch   1, step       12: bleu:   0.00, loss: 102603.7031, ppl: 11505.2773, duration: 5.9725s\n",
      "2020-02-25 17:22:54,279 Validation result (greedy) at epoch   1, step       14: bleu:   0.00, loss: 102603.2812, ppl: 11504.8281, duration: 5.6024s\n",
      "2020-02-25 17:23:00,367 Validation result (greedy) at epoch   1, step       16: bleu:   0.00, loss: 102602.8281, ppl: 11504.3564, duration: 5.9704s\n",
      "2020-02-25 17:23:06,199 Validation result (greedy) at epoch   1, step       18: bleu:   0.00, loss: 102602.2891, ppl: 11503.7852, duration: 5.5679s\n",
      "2020-02-25 17:23:11,977 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 102601.6641, ppl: 11503.1387, duration: 5.5691s\n",
      "2020-02-25 17:23:18,035 Validation result (greedy) at epoch   1, step       22: bleu:   0.00, loss: 102600.8672, ppl: 11502.3047, duration: 5.8011s\n",
      "2020-02-25 17:23:24,110 Validation result (greedy) at epoch   1, step       24: bleu:   0.00, loss: 102599.9375, ppl: 11501.3281, duration: 5.8188s\n",
      "2020-02-25 17:23:30,191 Validation result (greedy) at epoch   1, step       26: bleu:   0.00, loss: 102598.8750, ppl: 11500.2100, duration: 5.8115s\n",
      "2020-02-25 17:23:36,759 Validation result (greedy) at epoch   1, step       28: bleu:   0.00, loss: 102597.6719, ppl: 11498.9482, duration: 6.3861s\n",
      "2020-02-25 17:23:42,974 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 102596.3516, ppl: 11497.5664, duration: 5.9579s\n",
      "2020-02-25 17:23:49,037 Validation result (greedy) at epoch   1, step       32: bleu:   0.00, loss: 102594.7031, ppl: 11495.8457, duration: 5.7574s\n",
      "2020-02-25 17:23:54,939 Validation result (greedy) at epoch   1, step       34: bleu:   0.00, loss: 102592.8438, ppl: 11493.8945, duration: 5.7216s\n",
      "2020-02-25 17:24:00,965 Validation result (greedy) at epoch   1, step       36: bleu:   0.00, loss: 102590.6953, ppl: 11491.6475, duration: 5.7515s\n",
      "2020-02-25 17:24:06,985 Validation result (greedy) at epoch   1, step       38: bleu:   0.00, loss: 102588.1875, ppl: 11489.0176, duration: 5.7773s\n",
      "2020-02-25 17:24:12,996 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 102585.3047, ppl: 11485.9941, duration: 5.7550s\n",
      "2020-02-25 17:24:19,209 Validation result (greedy) at epoch   1, step       42: bleu:   0.00, loss: 102582.0391, ppl: 11482.5762, duration: 5.9996s\n",
      "2020-02-25 17:24:25,689 Validation result (greedy) at epoch   1, step       44: bleu:   0.00, loss: 102578.3047, ppl: 11478.6787, duration: 6.1692s\n",
      "2020-02-25 17:24:31,632 Validation result (greedy) at epoch   1, step       46: bleu:   0.00, loss: 102573.9062, ppl: 11474.0713, duration: 5.6125s\n",
      "2020-02-25 17:24:37,604 Validation result (greedy) at epoch   1, step       48: bleu:   0.00, loss: 102568.8359, ppl: 11468.7764, duration: 5.6062s\n",
      "2020-02-25 17:24:43,655 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 102563.3359, ppl: 11463.0244, duration: 5.8705s\n",
      "2020-02-25 17:24:50,155 Validation result (greedy) at epoch   1, step       52: bleu:   0.00, loss: 102557.4141, ppl: 11456.8389, duration: 6.1912s\n",
      "2020-02-25 17:24:55,943 Validation result (greedy) at epoch   1, step       54: bleu:   0.00, loss: 102551.0547, ppl: 11450.2080, duration: 5.5804s\n",
      "2020-02-25 17:25:01,771 Validation result (greedy) at epoch   1, step       56: bleu:   0.00, loss: 102544.0312, ppl: 11442.8730, duration: 5.5752s\n",
      "2020-02-25 17:25:07,562 Validation result (greedy) at epoch   1, step       58: bleu:   0.00, loss: 102536.4844, ppl: 11435.0068, duration: 5.5717s\n",
      "2020-02-25 17:25:13,439 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 102528.9844, ppl: 11427.1904, duration: 5.7694s\n",
      "2020-02-25 17:25:19,524 Validation result (greedy) at epoch   1, step       62: bleu:   0.00, loss: 102520.9297, ppl: 11418.8135, duration: 5.7615s\n",
      "2020-02-25 17:25:25,890 Validation result (greedy) at epoch   1, step       64: bleu:   0.00, loss: 102512.1797, ppl: 11409.7129, duration: 6.1711s\n",
      "2020-02-25 17:25:31,957 Validation result (greedy) at epoch   1, step       66: bleu:   0.00, loss: 102502.1953, ppl: 11399.3369, duration: 5.7549s\n",
      "2020-02-25 17:25:38,008 Validation result (greedy) at epoch   1, step       68: bleu:   0.00, loss: 102491.1328, ppl: 11387.8418, duration: 5.7490s\n",
      "2020-02-25 17:25:43,938 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 102479.3906, ppl: 11375.6621, duration: 5.7548s\n",
      "2020-02-25 17:25:49,936 Validation result (greedy) at epoch   1, step       72: bleu:   0.00, loss: 102467.6172, ppl: 11363.4648, duration: 5.7486s\n",
      "2020-02-25 17:25:55,879 Validation result (greedy) at epoch   1, step       74: bleu:   0.00, loss: 102455.2734, ppl: 11350.6943, duration: 5.7466s\n",
      "2020-02-25 17:26:01,815 Validation result (greedy) at epoch   1, step       76: bleu:   0.00, loss: 102442.1484, ppl: 11337.1172, duration: 5.7021s\n",
      "2020-02-25 17:26:07,956 Validation result (greedy) at epoch   1, step       78: bleu:   0.00, loss: 102427.7344, ppl: 11322.2393, duration: 5.7946s\n",
      "2020-02-25 17:26:14,145 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 102412.9141, ppl: 11306.9600, duration: 6.0118s\n",
      "2020-02-25 17:26:20,085 Validation result (greedy) at epoch   1, step       82: bleu:   0.00, loss: 102397.2891, ppl: 11290.8721, duration: 5.7341s\n",
      "2020-02-25 17:26:26,236 Validation result (greedy) at epoch   1, step       84: bleu:   0.00, loss: 102379.8984, ppl: 11272.9902, duration: 5.7942s\n",
      "2020-02-25 17:26:32,177 Validation result (greedy) at epoch   1, step       86: bleu:   0.00, loss: 102361.2891, ppl: 11253.8916, duration: 5.7329s\n",
      "2020-02-25 17:26:38,218 Validation result (greedy) at epoch   1, step       88: bleu:   0.00, loss: 102342.9375, ppl: 11235.0830, duration: 5.7637s\n",
      "2020-02-25 17:26:44,248 Validation result (greedy) at epoch   1, step       90: bleu:   0.00, loss: 102323.3828, ppl: 11215.0752, duration: 5.7454s\n",
      "2020-02-25 17:26:50,240 Validation result (greedy) at epoch   1, step       92: bleu:   0.00, loss: 102303.1016, ppl: 11194.3662, duration: 5.7357s\n",
      "2020-02-25 17:26:55,987 Validation result (greedy) at epoch   1, step       94: bleu:   0.00, loss: 102283.0391, ppl: 11173.9189, duration: 5.6074s\n",
      "2020-02-25 17:27:02,087 Validation result (greedy) at epoch   1, step       96: bleu:   0.00, loss: 102261.6719, ppl: 11152.1807, duration: 5.8391s\n",
      "2020-02-25 17:27:07,827 Validation result (greedy) at epoch   1, step       98: bleu:   0.00, loss: 102240.7266, ppl: 11130.9189, duration: 5.5913s\n",
      "2020-02-25 17:27:13,830 Validation result (greedy) at epoch   1, step      100: bleu:   0.00, loss: 102218.9531, ppl: 11108.8506, duration: 5.6767s\n",
      "2020-02-25 17:27:19,596 Validation result (greedy) at epoch   1, step      102: bleu:   0.00, loss: 102196.7500, ppl: 11086.3926, duration: 5.5529s\n",
      "2020-02-25 17:27:25,465 Validation result (greedy) at epoch   1, step      104: bleu:   0.00, loss: 102172.6406, ppl: 11062.0596, duration: 5.5598s\n",
      "2020-02-25 17:27:31,651 Validation result (greedy) at epoch   1, step      106: bleu:   0.00, loss: 102148.1094, ppl: 11037.3584, duration: 5.9710s\n",
      "2020-02-25 17:27:37,854 Validation result (greedy) at epoch   1, step      108: bleu:   0.00, loss: 102124.2031, ppl: 11013.3428, duration: 5.9784s\n",
      "2020-02-25 17:27:43,976 Validation result (greedy) at epoch   1, step      110: bleu:   0.00, loss: 102099.3750, ppl: 10988.4473, duration: 5.7945s\n",
      "2020-02-25 17:27:50,564 Validation result (greedy) at epoch   1, step      112: bleu:   0.00, loss: 102075.0234, ppl: 10964.0889, duration: 6.4294s\n",
      "2020-02-25 17:27:56,363 Validation result (greedy) at epoch   1, step      114: bleu:   0.00, loss: 102049.9219, ppl: 10939.0332, duration: 5.5465s\n",
      "2020-02-25 17:28:03,605 Validation result (greedy) at epoch   1, step      116: bleu:   0.00, loss: 102023.2344, ppl: 10912.4629, duration: 7.0362s\n",
      "2020-02-25 17:28:09,552 Validation result (greedy) at epoch   1, step      118: bleu:   0.00, loss: 101998.7031, ppl: 10888.0967, duration: 5.7664s\n",
      "2020-02-25 17:28:15,710 Validation result (greedy) at epoch   1, step      120: bleu:   0.00, loss: 101971.3984, ppl: 10861.0391, duration: 5.7741s\n",
      "2020-02-25 17:28:21,944 Validation result (greedy) at epoch   1, step      122: bleu:   0.00, loss: 101944.0234, ppl: 10833.9766, duration: 6.0266s\n",
      "2020-02-25 17:28:28,402 Validation result (greedy) at epoch   1, step      124: bleu:   0.00, loss: 101916.6172, ppl: 10806.9502, duration: 6.2547s\n",
      "2020-02-25 17:28:28,504 Epoch   1: total training loss 17158.94\n",
      "2020-02-25 17:28:28,505 Training ended after   1 epochs.\n",
      "2020-02-25 17:28:28,505 Best validation result (greedy) at step        2:   0.00 eval_metric.\n",
      "2020-02-25 17:28:53,747  dev bleu:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-02-25 17:28:53,749 Translations saved to: results/temp/00000002.hyps.dev\n",
      "2020-02-25 17:29:17,713 test bleu:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
      "2020-02-25 17:29:17,714 Translations saved to: results/temp/00000002.hyps.test\n",
      "2020-02-25 17:29:19,761 Hello! This is Joey-NMT.\n",
      "2020-02-25 17:29:19,761 Hello! This is Joey-NMT.\n",
      "2020-02-25 17:29:19,764 Total params: 99889\n",
      "2020-02-25 17:29:19,764 Total params: 99889\n",
      "2020-02-25 17:29:19,766 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-25 17:29:19,766 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-25 17:29:19,769 cfg.name                           : my_experiment\n",
      "2020-02-25 17:29:19,769 cfg.name                           : my_experiment\n",
      "2020-02-25 17:29:19,770 cfg.data.src                       : shp\n",
      "2020-02-25 17:29:19,770 cfg.data.src                       : shp\n",
      "2020-02-25 17:29:19,772 cfg.data.trg                       : es\n",
      "2020-02-25 17:29:19,772 cfg.data.trg                       : es\n",
      "2020-02-25 17:29:19,773 cfg.data.train                     : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-25 17:29:19,773 cfg.data.train                     : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-25 17:29:19,774 cfg.data.dev                       : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-25 17:29:19,774 cfg.data.dev                       : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-25 17:29:19,775 cfg.data.test                      : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-25 17:29:19,775 cfg.data.test                      : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-25 17:29:19,776 cfg.data.level                     : word\n",
      "2020-02-25 17:29:19,776 cfg.data.level                     : word\n",
      "2020-02-25 17:29:19,777 cfg.data.lowercase                 : True\n",
      "2020-02-25 17:29:19,777 cfg.data.lowercase                 : True\n",
      "2020-02-25 17:29:19,779 cfg.data.max_sent_length           : 150\n",
      "2020-02-25 17:29:19,779 cfg.data.max_sent_length           : 150\n",
      "2020-02-25 17:29:19,780 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-25 17:29:19,780 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-25 17:29:19,781 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-25 17:29:19,781 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-25 17:29:19,782 cfg.pretrained_data.src            : shp\n",
      "2020-02-25 17:29:19,782 cfg.pretrained_data.src            : shp\n",
      "2020-02-25 17:29:19,783 cfg.pretrained_data.trg            : es\n",
      "2020-02-25 17:29:19,783 cfg.pretrained_data.trg            : es\n",
      "2020-02-25 17:29:19,784 cfg.pretrained_data.train          : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-25 17:29:19,784 cfg.pretrained_data.train          : data/translate/preprocessed/Religioso/word/train\n",
      "2020-02-25 17:29:19,785 cfg.pretrained_data.dev            : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-25 17:29:19,785 cfg.pretrained_data.dev            : data/translate/preprocessed/Religioso/word/valid\n",
      "2020-02-25 17:29:19,787 cfg.pretrained_data.test           : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-25 17:29:19,787 cfg.pretrained_data.test           : data/translate/preprocessed/Religioso/word/test\n",
      "2020-02-25 17:29:19,788 cfg.pretrained_data.level          : word\n",
      "2020-02-25 17:29:19,788 cfg.pretrained_data.level          : word\n",
      "2020-02-25 17:29:19,789 cfg.pretrained_data.lowercase      : True\n",
      "2020-02-25 17:29:19,789 cfg.pretrained_data.lowercase      : True\n",
      "2020-02-25 17:29:19,790 cfg.pretrained_data.max_sent_length : 150\n",
      "2020-02-25 17:29:19,790 cfg.pretrained_data.max_sent_length : 150\n",
      "2020-02-25 17:29:19,791 cfg.pretrained_data.src_voc_min_freq : 1\n",
      "2020-02-25 17:29:19,791 cfg.pretrained_data.src_voc_min_freq : 1\n",
      "2020-02-25 17:29:19,792 cfg.pretrained_data.trg_voc_min_freq : 1\n",
      "2020-02-25 17:29:19,792 cfg.pretrained_data.trg_voc_min_freq : 1\n",
      "2020-02-25 17:29:19,793 cfg.testing.beam_size              : 5\n",
      "2020-02-25 17:29:19,793 cfg.testing.beam_size              : 5\n",
      "2020-02-25 17:29:19,794 cfg.testing.alpha                  : 1.0\n",
      "2020-02-25 17:29:19,794 cfg.testing.alpha                  : 1.0\n",
      "2020-02-25 17:29:19,795 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-25 17:29:19,795 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-25 17:29:19,796 cfg.training.reset_scheduler       : False\n",
      "2020-02-25 17:29:19,796 cfg.training.reset_scheduler       : False\n",
      "2020-02-25 17:29:19,797 cfg.training.reset_optimizer       : False\n",
      "2020-02-25 17:29:19,797 cfg.training.reset_optimizer       : False\n",
      "2020-02-25 17:29:19,799 cfg.training.random_seed           : 42\n",
      "2020-02-25 17:29:19,799 cfg.training.random_seed           : 42\n",
      "2020-02-25 17:29:19,800 cfg.training.optimizer             : adam\n",
      "2020-02-25 17:29:19,800 cfg.training.optimizer             : adam\n",
      "2020-02-25 17:29:19,801 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-25 17:29:19,801 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-25 17:29:19,802 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-25 17:29:19,802 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-25 17:29:19,803 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-25 17:29:19,803 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-25 17:29:19,804 cfg.training.weight_decay          : 0.0\n",
      "2020-02-25 17:29:19,804 cfg.training.weight_decay          : 0.0\n",
      "2020-02-25 17:29:19,805 cfg.training.batch_size            : 48\n",
      "2020-02-25 17:29:19,805 cfg.training.batch_size            : 48\n",
      "2020-02-25 17:29:19,806 cfg.training.batch_type            : sentence\n",
      "2020-02-25 17:29:19,806 cfg.training.batch_type            : sentence\n",
      "2020-02-25 17:29:19,807 cfg.training.eval_batch_size       : 10\n",
      "2020-02-25 17:29:19,807 cfg.training.eval_batch_size       : 10\n",
      "2020-02-25 17:29:19,808 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-25 17:29:19,808 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-25 17:29:19,809 cfg.training.batch_multiplier      : 1\n",
      "2020-02-25 17:29:19,809 cfg.training.batch_multiplier      : 1\n",
      "2020-02-25 17:29:19,810 cfg.training.scheduling            : plateau\n",
      "2020-02-25 17:29:19,810 cfg.training.scheduling            : plateau\n",
      "2020-02-25 17:29:19,811 cfg.training.patience              : 600\n",
      "2020-02-25 17:29:19,811 cfg.training.patience              : 600\n",
      "2020-02-25 17:29:19,812 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-25 17:29:19,812 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-25 17:29:19,813 cfg.training.epochs                : 1\n",
      "2020-02-25 17:29:19,813 cfg.training.epochs                : 1\n",
      "2020-02-25 17:29:19,814 cfg.training.validation_freq       : 2\n",
      "2020-02-25 17:29:19,814 cfg.training.validation_freq       : 2\n",
      "2020-02-25 17:29:19,815 cfg.training.logging_freq          : 1000\n",
      "2020-02-25 17:29:19,815 cfg.training.logging_freq          : 1000\n",
      "2020-02-25 17:29:19,816 cfg.training.eval_metric           : bleu\n",
      "2020-02-25 17:29:19,816 cfg.training.eval_metric           : bleu\n",
      "2020-02-25 17:29:19,817 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-25 17:29:19,817 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-25 17:29:19,818 cfg.training.model_dir             : results/temp\n",
      "2020-02-25 17:29:19,818 cfg.training.model_dir             : results/temp\n",
      "2020-02-25 17:29:19,819 cfg.training.overwrite             : True\n",
      "2020-02-25 17:29:19,819 cfg.training.overwrite             : True\n",
      "2020-02-25 17:29:19,820 cfg.training.shuffle               : True\n",
      "2020-02-25 17:29:19,820 cfg.training.shuffle               : True\n",
      "2020-02-25 17:29:19,821 cfg.training.use_cuda              : False\n",
      "2020-02-25 17:29:19,821 cfg.training.use_cuda              : False\n",
      "2020-02-25 17:29:19,822 cfg.training.max_output_length     : 60\n",
      "2020-02-25 17:29:19,822 cfg.training.max_output_length     : 60\n",
      "2020-02-25 17:29:19,823 cfg.training.print_valid_sents     : []\n",
      "2020-02-25 17:29:19,823 cfg.training.print_valid_sents     : []\n",
      "2020-02-25 17:29:19,824 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-25 17:29:19,824 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-25 17:29:19,825 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-25 17:29:19,825 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-25 17:29:19,826 cfg.pretraining.reset_best_ckpt    : False\n",
      "2020-02-25 17:29:19,826 cfg.pretraining.reset_best_ckpt    : False\n",
      "2020-02-25 17:29:19,826 cfg.pretraining.reset_scheduler    : False\n",
      "2020-02-25 17:29:19,826 cfg.pretraining.reset_scheduler    : False\n",
      "2020-02-25 17:29:19,827 cfg.pretraining.reset_optimizer    : False\n",
      "2020-02-25 17:29:19,827 cfg.pretraining.reset_optimizer    : False\n",
      "2020-02-25 17:29:19,828 cfg.pretraining.random_seed        : 42\n",
      "2020-02-25 17:29:19,828 cfg.pretraining.random_seed        : 42\n",
      "2020-02-25 17:29:19,829 cfg.pretraining.optimizer          : adam\n",
      "2020-02-25 17:29:19,829 cfg.pretraining.optimizer          : adam\n",
      "2020-02-25 17:29:19,830 cfg.pretraining.learning_rate      : 0.0002\n",
      "2020-02-25 17:29:19,830 cfg.pretraining.learning_rate      : 0.0002\n",
      "2020-02-25 17:29:19,831 cfg.pretraining.learning_rate_min  : 1e-05\n",
      "2020-02-25 17:29:19,831 cfg.pretraining.learning_rate_min  : 1e-05\n",
      "2020-02-25 17:29:19,842 cfg.pretraining.clip_grad_val      : 1.0\n",
      "2020-02-25 17:29:19,842 cfg.pretraining.clip_grad_val      : 1.0\n",
      "2020-02-25 17:29:19,847 cfg.pretraining.weight_decay       : 0.0\n",
      "2020-02-25 17:29:19,847 cfg.pretraining.weight_decay       : 0.0\n",
      "2020-02-25 17:29:19,848 cfg.pretraining.batch_size         : 48\n",
      "2020-02-25 17:29:19,848 cfg.pretraining.batch_size         : 48\n",
      "2020-02-25 17:29:19,849 cfg.pretraining.batch_type         : sentence\n",
      "2020-02-25 17:29:19,849 cfg.pretraining.batch_type         : sentence\n",
      "2020-02-25 17:29:19,851 cfg.pretraining.eval_batch_size    : 10\n",
      "2020-02-25 17:29:19,851 cfg.pretraining.eval_batch_size    : 10\n",
      "2020-02-25 17:29:19,851 cfg.pretraining.eval_batch_type    : sentence\n",
      "2020-02-25 17:29:19,851 cfg.pretraining.eval_batch_type    : sentence\n",
      "2020-02-25 17:29:19,852 cfg.pretraining.batch_multiplier   : 1\n",
      "2020-02-25 17:29:19,852 cfg.pretraining.batch_multiplier   : 1\n",
      "2020-02-25 17:29:19,853 cfg.pretraining.scheduling         : plateau\n",
      "2020-02-25 17:29:19,853 cfg.pretraining.scheduling         : plateau\n",
      "2020-02-25 17:29:19,854 cfg.pretraining.patience           : 600\n",
      "2020-02-25 17:29:19,854 cfg.pretraining.patience           : 600\n",
      "2020-02-25 17:29:19,855 cfg.pretraining.decrease_factor    : 0.5\n",
      "2020-02-25 17:29:19,855 cfg.pretraining.decrease_factor    : 0.5\n",
      "2020-02-25 17:29:19,856 cfg.pretraining.epochs             : 1\n",
      "2020-02-25 17:29:19,856 cfg.pretraining.epochs             : 1\n",
      "2020-02-25 17:29:19,856 cfg.pretraining.validation_freq    : 2\n",
      "2020-02-25 17:29:19,856 cfg.pretraining.validation_freq    : 2\n",
      "2020-02-25 17:29:19,857 cfg.pretraining.logging_freq       : 1000\n",
      "2020-02-25 17:29:19,857 cfg.pretraining.logging_freq       : 1000\n",
      "2020-02-25 17:29:19,858 cfg.pretraining.eval_metric        : bleu\n",
      "2020-02-25 17:29:19,858 cfg.pretraining.eval_metric        : bleu\n",
      "2020-02-25 17:29:19,859 cfg.pretraining.early_stopping_metric : eval_metric\n",
      "2020-02-25 17:29:19,859 cfg.pretraining.early_stopping_metric : eval_metric\n",
      "2020-02-25 17:29:19,860 cfg.pretraining.model_dir          : results/temp\n",
      "2020-02-25 17:29:19,860 cfg.pretraining.model_dir          : results/temp\n",
      "2020-02-25 17:29:19,861 cfg.pretraining.overwrite          : True\n",
      "2020-02-25 17:29:19,861 cfg.pretraining.overwrite          : True\n",
      "2020-02-25 17:29:19,861 cfg.pretraining.shuffle            : True\n",
      "2020-02-25 17:29:19,861 cfg.pretraining.shuffle            : True\n",
      "2020-02-25 17:29:19,862 cfg.pretraining.use_cuda           : False\n",
      "2020-02-25 17:29:19,862 cfg.pretraining.use_cuda           : False\n",
      "2020-02-25 17:29:19,863 cfg.pretraining.max_output_length  : 60\n",
      "2020-02-25 17:29:19,863 cfg.pretraining.max_output_length  : 60\n",
      "2020-02-25 17:29:19,864 cfg.pretraining.print_valid_sents  : []\n",
      "2020-02-25 17:29:19,864 cfg.pretraining.print_valid_sents  : []\n",
      "2020-02-25 17:29:19,865 cfg.pretraining.keep_last_ckpts    : 3\n",
      "2020-02-25 17:29:19,865 cfg.pretraining.keep_last_ckpts    : 3\n",
      "2020-02-25 17:29:19,869 cfg.pretraining.label_smoothing    : 0.0\n",
      "2020-02-25 17:29:19,869 cfg.pretraining.label_smoothing    : 0.0\n",
      "2020-02-25 17:29:19,870 cfg.model.initializer              : xavier\n",
      "2020-02-25 17:29:19,870 cfg.model.initializer              : xavier\n",
      "2020-02-25 17:29:19,871 cfg.model.init_weight              : 0.01\n",
      "2020-02-25 17:29:19,871 cfg.model.init_weight              : 0.01\n",
      "2020-02-25 17:29:19,872 cfg.model.init_gain                : 1.0\n",
      "2020-02-25 17:29:19,872 cfg.model.init_gain                : 1.0\n",
      "2020-02-25 17:29:19,872 cfg.model.bias_initializer         : zeros\n",
      "2020-02-25 17:29:19,872 cfg.model.bias_initializer         : zeros\n",
      "2020-02-25 17:29:19,873 cfg.model.embed_initializer        : normal\n",
      "2020-02-25 17:29:19,873 cfg.model.embed_initializer        : normal\n",
      "2020-02-25 17:29:19,874 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-25 17:29:19,874 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-25 17:29:19,875 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-25 17:29:19,875 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-25 17:29:19,876 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-25 17:29:19,876 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-25 17:29:19,878 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-25 17:29:19,878 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-25 17:29:19,879 cfg.model.tied_embeddings          : False\n",
      "2020-02-25 17:29:19,879 cfg.model.tied_embeddings          : False\n",
      "2020-02-25 17:29:19,880 cfg.model.tied_softmax             : False\n",
      "2020-02-25 17:29:19,880 cfg.model.tied_softmax             : False\n",
      "2020-02-25 17:29:19,881 cfg.model.encoder.type             : recurrent\n",
      "2020-02-25 17:29:19,881 cfg.model.encoder.type             : recurrent\n",
      "2020-02-25 17:29:19,882 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-25 17:29:19,882 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-25 17:29:19,883 cfg.model.encoder.embeddings.embedding_dim : 2\n",
      "2020-02-25 17:29:19,883 cfg.model.encoder.embeddings.embedding_dim : 2\n",
      "2020-02-25 17:29:19,884 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-25 17:29:19,884 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-25 17:29:19,885 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-25 17:29:19,885 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-25 17:29:19,885 cfg.model.encoder.hidden_size      : 5\n",
      "2020-02-25 17:29:19,885 cfg.model.encoder.hidden_size      : 5\n",
      "2020-02-25 17:29:19,887 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-25 17:29:19,887 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-25 17:29:19,888 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-25 17:29:19,888 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-25 17:29:19,888 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-25 17:29:19,888 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-25 17:29:19,890 cfg.model.encoder.freeze           : False\n",
      "2020-02-25 17:29:19,890 cfg.model.encoder.freeze           : False\n",
      "2020-02-25 17:29:19,890 cfg.model.decoder.type             : recurrent\n",
      "2020-02-25 17:29:19,890 cfg.model.decoder.type             : recurrent\n",
      "2020-02-25 17:29:19,892 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-25 17:29:19,892 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-25 17:29:19,892 cfg.model.decoder.embeddings.embedding_dim : 2\n",
      "2020-02-25 17:29:19,892 cfg.model.decoder.embeddings.embedding_dim : 2\n",
      "2020-02-25 17:29:19,893 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-25 17:29:19,893 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-25 17:29:19,894 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-25 17:29:19,894 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-25 17:29:19,895 cfg.model.decoder.hidden_size      : 5\n",
      "2020-02-25 17:29:19,895 cfg.model.decoder.hidden_size      : 5\n",
      "2020-02-25 17:29:19,896 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-25 17:29:19,896 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-25 17:29:19,897 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-25 17:29:19,897 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-25 17:29:19,898 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-25 17:29:19,898 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-25 17:29:19,899 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-25 17:29:19,899 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-25 17:29:19,900 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-25 17:29:19,900 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-25 17:29:19,901 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-25 17:29:19,901 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-25 17:29:19,902 cfg.model.decoder.freeze           : False\n",
      "2020-02-25 17:29:19,902 cfg.model.decoder.freeze           : False\n",
      "2020-02-25 17:29:19,903 Data set sizes: \n",
      "\ttrain 5996,\n",
      "\tvalid 749,\n",
      "\ttest 749\n",
      "2020-02-25 17:29:19,903 Data set sizes: \n",
      "\ttrain 5996,\n",
      "\tvalid 749,\n",
      "\ttest 749\n",
      "2020-02-25 17:29:19,903 First training example:\n",
      "\t[SRC] pero la palabra del seor permanece eternamente esta palabra es el evangelio que se les ha anunciado a ustedes\n",
      "\t[TRG] ikaxbi ja non ibon joira jawetianbi keyyamai iki\n",
      "2020-02-25 17:29:19,903 First training example:\n",
      "\t[SRC] pero la palabra del seor permanece eternamente esta palabra es el evangelio que se les ha anunciado a ustedes\n",
      "\t[TRG] ikaxbi ja non ibon joira jawetianbi keyyamai iki\n",
      "2020-02-25 17:29:19,904 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) que (5) de (6) y (7) a (8) la (9) el\n",
      "2020-02-25 17:29:19,904 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) que (5) de (6) y (7) a (8) la (9) el\n",
      "2020-02-25 17:29:19,905 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) iki (5) ja (6) jato (7) mato (8) ea (9) en\n",
      "2020-02-25 17:29:19,905 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) iki (5) ja (6) jato (7) mato (8) ea (9) en\n",
      "2020-02-25 17:29:19,906 Number of Src words (types): 9005\n",
      "2020-02-25 17:29:19,906 Number of Src words (types): 9005\n",
      "2020-02-25 17:29:19,906 Number of Trg words (types): 11507\n",
      "2020-02-25 17:29:19,906 Number of Trg words (types): 11507\n",
      "2020-02-25 17:29:19,907 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(2, 5, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(7, 5, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=2, vocab_size=9005),\n",
      "\ttrg_embed=Embeddings(embedding_dim=2, vocab_size=11507))\n",
      "2020-02-25 17:29:19,907 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(2, 5, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(7, 5, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=2, vocab_size=9005),\n",
      "\ttrg_embed=Embeddings(embedding_dim=2, vocab_size=11507))\n",
      "2020-02-25 17:29:19,919 EPOCH 1\n",
      "2020-02-25 17:29:19,919 EPOCH 1\n",
      "2020-02-25 17:29:25,908 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-25 17:29:25,908 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-25 17:29:25,909 Saving new checkpoint.\n",
      "2020-02-25 17:29:25,909 Saving new checkpoint.\n",
      "2020-02-25 17:29:25,914 Validation result (greedy) at epoch   1, step        2: bleu:   0.00, loss: 102605.6719, ppl: 11507.3408, duration: 5.5964s\n",
      "2020-02-25 17:29:25,914 Validation result (greedy) at epoch   1, step        2: bleu:   0.00, loss: 102605.6719, ppl: 11507.3408, duration: 5.5964s\n",
      "2020-02-25 17:29:33,047 Validation result (greedy) at epoch   1, step        4: bleu:   0.00, loss: 102605.2969, ppl: 11506.9453, duration: 6.8524s\n",
      "2020-02-25 17:29:33,047 Validation result (greedy) at epoch   1, step        4: bleu:   0.00, loss: 102605.2969, ppl: 11506.9453, duration: 6.8524s\n",
      "2020-02-25 17:29:39,030 Validation result (greedy) at epoch   1, step        6: bleu:   0.00, loss: 102604.6172, ppl: 11506.2324, duration: 5.7784s\n",
      "2020-02-25 17:29:39,030 Validation result (greedy) at epoch   1, step        6: bleu:   0.00, loss: 102604.6172, ppl: 11506.2324, duration: 5.7784s\n",
      "2020-02-25 17:29:45,035 Validation result (greedy) at epoch   1, step        8: bleu:   0.00, loss: 102603.3516, ppl: 11504.9043, duration: 5.8003s\n",
      "2020-02-25 17:29:45,035 Validation result (greedy) at epoch   1, step        8: bleu:   0.00, loss: 102603.3516, ppl: 11504.9043, duration: 5.8003s\n",
      "2020-02-25 17:29:51,022 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 102601.5234, ppl: 11502.9844, duration: 5.8143s\n",
      "2020-02-25 17:29:51,022 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 102601.5234, ppl: 11502.9844, duration: 5.8143s\n",
      "2020-02-25 17:29:57,170 Validation result (greedy) at epoch   1, step       12: bleu:   0.00, loss: 102599.0781, ppl: 11500.4287, duration: 5.8052s\n",
      "2020-02-25 17:29:57,170 Validation result (greedy) at epoch   1, step       12: bleu:   0.00, loss: 102599.0781, ppl: 11500.4287, duration: 5.8052s\n",
      "2020-02-25 17:30:03,262 Validation result (greedy) at epoch   1, step       14: bleu:   0.00, loss: 102595.6406, ppl: 11496.8213, duration: 5.8424s\n",
      "2020-02-25 17:30:03,262 Validation result (greedy) at epoch   1, step       14: bleu:   0.00, loss: 102595.6406, ppl: 11496.8213, duration: 5.8424s\n",
      "2020-02-25 17:30:09,248 Validation result (greedy) at epoch   1, step       16: bleu:   0.00, loss: 102591.4688, ppl: 11492.4473, duration: 5.8540s\n",
      "2020-02-25 17:30:09,248 Validation result (greedy) at epoch   1, step       16: bleu:   0.00, loss: 102591.4688, ppl: 11492.4473, duration: 5.8540s\n",
      "2020-02-25 17:30:15,101 Validation result (greedy) at epoch   1, step       18: bleu:   0.00, loss: 102585.9922, ppl: 11486.7168, duration: 5.6057s\n",
      "2020-02-25 17:30:15,101 Validation result (greedy) at epoch   1, step       18: bleu:   0.00, loss: 102585.9922, ppl: 11486.7168, duration: 5.6057s\n"
     ]
    }
   ],
   "source": [
    "train(\"joeynmt/configs/sample_{name}.yaml\".format(name=\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
