{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/krivas/projects/transfer_learning_nmt/nb\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../ -name _about* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘../nb/.ipynb_checkpoints’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!find ../ -name *ipynb_checkpoints* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyphen\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "#os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_f(path, arr):\n",
    "    cont = 0\n",
    "    with open(path, 'w') as f:\n",
    "        for l in arr:\n",
    "            if len(l):\n",
    "                print(l, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multi_txt(path_1, path_2, use_max_sent=True, max_sent=5000):\n",
    "    lines = []\n",
    "    cont = 0\n",
    "    with open(path_1, 'r') as f1, open(path_2, 'r') as f2:\n",
    "        for line_1, line_2 in zip(f1.read().split('\\n'), f2.read().split('\\n')):\n",
    "            if len(line_1) and len(line_2) and cont < max_sent:\n",
    "                lines.append(line_1 + '\\t' + line_2)\n",
    "                if use_max_sent:\n",
    "                    cont += 1    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word_transfer(word_dir, train_in, dev_in, test_in, train_out, dev_out, test_out, use_max_sent):\n",
    "    os.makedirs(word_dir, exist_ok=True)\n",
    "    \n",
    "    train = read_multi_txt(train_in, train_out, use_max_sent, 5000)\n",
    "    valid = read_multi_txt(dev_in, dev_out, use_max_sent, 500)\n",
    "    test = read_multi_txt(test_in, test_out, use_max_sent, 500)\n",
    "    \n",
    "    save_f(word_dir / 'train.tsv', train)\n",
    "    save_f(word_dir / f'train.{lang_in}', [line.split('\\t')[0].strip() for line in train if len(line)])\n",
    "    save_f(word_dir / f'train.{lang_out}', [line.split('\\t')[1].strip() for line in train if len(line)])\n",
    "    \n",
    "    save_f(word_dir / 'test.tsv', test)\n",
    "    save_f(word_dir / f'test.{lang_in}', [line.split('\\t')[0].strip() for line in test if len(line)])\n",
    "    save_f(word_dir / f'test.{lang_out}', [line.split('\\t')[1].strip() for line in test if len(line)])\n",
    "\n",
    "    save_f(word_dir / 'valid.tsv', valid)\n",
    "    save_f(word_dir / f'valid.{lang_in}', [line.split('\\t')[0].strip() for line in valid if len(line)])\n",
    "    save_f(word_dir / f'valid.{lang_out}', [line.split('\\t')[1].strip() for line in valid if len(line)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word_translation(file_path, word_dir, lang_in, lang_out):\n",
    "    os.makedirs(word_dir, exist_ok=True)\n",
    "    p_lines = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f.read().split('\\n'):\n",
    "            line = re.sub(r'([.¡!¿?;,:])', r' \\1 ', line)\n",
    "            line = ' '.join([w for w in line.split(' ')])\n",
    "            line = line.replace('  ', ' ')\n",
    "            line = line.strip(' ')\n",
    "            p_lines.append(line)\n",
    "    \n",
    "    train, temp = train_test_split(p_lines, test_size=0.1, random_state=0)\n",
    "    valid, test = train_test_split(temp, test_size=0.5, random_state=0)\n",
    "    #print([line.split('\\t') for line in train][0])\n",
    "    #print([line.split('\\t')[0] for line in train][:10])\n",
    "    save_f(word_dir / 'train.tsv', train)\n",
    "    save_f(word_dir / f'train.{lang_in}', [line.split('\\t')[1].strip() for line in train if len(line)])\n",
    "    save_f(word_dir / f'train.{lang_out}', [line.split('\\t')[0].strip() for line in train if len(line)])\n",
    "    \n",
    "    save_f(word_dir / 'test.tsv', test)\n",
    "    save_f(word_dir / f'test.{lang_in}', [line.split('\\t')[1].strip() for line in test if len(line)])\n",
    "    save_f(word_dir / f'test.{lang_out}', [line.split('\\t')[0].strip() for line in test if len(line)])\n",
    "\n",
    "    save_f(word_dir / 'valid.tsv', valid)\n",
    "    save_f(word_dir / f'valid.{lang_in}', [line.split('\\t')[1].strip() for line in valid if len(line)])\n",
    "    save_f(word_dir / f'valid.{lang_out}', [line.split('\\t')[0].strip() for line in valid if len(line)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_char_segments(word_dir, char_dir, lang_in, lang_out):\n",
    "    os.makedirs(char_dir, exist_ok=True)\n",
    "    for lang in [lang_out, lang_in]:\n",
    "        spm.SentencePieceTrainer.Train(f'--input={word_dir}/all.{lang} --model_prefix=m --vocab_size=1000 --character_coverage=1.0 --model_type=char')   \n",
    "\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.Load(\"m.model\")\n",
    "        for file in [f'train.{lang}', f'valid.{lang}', f'test.{lang}']:\n",
    "            f_in = open(word_dir / file, 'r')\n",
    "            f_out = open(char_dir / file, 'w')\n",
    "            \n",
    "            for line in f_in.read().split('\\n'):\n",
    "                temp = []\n",
    "                for word in sp.EncodeAsPieces(line.replace('<unk>', '<unknown>')):\n",
    "                    if str('\\u2581') in word:\n",
    "                        word = word.replace(str('\\u2581'), '@@')\n",
    "                    temp.append(word)\n",
    "                f_out.write(\" \".join(temp) + \"\\n\")\n",
    "\n",
    "            f_in.close()\n",
    "            f_out.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bpe_segments(word_dir, prepro_dir, lang_in, lang_out, n_opers, dropout=False):\n",
    "    \n",
    "    os.system(f'cat {word_dir}/train.{lang_in} {word_dir}/valid.{lang_in} > {word_dir}/all.{lang_in}')  \n",
    "    os.system(f'cat {word_dir}/train.{lang_out} {word_dir}/valid.{lang_out} > {word_dir}/all.{lang_out}')\n",
    "    \n",
    "    for oper in n_opers:\n",
    "        bpe_dir = prepro_dir / (f'bpe_drop_{oper}' if dropout else f'bpe_{oper}')\n",
    "        os.makedirs(bpe_dir, exist_ok=True)\n",
    "        p_lines = []\n",
    "          \n",
    "        os.system(f'cat {word_dir}/all.{lang_out} {word_dir}/all.{lang_in} | subword-nmt learn-bpe -s {oper} -o {bpe_dir}/codes.all')\n",
    "\n",
    "        for lang in [lang_out, lang_in]:\n",
    "            os.system(f'subword-nmt apply-bpe --dropout {0.1 if dropout else 0} -c {bpe_dir}/codes.all < {word_dir}/all.{lang} | subword-nmt get-vocab > {bpe_dir}/vocab.{lang}')\n",
    "            \n",
    "            os.system(f'subword-nmt apply-bpe --dropout {0.1 if dropout else 0} -c {bpe_dir}/codes.all < {word_dir}/test.{lang} > {bpe_dir}/test.bpe.{lang}')\n",
    "            os.system(f'subword-nmt apply-bpe --dropout {0.1 if dropout else 0} -c {bpe_dir}/codes.all < {word_dir}/train.{lang} > {bpe_dir}/train.bpe.{lang}')\n",
    "            os.system(f'subword-nmt apply-bpe --dropout {0.1 if dropout else 0} -c {bpe_dir}/codes.all < {word_dir}/valid.{lang} > {bpe_dir}/valid.bpe.{lang}')\n",
    "            \n",
    "        print('join corpus')\n",
    "        for corpus in ['valid', 'test', 'train']:\n",
    "            l1 = open(f'{bpe_dir}/{corpus}.bpe.{lang_out}', 'r').read().split('\\n')\n",
    "            l2 = open(f'{bpe_dir}/{corpus}.bpe.{lang_in}', 'r').read().split('\\n')\n",
    "            pd.DataFrame(list(zip(l2, l1))).to_csv(f'{bpe_dir}/{corpus}.tsv', header=None, index=False, sep='\\t')\n",
    "            save_f(bpe_dir / f'{corpus}.{lang_out}', l1)\n",
    "            save_f(bpe_dir / f'{corpus}.{lang_in}', l2)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_segmentation(prepro_dir, lang_in, lang_out, n_opers=[5000]):\n",
    "    save_bpe_segments(prepro_dir / 'word', prepro_dir, lang_in, lang_out, n_opers=n_opers)    \n",
    "    save_bpe_segments(prepro_dir / 'word', prepro_dir, lang_in, lang_out, n_opers=n_opers, dropout=True)\n",
    "    save_char_segments(prepro_dir / 'word', prepro_dir / 'char', lang_in, lang_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.read().split('\\n'):\n",
    "            if len(line):\n",
    "                lines.append(line)\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/transfer/raw/splits.es/train/es-de.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-tr.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-uk.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-nl.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-pl.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-it.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-ru.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-fr.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-en.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-pt.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-shp.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-da.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-hu.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-cmn.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.es/train/es-fi.train.es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-fr.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-bn.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-hi.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-tl.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-vi.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-cs.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-pl.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-es.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-cmn.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-uk.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-sr.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-he.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-it.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-bg.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-de.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-el.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-lt.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-mk.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-hu.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-fi.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-be.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-tr.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-shp.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-nb.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-ar.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-nds.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-sv.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-mr.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-da.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-ro.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-ru.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-yue.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-nl.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-tk.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-kab.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-id.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-is.train.en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/transfer/raw/splits.en/train/en-pt.train.en\n",
      "join corpus\n",
      "join corpus\n"
     ]
    }
   ],
   "source": [
    "raw_dir = base_dir / 'transfer' / 'raw'\n",
    "prepro_dir = base_dir / 'transfer' / 'preprocessed'\n",
    "for lang in ['es', 'en']:\n",
    "    lang_dir = raw_dir / f'splits.{lang}'\n",
    "    pair_langs = read_txt(lang_dir / f'all.train.{lang}-ll.lang-pairs')\n",
    "    pair_langs = list(set(pair_langs))\n",
    "    for pair_lang in pair_langs:\n",
    "        lang_out, lang_in = pair_lang.split(' ')\n",
    "        train_in = lang_dir / 'train' / f'{lang_in}-{lang_out}.train.{lang_in}'\n",
    "        dev_in = lang_dir / 'dev' / f'{lang_in}-{lang_out}.dev.{lang_in}'\n",
    "        test_in = lang_dir / 'test' / f'{lang_in}-{lang_out}.test.{lang_in}'\n",
    "        \n",
    "        train_out = lang_dir / 'train' / f'{lang_in}-{lang_out}.train.{lang_out}'\n",
    "        dev_out = lang_dir / 'dev' / f'{lang_in}-{lang_out}.dev.{lang_out}'\n",
    "        test_out = lang_dir / 'test' / f'{lang_in}-{lang_out}.test.{lang_out}'\n",
    "        \n",
    "        print(train_in)\n",
    "        if 'shp' in pair_lang:\n",
    "            use_max_sent = False\n",
    "        else:\n",
    "            use_max_sent = True\n",
    "            \n",
    "        segment_dir = prepro_dir / f'splits.{lang}' / f'{lang_in}-{lang_out}'\n",
    "        save_word_transfer(segment_dir / 'word',\\\n",
    "                           train_in, dev_in, test_in,\\\n",
    "                           train_out, dev_out, test_out,\\\n",
    "                           use_max_sent)\n",
    "        save_segmentation(segment_dir, lang_in, lang_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_dir = base_dir / 'translate' / 'raw'\n",
    "prepro_dir = base_dir / 'translate' / 'preprocessed'\n",
    "for dir_temp in os.listdir(raw_dir):\n",
    "    lang_dir = raw_dir / dir_temp\n",
    "    if os.path.isdir(lang_dir):\n",
    "        print(lang_dir)\n",
    "        #file_path = lang_dir / 'all.txt'#lang_dir / (os.listdir(lang_dir)[0] if 'txt' in os.listdir(lang_dir)[0] else os.listdir(lang_dir)[1])\n",
    "        lang_in = 'es'\n",
    "        lang_out = 'shp'\n",
    "        save_word_translation(file_path, prepro_dir / 'word', lang_in, lang_out)\n",
    "        save_segmentation(prepro_dir / dir_temp, lang_in, lang_out, list(range(1000, 11000, 1000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flashcards'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1000, 11000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flashcards'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shp-es'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/raw/shp-es/all.shi')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
