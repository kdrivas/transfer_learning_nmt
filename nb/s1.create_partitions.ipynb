{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/krivas/projects/transfer_learning_nmt/nb\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ../ -name _about* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘../data/transfer/.ipynb_checkpoints’: No such file or directory\r\n",
      "find: ‘../data/raw/.ipynb_checkpoints’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!find ../ -name *ipynb_checkpoints* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyphen\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "#os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_f(path, arr):\n",
    "    with open(path, 'w') as f:\n",
    "        for l in arr:\n",
    "            if len(l):\n",
    "                print(l, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word_segments(file_path, word_dir, lang_in, lang_out):\n",
    "    os.makedirs(word_dir, exist_ok=True)\n",
    "    p_lines = []\n",
    "    with open(file_path) as f:\n",
    "        for line in f.read().split('\\n'):\n",
    "            line = re.sub(r'([.¡!¿?;,:])', r' \\1 ', line)\n",
    "            line = ' '.join([w for w in line.split(' ')])\n",
    "            line = line.replace('  ', ' ')\n",
    "            p_lines.append(line)\n",
    "    \n",
    "    train, temp = train_test_split(p_lines, test_size=0.2, random_state=0)\n",
    "    valid, test = train_test_split(temp, test_size=0.5, random_state=0)\n",
    "    #print([line.split('\\t') for line in train][0])\n",
    "    #print([line.split('\\t')[0] for line in train][:10])\n",
    "    save_f(word_dir / 'train.tsv', train)\n",
    "    save_f(word_dir / f'train.{lang_in}', [line.split('\\t')[1].strip() for line in train if len(line)])\n",
    "    save_f(word_dir / f'train.{lang_out}', [line.split('\\t')[0].strip() for line in train if len(line)])\n",
    "    \n",
    "    save_f(word_dir / 'test.tsv', test)\n",
    "    save_f(word_dir / f'test.{lang_in}', [line.split('\\t')[1].strip() for line in test if len(line)])\n",
    "    save_f(word_dir / f'test.{lang_out}', [line.split('\\t')[0].strip() for line in test if len(line)])\n",
    "\n",
    "    save_f(word_dir / 'valid.tsv', valid)\n",
    "    save_f(word_dir / f'valid.{lang_in}', [line.split('\\t')[1].strip() for line in valid if len(line)])\n",
    "    save_f(word_dir / f'valid.{lang_out}', [line.split('\\t')[0].strip() for line in valid if len(line)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bpe_segments(word_dir, prepro_dir, lang_in, lang_out, n_opers, dropout=False):\n",
    "    \n",
    "    os.system(f'cat {word_dir}/train.{lang_in} {word_dir}/valid.{lang_in} > {word_dir}/all.{lang_in}')  \n",
    "    os.system(f'cat {word_dir}/train.{lang_out} {word_dir}/valid.{lang_out} > {word_dir}/all.{lang_out}')\n",
    "    \n",
    "    for oper in n_opers:\n",
    "        bpe_dir = prepro_dir / (f'bpe_drop_{oper}' if dropout else f'bpe_{oper}')\n",
    "        os.makedirs(bpe_dir, exist_ok=True)\n",
    "        p_lines = []\n",
    "          \n",
    "        os.system(f'cat {word_dir}/all.{lang_out} {word_dir}/all.{lang_in} | subword-nmt learn-bpe -s {oper} -o {bpe_dir}/codes.all')\n",
    "\n",
    "        for lang in [lang_out, lang_in]:\n",
    "            os.system(f'subword-nmt apply-bpe --dropout {0.1 if dropout else 0} -c {bpe_dir}/codes.all < {word_dir}/all.{lang} | subword-nmt get-vocab > {bpe_dir}/vocab.{lang}')\n",
    "            \n",
    "            os.system(f'subword-nmt apply-bpe --dropout {0.1 if dropout else 0} -c {bpe_dir}/codes.all < {word_dir}/test.{lang} > {bpe_dir}/test.bpe.{lang}')\n",
    "            os.system(f'subword-nmt apply-bpe --dropout {0.1 if dropout else 0} -c {bpe_dir}/codes.all < {word_dir}/train.{lang} > {bpe_dir}/train.bpe.{lang}')\n",
    "            os.system(f'subword-nmt apply-bpe --dropout {0.1 if dropout else 0} -c {bpe_dir}/codes.all < {word_dir}/valid.{lang} > {bpe_dir}/valid.bpe.{lang}')\n",
    "            \n",
    "        print('join corpus')\n",
    "        for corpus in ['valid', 'test', 'train']:\n",
    "            l1 = open(f'{bpe_dir}/{corpus}.bpe.{lang_out}', 'r').read().split('\\n')\n",
    "            l2 = open(f'{bpe_dir}/{corpus}.bpe.{lang_in}', 'r').read().split('\\n')\n",
    "            pd.DataFrame(list(zip(l2, l1))).to_csv(f'{bpe_dir}/{corpus}.tsv', header=None, index=False, sep='\\t')\n",
    "            save_f(bpe_dir / f'{corpus}.{lang_out}', l1)\n",
    "            save_f(bpe_dir / f'{corpus}.{lang_in}', l2)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_char_segments(word_dir, char_dir, lang_in, lang_out):\n",
    "    os.makedirs(char_dir, exist_ok=True)\n",
    "    for lang in [lang_out, lang_in]:\n",
    "        spm.SentencePieceTrainer.Train(f'--input={word_dir}/all.{lang} --model_prefix=m --vocab_size=1000 --character_coverage=1.0 --model_type=char')   \n",
    "\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.Load(\"m.model\")\n",
    "        for file in [f'train.{lang}', f'valid.{lang}', f'test.{lang}']:\n",
    "            f_in = open(word_dir / file, 'r')\n",
    "            f_out = open(char_dir / file, 'w')\n",
    "            \n",
    "            for line in f_in.read().split('\\n'):\n",
    "                temp = []\n",
    "                for word in sp.EncodeAsPieces(line.replace('<unk>', '<unknown>')):\n",
    "                    if str('\\u2581') in word:\n",
    "                        word = word.replace(str('\\u2581'), '@@')\n",
    "                    temp.append(word)\n",
    "                f_out.write(\" \".join(temp) + \"\\n\")\n",
    "\n",
    "            f_in.close()\n",
    "            f_out.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_segmentation(file_path, prepro_dir, lang_in, lang_out, n_opers=[5000]):\n",
    "    save_word_segments(file_path, prepro_dir / 'Flashcards' / 'word', lang_in, lang_out)\n",
    "    save_bpe_segments(prepro_dir / 'Flashcards' / 'word', prepro_dir / 'Flashcards', lang_in, lang_out, n_opers=n_opers)    \n",
    "    save_bpe_segments(prepro_dir / 'Flashcards' / 'word', prepro_dir / 'Flashcards', lang_in, lang_out, n_opers=n_opers, dropout=True)\n",
    "    save_char_segments(prepro_dir / 'Flashcards' / 'word', prepro_dir / 'Flashcards' / 'char', lang_in, lang_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/nob-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/yue-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/heb-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ind-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/kat-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/tur-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/war-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/cbk-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/bel-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/spa-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/slk-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/mar-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ara-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/mkd-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/max-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/slv-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/srp-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ber-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/shp-es\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/shp-en\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/tel-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ceb-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/jpn-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/cat-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/dtp-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/sqi-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/zza-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/kab-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/nds-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/mri-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/hun-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/fin-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ces-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/urd-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/arq-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/vie-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/tgl-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/bul-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/deu-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/est-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/zsm-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/lit-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ukr-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/nld-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/pam-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/por-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/aze-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/pes-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/bos-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/hin-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/khm-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ron-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/fra-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/pol-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/dan-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/eus-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/isl-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/afr-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ben-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/lvs-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/mya-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/kan-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ell-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/kur-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/chv-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/tha-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ita-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/hrv-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/cmn-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/swe-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/uig-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/kor-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/ilo-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/rus-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/mal-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/tam-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/glg-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/nst-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/kha-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/tuk-eng\n",
      "join corpus\n",
      "join corpus\n",
      "../data/raw/tat-eng\n",
      "join corpus\n",
      "join corpus\n"
     ]
    }
   ],
   "source": [
    "raw_dir = base_dir / 'transfer' / 'raw'\n",
    "prepro_dir = base_dir / 'transfer' / 'preprocessed'\n",
    "for dir_temp in os.listdir(raw_dir):\n",
    "    lang_dir = raw_dir / dir_temp\n",
    "    print(lang_dir)\n",
    "    file_path = lang_dir / (os.listdir(lang_dir)[0] if 'txt' in os.listdir(lang_dir)[0] else os.listdir(lang_dir)[1])\n",
    "    lang_in, lang_out = dir_temp.split('-')\n",
    "    save_segmentation(file_path, prepro_dir / dir_temp, lang_in, lang_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/translate/raw/Flashcards\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4d24f7b17bd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'txt'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlang_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msave_segmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepro_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdir_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "raw_dir = base_dir / 'translate' / 'raw'\n",
    "prepro_dir = base_dir / 'translate' / 'preprocessed'\n",
    "for dir_temp in os.listdir(raw_dir):\n",
    "    lang_dir = raw_dir / dir_temp\n",
    "    print(lang_dir)\n",
    "    file_path = lang_dir / (os.listdir(lang_dir)[0] if 'txt' in os.listdir(lang_dir)[0] else os.listdir(lang_dir)[1])\n",
    "    lang_in = \n",
    "    lang_out = dir_temp.split('-')\n",
    "    save_segmentation(file_path, prepro_dir / dir_temp, lang_in, lang_out, list(range(1000, 11000, 1000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flashcards'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1000, 11000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flashcards'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shp-es'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/raw/shp-es/all.shi')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
