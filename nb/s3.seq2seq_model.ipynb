{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: ‘./data/translate/raw/Religioso/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./data/translate/preprocessed/Religioso/char/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./nb/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./src/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./results/.ipynb_checkpoints’: No such file or directory\n",
      "find: ‘./joeynmt/joeynmt/.ipynb_checkpoints’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!find . -name .ipynb* -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "name: \"my_experiment\"\n",
    "\n",
    "# This configuration serves the purpose of documenting and explaining the settings, *NOT* as an example for good hyperparamter settings.\n",
    "\n",
    "data: # specify your data here\n",
    "    src: {lang_src}                       # src language: expected suffix of train files, e.g. \"train.de\"\n",
    "    trg: {lang_tgt}                       # trg language\n",
    "    train: {train_path}     # training data\n",
    "    dev: {dev_path}         # development data for validation\n",
    "    test: {test_path}       # test data for testing final model; optional\n",
    "    level: {level}                  # segmentation level: either \"word\", \"bpe\" or \"char\"\n",
    "    lowercase: True                 # lowercase the data, also for validation\n",
    "    max_sent_length: 150             # filter out longer sentences from training (src+trg)\n",
    "    src_voc_min_freq: 1             # src minimum frequency for a token to become part of the vocabulary\n",
    "    trg_voc_min_freq: 1             # trg minimum frequency for a token to become part of the vocabulary\n",
    "    #src_vocab: \"my_model/src_vocab.txt\"  # if specified, load a vocabulary from this file\n",
    "    #trg_vocab: \"my_model/trg_vocab.txt\"  # one token per line, line number is index\n",
    "\n",
    "testing:                            # specify which inference algorithm to use for testing (for validation it's always greedy decoding)\n",
    "    beam_size: 5                    # size of the beam for beam search\n",
    "    alpha: 1.0                      # length penalty for beam search\n",
    "\n",
    "training:                           # specify training details here\n",
    "    reset_best_ckpt: False          # if True, reset the tracking of the best checkpoint and scores. Use for domain adaptation or fine-tuning with new metrics or dev data.\n",
    "    reset_scheduler: False          # if True, overwrite scheduler in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    reset_optimizer: False          # if True, overwrite optimizer in loaded checkpoint with parameters specified in this config. Use for domain adaptation or fine-tuning.\n",
    "    random_seed: 42                 # set this seed to make training deterministic\n",
    "    optimizer: \"adam\"               # choices: \"sgd\", \"adam\", \"adadelta\", \"adagrad\", \"rmsprop\", default is SGD\n",
    "    learning_rate: 0.0005           # initial learning rate, default: 3.0e-4 / 0.005\n",
    "    learning_rate_min: 0.0001       # stop learning when learning rate is reduced below this threshold, default: 1.0e-8\n",
    "    #learning_rate_factor: 1        # factor for Noam scheduler (used with Transformer)\n",
    "    #learning_rate_warmup: 4000     # warmup steps for Noam scheduler (used with Transformer)\n",
    "    clip_grad_val: 1.0              # clip the gradients to this value when they exceed it, optional\n",
    "    #clip_grad_norm: 1.0            # norm clipping instead of value clipping\n",
    "    weight_decay: 0.                # l2 regularization, default: 0\n",
    "    batch_size: 48                  # mini-batch size as number of sentences (when batch_type is \"sentence\"; default) or total number of tokens (when batch_type is \"token\")\n",
    "    batch_type: \"sentence\"          # create batches with sentences (\"sentence\", default) or tokens (\"token\")\n",
    "    eval_batch_size: 10            # mini-batch size for evaluation (see batch_size above)\n",
    "    eval_batch_type: \"sentence\"     # evaluation batch type (\"sentence\", default) or tokens (\"token\")\n",
    "    batch_multiplier: 1             # increase the effective batch size with values >1 to batch_multiplier*batch_size without increasing memory consumption by making updates only every batch_multiplier batches\n",
    "    scheduling: \"plateau\"           # learning rate scheduling, optional, if not specified stays constant, options: \"plateau\", \"exponential\", \"decaying\", \"noam\" (for Transformer), \"warmupexponentialdecay\"\n",
    "    patience: 500                     # specific to plateau scheduler: wait for this many validations without improvement before decreasing the learning rate\n",
    "    decrease_factor: 0.5            # specific to plateau & exponential scheduler: decrease the learning rate by this factor\n",
    "    epochs: 30                      # train for this many epochs\n",
    "    validation_freq: {val_freq}            # validate after this many updates (number of mini-batches), default: 1000\n",
    "    logging_freq: 1000               # log the training progress after this many updates, default: 100\n",
    "    eval_metric: \"bleu\"             # validation metric, default: \"bleu\", other options: \"chrf\", \"token_accuracy\", \"sequence_accuracy\"\n",
    "    early_stopping_metric: \"eval_metric\"   # when a new high score on this metric is achieved, a checkpoint is written, when \"eval_metric\" (default) is maximized, when \"loss\" or \"ppl\" is minimized\n",
    "    model_dir: {model_dir} # directory where models and validation results are stored, required\n",
    "    overwrite: True                 # overwrite existing model directory, default: False. Do not set to True unless for debugging!\n",
    "    shuffle: True                   # shuffle the training data, default: True\n",
    "    use_cuda: True                  # use CUDA for acceleration on GPU, required. Set to False when working on CPU.\n",
    "    max_output_length: 60           # maximum output length for decoding, default: None. If set to None, allow sentences of max 1.5*src length\n",
    "    print_valid_sents: []    # print this many validation sentences during each validation run, default: [0, 1, 2]\n",
    "    keep_last_ckpts: 3              # keep this many of the latest checkpoints, if -1: all of them, default: 5\n",
    "    label_smoothing: 0.0            # label smoothing: reference tokens will have 1-label_smoothing probability instead of 1, rest of probability mass is uniformly distributed over the rest of the vocabulary, default: 0.0 (off)\n",
    "\n",
    "model:                              # specify your model architecture here\n",
    "    initializer: \"xavier\"           # initializer for all trainable weights (xavier, zeros, normal, uniform)\n",
    "    init_weight: 0.01               # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    init_gain: 1.0                  # gain for Xavier initializer (default: 1.0)\n",
    "    bias_initializer: \"zeros\"       # initializer for bias terms (xavier, zeros, normal, uniform)\n",
    "    embed_initializer: \"normal\"     # initializer for embeddings (xavier, zeros, normal, uniform)\n",
    "    embed_init_weight: 0.1          # weight to initialize; for uniform, will use [-weight, weight]\n",
    "    embed_init_gain: 1.0            # gain for Xavier initializer for embeddings (default: 1.0)\n",
    "    init_rnn_orthogonal: False      # use orthogonal initialization for recurrent weights (default: False)\n",
    "    lstm_forget_gate: 1.            # initialize LSTM forget gate with this value (default: 1.)\n",
    "    tied_embeddings: False           # tie src and trg embeddings, only applicable if vocabularies are the same, default: False\n",
    "    tied_softmax: False             # tie trg embeddings and softmax (for Transformer; can be used together with tied_embeddings), default: False\n",
    "    encoder:\n",
    "        type: \"recurrent\"           # encoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"             # type of recurrent unit to use, either \"gru\" or \"lstm\", default: \"lstm\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}      # size of embeddings\n",
    "            scale: False            # scale the embeddings by sqrt of their size, default: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}            # size of RNN\n",
    "        bidirectional: True         # use a bi-directional encoder, default: True\n",
    "        dropout: 0.3                # apply dropout to the inputs to the RNN, default: 0.0\n",
    "        num_layers: 2               # stack this many layers of equal size, default: 1\n",
    "        freeze: False               # if True, encoder parameters are not updated during training (does not include embedding parameters)\n",
    "    decoder:\n",
    "        type: \"recurrent\"           # decoder type: \"recurrent\" for LSTM or GRU, or \"transformer\" for a Transformer\n",
    "        rnn_type: \"gru\"\n",
    "        embeddings:\n",
    "            embedding_dim: {emb_size}\n",
    "            scale: False\n",
    "            freeze: False           # if True, embeddings are not updated during training\n",
    "        hidden_size: {hidden_size}\n",
    "        dropout: 0.3\n",
    "        hidden_dropout: 0.2         # apply dropout to the attention vector, default: 0.0\n",
    "        num_layers: 2\n",
    "        input_feeding: True         # combine hidden state and attention vector before feeding to rnn, default: True\n",
    "        init_hidden: \"last\"         # initialized the decoder hidden state: use linear projection of last encoder state (\"bridge\") or simply the last state (\"last\") or zeros (\"zero\"), default: \"bridge\"\n",
    "        attention: \"bahdanau\"       # attention mechanism, choices: \"bahdanau\" (MLP attention), \"luong\" (bilinear attention), default: \"bahdanau\"\n",
    "        freeze: False               # if True, decoder parameters are not updated during training (does not include embedding parameters, but attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/translate/Religioso/char\n",
      "2020-02-25 10:08:37,506 Hello! This is Joey-NMT.\n",
      "2020-02-25 10:08:41.511416: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2020-02-25 10:08:41.511558: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2020-02-25 10:08:41.511580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2020-02-25 10:08:46,385 Total params: 12451936\n",
      "2020-02-25 10:08:46,385 Trainable parameters: ['decoder.att_vector_layer.bias', 'decoder.att_vector_layer.weight', 'decoder.attention.energy_layer.weight', 'decoder.attention.key_layer.weight', 'decoder.attention.query_layer.weight', 'decoder.output_layer.weight', 'decoder.rnn.bias_hh_l0', 'decoder.rnn.bias_hh_l1', 'decoder.rnn.bias_ih_l0', 'decoder.rnn.bias_ih_l1', 'decoder.rnn.weight_hh_l0', 'decoder.rnn.weight_hh_l1', 'decoder.rnn.weight_ih_l0', 'decoder.rnn.weight_ih_l1', 'encoder.rnn.bias_hh_l0', 'encoder.rnn.bias_hh_l0_reverse', 'encoder.rnn.bias_hh_l1', 'encoder.rnn.bias_hh_l1_reverse', 'encoder.rnn.bias_ih_l0', 'encoder.rnn.bias_ih_l0_reverse', 'encoder.rnn.bias_ih_l1', 'encoder.rnn.bias_ih_l1_reverse', 'encoder.rnn.weight_hh_l0', 'encoder.rnn.weight_hh_l0_reverse', 'encoder.rnn.weight_hh_l1', 'encoder.rnn.weight_hh_l1_reverse', 'encoder.rnn.weight_ih_l0', 'encoder.rnn.weight_ih_l0_reverse', 'encoder.rnn.weight_ih_l1', 'encoder.rnn.weight_ih_l1_reverse', 'src_embed.lut.weight', 'trg_embed.lut.weight']\n",
      "2020-02-25 10:08:55,235 cfg.name                           : my_experiment\n",
      "2020-02-25 10:08:55,235 cfg.data.src                       : es\n",
      "2020-02-25 10:08:55,235 cfg.data.trg                       : shp\n",
      "2020-02-25 10:08:55,236 cfg.data.train                     : data/translate/preprocessed/Religioso/char/train\n",
      "2020-02-25 10:08:55,236 cfg.data.dev                       : data/translate/preprocessed/Religioso/char/valid\n",
      "2020-02-25 10:08:55,236 cfg.data.test                      : data/translate/preprocessed/Religioso/char/test\n",
      "2020-02-25 10:08:55,236 cfg.data.level                     : char\n",
      "2020-02-25 10:08:55,236 cfg.data.lowercase                 : True\n",
      "2020-02-25 10:08:55,236 cfg.data.max_sent_length           : 150\n",
      "2020-02-25 10:08:55,236 cfg.data.src_voc_min_freq          : 1\n",
      "2020-02-25 10:08:55,236 cfg.data.trg_voc_min_freq          : 1\n",
      "2020-02-25 10:08:55,236 cfg.testing.beam_size              : 5\n",
      "2020-02-25 10:08:55,236 cfg.testing.alpha                  : 1.0\n",
      "2020-02-25 10:08:55,236 cfg.training.reset_best_ckpt       : False\n",
      "2020-02-25 10:08:55,237 cfg.training.reset_scheduler       : False\n",
      "2020-02-25 10:08:55,237 cfg.training.reset_optimizer       : False\n",
      "2020-02-25 10:08:55,237 cfg.training.random_seed           : 42\n",
      "2020-02-25 10:08:55,237 cfg.training.optimizer             : adam\n",
      "2020-02-25 10:08:55,237 cfg.training.learning_rate         : 0.0005\n",
      "2020-02-25 10:08:55,237 cfg.training.learning_rate_min     : 0.0001\n",
      "2020-02-25 10:08:55,237 cfg.training.clip_grad_val         : 1.0\n",
      "2020-02-25 10:08:55,237 cfg.training.weight_decay          : 0.0\n",
      "2020-02-25 10:08:55,237 cfg.training.batch_size            : 48\n",
      "2020-02-25 10:08:55,237 cfg.training.batch_type            : sentence\n",
      "2020-02-25 10:08:55,237 cfg.training.eval_batch_size       : 10\n",
      "2020-02-25 10:08:55,238 cfg.training.eval_batch_type       : sentence\n",
      "2020-02-25 10:08:55,238 cfg.training.batch_multiplier      : 1\n",
      "2020-02-25 10:08:55,238 cfg.training.scheduling            : plateau\n",
      "2020-02-25 10:08:55,238 cfg.training.patience              : 500\n",
      "2020-02-25 10:08:55,238 cfg.training.decrease_factor       : 0.5\n",
      "2020-02-25 10:08:55,238 cfg.training.epochs                : 30\n",
      "2020-02-25 10:08:55,238 cfg.training.validation_freq       : 10\n",
      "2020-02-25 10:08:55,238 cfg.training.logging_freq          : 1000\n",
      "2020-02-25 10:08:55,238 cfg.training.eval_metric           : bleu\n",
      "2020-02-25 10:08:55,238 cfg.training.early_stopping_metric : eval_metric\n",
      "2020-02-25 10:08:55,238 cfg.training.model_dir             : results/translate/es-shp_300_512/char\n",
      "2020-02-25 10:08:55,239 cfg.training.overwrite             : True\n",
      "2020-02-25 10:08:55,239 cfg.training.shuffle               : True\n",
      "2020-02-25 10:08:55,239 cfg.training.use_cuda              : True\n",
      "2020-02-25 10:08:55,239 cfg.training.max_output_length     : 60\n",
      "2020-02-25 10:08:55,239 cfg.training.print_valid_sents     : []\n",
      "2020-02-25 10:08:55,239 cfg.training.keep_last_ckpts       : 3\n",
      "2020-02-25 10:08:55,239 cfg.training.label_smoothing       : 0.0\n",
      "2020-02-25 10:08:55,239 cfg.model.initializer              : xavier\n",
      "2020-02-25 10:08:55,239 cfg.model.init_weight              : 0.01\n",
      "2020-02-25 10:08:55,239 cfg.model.init_gain                : 1.0\n",
      "2020-02-25 10:08:55,239 cfg.model.bias_initializer         : zeros\n",
      "2020-02-25 10:08:55,240 cfg.model.embed_initializer        : normal\n",
      "2020-02-25 10:08:55,240 cfg.model.embed_init_weight        : 0.1\n",
      "2020-02-25 10:08:55,240 cfg.model.embed_init_gain          : 1.0\n",
      "2020-02-25 10:08:55,240 cfg.model.init_rnn_orthogonal      : False\n",
      "2020-02-25 10:08:55,240 cfg.model.lstm_forget_gate         : 1.0\n",
      "2020-02-25 10:08:55,240 cfg.model.tied_embeddings          : False\n",
      "2020-02-25 10:08:55,240 cfg.model.tied_softmax             : False\n",
      "2020-02-25 10:08:55,240 cfg.model.encoder.type             : recurrent\n",
      "2020-02-25 10:08:55,240 cfg.model.encoder.rnn_type         : gru\n",
      "2020-02-25 10:08:55,240 cfg.model.encoder.embeddings.embedding_dim : 300\n",
      "2020-02-25 10:08:55,240 cfg.model.encoder.embeddings.scale : False\n",
      "2020-02-25 10:08:55,241 cfg.model.encoder.embeddings.freeze : False\n",
      "2020-02-25 10:08:55,241 cfg.model.encoder.hidden_size      : 512\n",
      "2020-02-25 10:08:55,241 cfg.model.encoder.bidirectional    : True\n",
      "2020-02-25 10:08:55,241 cfg.model.encoder.dropout          : 0.3\n",
      "2020-02-25 10:08:55,241 cfg.model.encoder.num_layers       : 2\n",
      "2020-02-25 10:08:55,241 cfg.model.encoder.freeze           : False\n",
      "2020-02-25 10:08:55,241 cfg.model.decoder.type             : recurrent\n",
      "2020-02-25 10:08:55,241 cfg.model.decoder.rnn_type         : gru\n",
      "2020-02-25 10:08:55,241 cfg.model.decoder.embeddings.embedding_dim : 300\n",
      "2020-02-25 10:08:55,241 cfg.model.decoder.embeddings.scale : False\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.embeddings.freeze : False\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.hidden_size      : 512\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.dropout          : 0.3\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.hidden_dropout   : 0.2\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.num_layers       : 2\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.input_feeding    : True\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.init_hidden      : last\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.attention        : bahdanau\n",
      "2020-02-25 10:08:55,242 cfg.model.decoder.freeze           : False\n",
      "2020-02-25 10:08:55,242 Data set sizes: \n",
      "\ttrain 4975,\n",
      "\tvalid 749,\n",
      "\ttest 749\n",
      "2020-02-25 10:08:55,242 First training example:\n",
      "\t[SRC] @@ i k a x b i @@ j a @@ n o n @@ i b o n @@ j o i r a @@ j a w e t i a n b i @@ k e y ó y a m a i @@ i k i\n",
      "\t[TRG] @@ p e r o @@ l a @@ p a l a b r a @@ d e l @@ s e ñ o r @@ p e r m a n e c e @@ e t e r n a m e n t e @@ e s t a @@ p a l a b r a @@ e s @@ e l @@ e v a n g e l i o @@ q u e @@ s e @@ l e s @@ h a @@ a n u n c i a d o @@ a @@ u s t e d e s\n",
      "2020-02-25 10:08:55,243 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) a (5) @@ (6) i (7) n (8) o (9) k\n",
      "2020-02-25 10:08:55,243 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) @@ (5) e (6) a (7) o (8) s (9) n\n",
      "2020-02-25 10:08:55,243 Number of Src words (types): 36\n",
      "2020-02-25 10:08:55,243 Number of Trg words (types): 36\n",
      "2020-02-25 10:08:55,243 Model(\n",
      "\tencoder=RecurrentEncoder(GRU(300, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)),\n",
      "\tdecoder=RecurrentDecoder(rnn=GRU(812, 512, num_layers=2, batch_first=True, dropout=0.3), attention=BahdanauAttention),\n",
      "\tsrc_embed=Embeddings(embedding_dim=300, vocab_size=36),\n",
      "\ttrg_embed=Embeddings(embedding_dim=300, vocab_size=36))\n",
      "2020-02-25 10:08:55,244 EPOCH 1\n",
      "2020-02-25 10:09:15,146 Hooray! New best validation result [eval_metric]!\n",
      "2020-02-25 10:09:15,146 Saving new checkpoint.\n",
      "2020-02-25 10:09:19,249 Validation result (greedy) at epoch   1, step       10: bleu:   0.00, loss: 166065.5312, ppl:  17.8630, duration: 18.5022s\n",
      "2020-02-25 10:09:38,054 Validation result (greedy) at epoch   1, step       20: bleu:   0.00, loss: 162445.1719, ppl:  16.7749, duration: 14.4148s\n",
      "2020-02-25 10:09:56,641 Validation result (greedy) at epoch   1, step       30: bleu:   0.00, loss: 158454.1719, ppl:  15.6521, duration: 14.4111s\n",
      "2020-02-25 10:10:15,029 Validation result (greedy) at epoch   1, step       40: bleu:   0.00, loss: 149395.0625, ppl:  13.3745, duration: 14.4054s\n",
      "2020-02-25 10:10:33,790 Validation result (greedy) at epoch   1, step       50: bleu:   0.00, loss: 140036.0000, ppl:  11.3689, duration: 14.4229s\n",
      "2020-02-25 10:10:52,293 Validation result (greedy) at epoch   1, step       60: bleu:   0.00, loss: 133486.3906, ppl:  10.1471, duration: 14.3721s\n",
      "2020-02-25 10:11:10,963 Validation result (greedy) at epoch   1, step       70: bleu:   0.00, loss: 129335.2969, ppl:   9.4417, duration: 14.4243s\n",
      "2020-02-25 10:11:29,797 Validation result (greedy) at epoch   1, step       80: bleu:   0.00, loss: 126377.1406, ppl:   8.9691, duration: 14.3960s\n",
      "2020-02-25 10:11:47,961 Validation result (greedy) at epoch   1, step       90: bleu:   0.00, loss: 123890.2812, ppl:   8.5901, duration: 14.2109s\n",
      "2020-02-25 10:12:06,361 Validation result (greedy) at epoch   1, step      100: bleu:   0.00, loss: 123115.6875, ppl:   8.4754, duration: 14.4254s\n",
      "2020-02-25 10:12:07,974 Epoch   1: total training loss 17488.72\n",
      "2020-02-25 10:12:07,975 EPOCH 2\n",
      "2020-02-25 10:12:25,320 Validation result (greedy) at epoch   2, step      110: bleu:   0.00, loss: 118773.0391, ppl:   7.8600, duration: 14.4281s\n",
      "2020-02-25 10:12:44,173 Validation result (greedy) at epoch   2, step      120: bleu:   0.00, loss: 117022.5000, ppl:   7.6247, duration: 14.2808s\n",
      "2020-02-25 10:13:02,431 Validation result (greedy) at epoch   2, step      130: bleu:   0.00, loss: 116022.2500, ppl:   7.4935, duration: 14.4194s\n",
      "2020-02-25 10:13:21,385 Validation result (greedy) at epoch   2, step      140: bleu:   0.00, loss: 114102.9062, ppl:   7.2479, duration: 14.4065s\n",
      "2020-02-25 10:13:40,061 Validation result (greedy) at epoch   2, step      150: bleu:   0.00, loss: 112728.0234, ppl:   7.0770, duration: 14.3704s\n",
      "2020-02-25 10:13:58,360 Validation result (greedy) at epoch   2, step      160: bleu:   0.00, loss: 112095.4453, ppl:   6.9997, duration: 14.3887s\n"
     ]
    }
   ],
   "source": [
    "base_path = 'data/translate/preprocessed'\n",
    "datas = os.listdir(base_path)\n",
    "emb_size = 300\n",
    "hidden_size = 512\n",
    "for data in datas:\n",
    "    data_path = os.path.join(base_path, data)\n",
    "    for lang_in, lang_out in [['es', 'shp'], ['shp', 'es']]:\n",
    "        segmentations = os.listdir(data_path)\n",
    "        for segment in segmentations:\n",
    "            segment_path = os.path.join(data_path, segment)\n",
    "            print(os.path.join('results/translate', data, segment))\n",
    "            if 'bpe_drop' in segment:\n",
    "                level = 'bpe'\n",
    "            elif 'bpe' in segment:\n",
    "                level = 'bpe'\n",
    "            elif 'char' in segment:\n",
    "                level = 'char'\n",
    "            elif 'word' in segment:\n",
    "                level = 'word'\n",
    "            elif 'syl' in segment:\n",
    "                level = 'syl'\n",
    "            else:\n",
    "                level = None\n",
    "                \n",
    "            val_freq = 10\n",
    "            f_config = config.format(lang_src=lang_in, lang_tgt=lang_out, \n",
    "                                     train_path=os.path.join(segment_path, 'train'),\n",
    "                                     test_path=os.path.join(segment_path, 'test'),\n",
    "                                     dev_path=os.path.join(segment_path, 'valid'),\n",
    "                                     level=level,\n",
    "                                     emb_size=emb_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     val_freq=val_freq,\n",
    "                                     model_dir=os.path.join('results/translate',\\\n",
    "                                                            f'{lang_in}-{lang_out}_{emb_size}_{hidden_size}', segment))\n",
    "\n",
    "            with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=\"test\"),'w') as f:\n",
    "                f.write(f_config)\n",
    "\n",
    "            !python3 joeynmt/joeynmt train \"joeynmt/configs/transformer_test.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['char',\n",
       " 'bpe_drop_10000',\n",
       " 'bpe_drop_2000',\n",
       " 'bpe_drop_3000',\n",
       " 'bpe_1000',\n",
       " 'bpe_3000',\n",
       " 'bpe_7000',\n",
       " 'bpe_drop_8000',\n",
       " 'bpe_10000',\n",
       " 'bpe_2000',\n",
       " 'bpe_drop_6000',\n",
       " 'word',\n",
       " 'bpe_5000',\n",
       " 'bpe_drop_9000',\n",
       " 'bpe_drop_7000',\n",
       " 'bpe_9000',\n",
       " 'bpe_drop_5000',\n",
       " 'bpe_6000',\n",
       " 'bpe_4000',\n",
       " 'bpe_drop_4000',\n",
       " 'bpe_drop_1000',\n",
       " 'bpe_8000']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6745/6745 [00:00<00:00, 30522.11it/s]\n"
     ]
    }
   ],
   "source": [
    "base_path = 'data/translate/preprocessed'\n",
    "langs = os.listdir(base_path)\n",
    "emb_size = 300\n",
    "hidden_size = 256\n",
    "for lang in langs:\n",
    "    lang_path = os.path.join(base_path, lang)\n",
    "    corpus = os.listdir(lang_path)\n",
    "    corpus_path = os.path.join(lang_path, corpus[0])\n",
    "    segmentations = os.listdir(corpus_path)\n",
    "    lang_src = lang.split(\"-\")[0]\n",
    "    lang_tgt = lang.split(\"-\")[1]\n",
    "    for lang_in, lang_out in [[lang_src, lang_tgt], [lang_tgt, lang_src]]:\n",
    "        for segmentation in segmentations:\n",
    "            segmentation_path = os.path.join(corpus_path, segmentation)\n",
    "            print(os.path.join('results/nmt', lang, segmentation))\n",
    "            partitions = os.listdir(segmentation_path)\n",
    "            if 'bpe_drop' in segmentation:\n",
    "                level = 'bpe'\n",
    "            elif 'bpe' in segmentation:\n",
    "                level = 'bpe'\n",
    "            elif 'char' in segmentation:\n",
    "                level = 'char'\n",
    "            elif 'word' in segmentation:\n",
    "                level = 'word'\n",
    "            elif 'syl' in segmentation:\n",
    "                level = 'syl'\n",
    "            else:\n",
    "                level = None\n",
    "                \n",
    "            if lang in ['slk-eng', 'shp-es', 'shp-en', 'cat-eng', 'ces-eng', 'bul-eng',\\\n",
    "                        'ron-eng', 'lvs-eng', 'hrv-eng']:\n",
    "                val_freq = 10\n",
    "            else:\n",
    "                val_freq = 40\n",
    "            f_config = config.format(lang_src=lang_in, lang_tgt=lang_out, \n",
    "                                     train_path=os.path.join(segmentation_path, 'train'),\n",
    "                                     test_path=os.path.join(segmentation_path, 'test'),\n",
    "                                     dev_path=os.path.join(segmentation_path, 'valid'),\n",
    "                                     level=level,\n",
    "                                     emb_size=emb_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     val_freq=val_freq,\n",
    "                                     model_dir=os.path.join('results/nmt',\\\n",
    "                                                            f'{lang_in}-{lang_out}_{emb_size}_{hidden_size}', segmentation))\n",
    "\n",
    "            with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=\"test\"),'w') as f:\n",
    "                f.write(f_config)\n",
    "\n",
    "            !python3 joeynmt/joeynmt train \"joeynmt/configs/transformer_test.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed(seed)\n",
    "rnn = torch.nn.LSTM(emb_sz, n_hid, dropout=0.25, num_layers=2, bidirectional=True, batch_first=True)\n",
    "\n",
    "random_seed(seed)\n",
    "encoder = PytorchSeq2SeqWrapper(rnn)\n",
    "\n",
    "random_seed(seed)\n",
    "source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})\n",
    "\n",
    "attention = DotProductAttention()\n",
    "#attention = LinearAttention(n_hid, n_hid, activation=Activation.by_name('tanh')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_decoding_steps = 100   # TODO: make this variable\n",
    "random_seed(seed)\n",
    "model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
    "                      target_embedding_dim=emb_sz,\n",
    "                      target_namespace='target_tokens',\n",
    "                      attention=attention,\n",
    "                      seed=seed,\n",
    "                      scheduled_sampling_ratio=0,\n",
    "                      mode='word',\n",
    "                      beam_size=1,\n",
    "                      use_bleu=True).cuda(cuda_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "iterator = BucketIterator(batch_size=48, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
      "loss: 6.6997 ||: 100%|██████████| 125/125 [00:12<00:00,  9.84it/s]\n",
      "BLEU: 0.0000, loss: 6.4466 ||: 100%|██████████| 16/16 [00:01<00:00, 10.13it/s]\n",
      "loss: 6.0505 ||: 100%|██████████| 125/125 [00:12<00:00, 10.34it/s]\n",
      "BLEU: 0.0000, loss: 6.2729 ||: 100%|██████████| 16/16 [00:03<00:00,  4.33it/s]\n",
      "loss: 5.7174 ||: 100%|██████████| 125/125 [00:12<00:00, 10.30it/s]\n",
      "BLEU: 0.0000, loss: 6.1824 ||: 100%|██████████| 16/16 [00:03<00:00,  4.24it/s]\n",
      "loss: 5.3607 ||: 100%|██████████| 125/125 [00:12<00:00, 10.14it/s]\n",
      "BLEU: 0.0025, loss: 6.1199 ||: 100%|██████████| 16/16 [00:04<00:00,  3.84it/s]\n",
      "loss: 4.9794 ||: 100%|██████████| 125/125 [00:12<00:00, 10.29it/s]\n",
      "BLEU: 0.0000, loss: 6.0624 ||: 100%|██████████| 16/16 [00:02<00:00,  5.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 3,\n",
       " 'peak_cpu_memory_MB': 2419.492,\n",
       " 'peak_gpu_0_memory_MB': 2851,\n",
       " 'training_duration': '00:01:17',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 4,\n",
       " 'epoch': 4,\n",
       " 'training_loss': 4.979407690048218,\n",
       " 'training_cpu_memory_MB': 2419.492,\n",
       " 'training_gpu_0_memory_MB': 2851,\n",
       " 'validation_BLEU': 1.8683536506125043e-06,\n",
       " 'validation_loss': 6.062360018491745,\n",
       " 'best_validation_BLEU': 0.002512392654034492,\n",
       " 'best_validation_loss': 6.1198570728302}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  validation_metric='+BLEU',\n",
    "                  num_epochs=5,\n",
    "                  #serialization_dir='temp1',\n",
    "                  cuda_device=cuda_id)\n",
    "\n",
    "trainer.train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
      "loss: 6.7027 ||: 100%|██████████| 125/125 [00:13<00:00,  9.19it/s]\n",
      "BLEU: 0.0000, loss: 6.4570 ||: 100%|██████████| 16/16 [00:01<00:00,  9.67it/s]\n",
      "loss: 6.0616 ||: 100%|██████████| 125/125 [00:13<00:00,  9.30it/s]\n",
      "BLEU: 0.0000, loss: 6.2846 ||: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\n",
      "loss: 5.7169 ||: 100%|██████████| 125/125 [00:13<00:00,  9.28it/s]\n",
      "BLEU: 0.0000, loss: 6.1733 ||: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
      "loss: 5.3694 ||: 100%|██████████| 125/125 [00:13<00:00,  9.25it/s]\n",
      "BLEU: 0.0035, loss: 6.1236 ||: 100%|██████████| 16/16 [00:02<00:00,  5.98it/s]\n",
      "loss: 4.9955 ||: 100%|██████████| 125/125 [00:13<00:00,  9.42it/s]\n",
      "BLEU: 0.0000, loss: 6.0683 ||: 100%|██████████| 16/16 [00:02<00:00,  5.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 3,\n",
       " 'peak_cpu_memory_MB': 2502.136,\n",
       " 'peak_gpu_0_memory_MB': 2851,\n",
       " 'training_duration': '00:01:23',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 4,\n",
       " 'epoch': 4,\n",
       " 'training_loss': 4.995536445617676,\n",
       " 'training_cpu_memory_MB': 2502.136,\n",
       " 'training_gpu_0_memory_MB': 2851,\n",
       " 'validation_BLEU': 1.6411359862230068e-06,\n",
       " 'validation_loss': 6.068288892507553,\n",
       " 'best_validation_BLEU': 0.003538747884663674,\n",
       " 'best_validation_loss': 6.123574078083038}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  validation_metric='+BLEU',\n",
    "                  num_epochs=5,\n",
    "                  #serialization_dir='temp1',\n",
    "                  cuda_device=cuda_id)\n",
    "\n",
    "trainer.train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
      "loss: 6.7027 ||: 100%|██████████| 125/125 [00:13<00:00,  9.19it/s]\n",
      "BLEU: 0.0000, loss: 6.4570 ||: 100%|██████████| 16/16 [00:01<00:00,  9.67it/s]\n",
      "loss: 6.0616 ||: 100%|██████████| 125/125 [00:13<00:00,  9.30it/s]\n",
      "BLEU: 0.0000, loss: 6.2846 ||: 100%|██████████| 16/16 [00:04<00:00,  3.71it/s]\n",
      "loss: 5.7169 ||: 100%|██████████| 125/125 [00:13<00:00,  9.28it/s]\n",
      "BLEU: 0.0000, loss: 6.1733 ||: 100%|██████████| 16/16 [00:04<00:00,  3.48it/s]\n",
      "loss: 5.3874 ||:  35%|███▌      | 44/125 [00:05<00:09,  8.32it/s]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  validation_metric='+BLEU',\n",
    "                  num_epochs=5,\n",
    "                  #serialization_dir='temp1',\n",
    "                  cuda_device=cuda_id)\n",
    "\n",
    "trainer.train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 8,\n",
       " 'best_validation_BLEU': 0.011378326800353748,\n",
       " 'best_validation_loss': 6.618885815143585}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLEU: 0.00, loss: 6.68 ||: 100%|██████████| 16/16 [00:02<00:00,  7.01it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"temp/best.th\", 'rb') as f_model:\n",
    "    model.load_state_dict(torch.load(f_model))\n",
    "model.eval()\n",
    "e = evaluate(model, test_dataset, iterator, cuda_id, None)    \n",
    "\n",
    "predictor = SimpleSeq2SeqPredictor(model, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, no, junten, tesoros, y, reservas, aquí, en, la, tierra, donde, la, polilla, y, el, óxido, hacen, estragos, y, donde, los, ladrones, rompen, el, muro, y, roban, @end@]\n",
      "GOLD: [@start@, nato, mainxon, icha, jawéki, kaiakasi, iamakanwe, ja, jawékibora, arai, keyotai, jatíribibora, jawen, ibon, pia, payotai, jainxonra, yometsobaon, mato, yometsoati, atipanke, @end@]\n",
      "PRED: ['jaskara', 'iken', 'ja', 'non', 'mato', 'ja', 'non', 'ja', 'ja', 'ja', 'ja', 'ja', 'ja', 'non', 'mato']\n",
      "SOURCE: [@start@, ahí, tienen, pues, a, tito, mi, compañero, y, ayudante, cerca, de, ustedes, y, con, él, tienen, a, hermanos, nuestros, delegados, de, las, iglesias, personas, que, son, la, gloria, de, cristo, @end@]\n",
      "GOLD: [@start@, ja, titora, ebetanbi, niai, iki, itan, mato, akinni, ebetanbi, teetai, iki, jatian, ja, non, wetsa, rabéra, non, wetsabaon, raana, iki, jaton, ikábo, oinxonra, jonibaon, non, ibo, rabikanai, @end@]\n",
      "PRED: ['ja', 'mato', 'mato', 'yoiai', 'ja', 'non', 'mato', 'ja', 'non', 'mato', 'ja', 'non', 'mato', 'ja', 'non', 'mato', 'mato', 'mato', 'mato', 'mato', 'mato']\n",
      "SOURCE: [@start@, la, paz, esté, con, ustedes, @end@]\n",
      "GOLD: [@start@, mato, jakonbires, dios, betan, ibanon, @end@]\n",
      "PRED: ['enra', 'mato', 'yoiribiai']\n",
      "SOURCE: [@start@, dijo, faraón, a, josé, @end@]\n",
      "GOLD: [@start@, ja, josé, moa, joketian, faraónman, yoia, iki, neskáakin, @end@]\n",
      "PRED: ['jatian', 'ja', 'jaonmea', 'onannaibo', 'neskáakin', 'yoia', 'iki']\n",
      "SOURCE: [@start@, también, lo, acompañaban, otras, barcas, @end@]\n",
      "GOLD: [@start@, jatian, jato, chibani, jatíribibo, wetsa, botenbo, bokana, iki, @end@]\n",
      "PRED: ['ja', 'ja', 'pichika', 'chonka', 'nete', 'iti', 'iki']\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "for instance in itertools.islice(validation_dataset, 5):\n",
    "    print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
    "    print('GOLD:', instance.fields['target_tokens'].tokens)\n",
    "    print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/home/krivas/.local/share/jupyter/runtime/kernel-291d3c59-6d17-4313-8051-f97e189c433c.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-eb3acdde8a90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fire/core.py\u001b[0m in \u001b[0;36mFire\u001b[0;34m(component, command, name)\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaller_locals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mcomponent_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Fire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcomponent_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fire/core.py\u001b[0m in \u001b[0;36m_Fire\u001b[0;34m(component, args, context, name)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         component, consumed_args, remaining_args, capacity = _CallCallable(\n\u001b[0;32m--> 366\u001b[0;31m             component, remaining_args)\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# Update the trace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fire/core.py\u001b[0m in \u001b[0;36m_CallCallable\u001b[0;34m(fn, args)\u001b[0m\n\u001b[1;32m    540\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0mvarargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvarargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-eb3acdde8a90>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir, results_dir, cuda_id, emb_sz, n_hid, epochs, seed)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#name_file='../data/nmt/preprocessed/shp-es/Flashcards/syl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#dir_results = 'results_3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'preprocessed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/home/krivas/.local/share/jupyter/runtime/kernel-291d3c59-6d17-4313-8051-f97e189c433c.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(data_dir, results_dir, cuda_id=1, emb_sz=300, n_hid=512, epochs=50, seed=0):\n",
    "    #name_file='../data/nmt/preprocessed/shp-es/Flashcards/syl'\n",
    "    #dir_results = 'results_3'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    data_dir = Path.cwd() / data_dir / 'preprocessed'\n",
    "    results_dir = Path.cwd() / results_dir\n",
    "    print(data_dir)\n",
    "    langs = os.listdir(data_dir)\n",
    "    print(langs)\n",
    "    for lang in langs:\n",
    "        folders = os.listdir(data_dir / lang)\n",
    "        with open(results_dir / f'{lang}_{emb_sz}_{seed}.txt', 'w') as f_results:\n",
    "            for folder in folders:\n",
    "                segmentations = os.listdir(data_dir / lang / folder)\n",
    "                print(segmentations)\n",
    "                for segmentation in segmentations:\n",
    "                    print('start with:', segmentation)\n",
    "                    path = data_dir / lang / folder / segmentation\n",
    "                    shutil.rmtree('temp', ignore_errors=True)\n",
    "                    os.makedirs('temp', exist_ok=True)\n",
    "                    \n",
    "                    random_seed(seed)\n",
    "                    reader = Seq2SeqDatasetReader(\n",
    "                        source_tokenizer=WordTokenizer(word_splitter=JustSpacesWordSplitter()),\n",
    "                        target_tokenizer=WordTokenizer(word_splitter=JustSpacesWordSplitter()),\n",
    "                        delimiter='\\t',\n",
    "                        source_token_indexers={'tokens': SingleIdTokenIndexer()},\n",
    "                        target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\n",
    "\n",
    "                    train_dataset = reader.read(os.path.join(path, 'train.tsv'))\n",
    "                    validation_dataset = reader.read(os.path.join(path, 'valid.tsv'))\n",
    "                    test_dataset = reader.read(os.path.join(path, 'test.tsv'))\n",
    "\n",
    "\n",
    "                    vocab = Vocabulary.from_instances(train_dataset + validation_dataset,\n",
    "                                                      min_count={'tokens': 1, 'target_tokens': 1})\n",
    "\n",
    "                    random_seed(seed)\n",
    "                    en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                                             embedding_dim=emb_sz)\n",
    "\n",
    "                    random_seed(seed)\n",
    "                    encoder = PytorchSeq2SeqWrapper(torch.nn.GRU(emb_sz, n_hid, dropout=0.25, num_layers=2, bidirectional=True, batch_first=True))\n",
    "\n",
    "                    source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})\n",
    "                    attention = DotProductAttention()\n",
    "\n",
    "                    max_decoding_steps = 100   # TODO: make this variable\n",
    "                    random_seed(seed)\n",
    "                    model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
    "                                          target_embedding_dim=emb_sz,\n",
    "                                          target_namespace='target_tokens',\n",
    "                                          attention=attention,\n",
    "                                          seed=seed,\n",
    "                                          mode=segmentation,\n",
    "                                          beam_size=1,\n",
    "                                          use_bleu=True).cuda(cuda_id)\n",
    "\n",
    "                    optimizer = optim.Adam(model.parameters())\n",
    "                    iterator = BucketIterator(batch_size=100, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
    "\n",
    "                    iterator.index_with(vocab)\n",
    "\n",
    "                    if not os.path.exists('temp/best.th'):\n",
    "                        trainer = Trainer(model=model,\n",
    "                                              optimizer=optimizer,\n",
    "                                              iterator=iterator,\n",
    "                                              train_dataset=train_dataset,\n",
    "                                              validation_dataset=validation_dataset,\n",
    "                                              validation_metric='+BLEU',\n",
    "                                              num_epochs=epochs,\n",
    "                                              serialization_dir='temp',\n",
    "                                              cuda_device=cuda_id)\n",
    "\n",
    "                        trainer.train()     \n",
    "\n",
    "                    with open(\"temp/best.th\", 'rb') as f_model:\n",
    "                        model.load_state_dict(torch.load(f_model))\n",
    "                    model.eval()\n",
    "                    e = evaluate(model, test_dataset, iterator, cuda_id, None)    \n",
    "\n",
    "                    predictor = SimpleSeq2SeqPredictor(model, reader)\n",
    "\n",
    "                    scores = []\n",
    "                    if segmentation == 'syl' or segmentation == 'char':\n",
    "                        for instance in test_dataset:\n",
    "                            ref = (''.join([token.text for token in instance.fields['target_tokens'].tokens][1:-1]).replace('@@', ' ').replace('+', '').strip()).split()\n",
    "                            hyp = (''.join(predictor.predict_instance(instance)['predicted_tokens']).replace('@@', ' ').replace('+', '').strip()).split()\n",
    "                            score = cer(hyp, ref)\n",
    "                            scores.append(score)\n",
    "                    else:\n",
    "                        for instance in test_dataset:\n",
    "                            ref = (' '.join([token.text for token in instance.fields['target_tokens'].tokens][1:-1]).replace('@@ ', '').replace('+', '').strip()).split()\n",
    "                            hyp = (' '.join(predictor.predict_instance(instance)['predicted_tokens']).replace('@@ ', '').replace('+', '').strip()).split()\n",
    "                            score = cer(hyp, ref)\n",
    "                            scores.append(score)\n",
    "\n",
    "                    average = sum(scores) / len(scores)\n",
    "                    variance = sum((s - average) ** 2 for s in scores) / len(scores)\n",
    "                    standard_deviation = math.sqrt(variance)\n",
    "\n",
    "                    print(f'{str(segmentation)} CharacTER: {average} BLEU: {e[\"BLEU\"]}', file=f_results)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Seq2SeqDatasetReader(\n",
    "        source_tokenizer=WordTokenizer(),\n",
    "        target_tokenizer=WordTokenizer(),\n",
    "        source_token_indexers={'tokens': SingleIdTokenIndexer()},\n",
    "        target_token_indexers={'tokens': SingleIdTokenIndexer(namespace='target_tokens')})\n",
    "train_dataset = reader.read('../data/translate/preprocessed/Educativo/word/train.tsv')\n",
    "validation_dataset = reader.read('../data/translate/preprocessed/Educativo/word/valid.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset,\n",
    "                                      min_count={'tokens': 1, 'target_tokens': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                             embedding_dim=EN_EMBEDDING_DIM)\n",
    "encoder = PytorchSeq2SeqWrapper(\n",
    "torch.nn.LSTM(EN_EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_embedder = BasicTextFieldEmbedder({\"tokens\": en_embedding})\n",
    "\n",
    "attention = DotProductAttention()\n",
    "\n",
    "max_decoding_steps = 20   # TODO: make this variable\n",
    "model = SimpleSeq2Seq(vocab, source_embedder, encoder, max_decoding_steps,\n",
    "                          target_embedding_dim=ZH_EMBEDDING_DIM,\n",
    "                          target_namespace='target_tokens',\n",
    "                          attention=attention,\n",
    "                          beam_size=8,\n",
    "                          use_bleu=True).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"source_tokens\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                      optimizer=optimizer,\n",
    "                      iterator=iterator,\n",
    "                      train_dataset=train_dataset,\n",
    "                      validation_dataset=validation_dataset,\n",
    "                      num_epochs=1,\n",
    "                      cuda_device=CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 6.1913 ||: 100%|██████████| 125/125 [00:03<00:00, 41.52it/s]\n",
      "BLEU: 0.0002, loss: 5.7586 ||: 100%|██████████| 16/16 [00:00<00:00, 25.53it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['¿']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['¿']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['¿']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: []\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 5.5190 ||: 100%|██████████| 125/125 [00:05<00:00, 22.95it/s]\n",
      "BLEU: 0.0000, loss: 5.5032 ||: 100%|██████████| 16/16 [00:01<00:00, 15.44it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['¿', 'bake']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['¿', 'ati']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['¿', 'bake']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿', 'yoyo', 'ati']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['yoyo']\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 5.2188 ||: 100%|██████████| 125/125 [00:05<00:00, 24.04it/s]\n",
      "BLEU: 0.0279, loss: 5.3261 ||: 100%|██████████| 16/16 [00:01<00:00, 12.91it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['yoyo', 'ati']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['yoyo', 'ati']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['benbo', 'bake']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['benbo', 'bake']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['yoyo', 'ati']\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.9188 ||: 100%|██████████| 125/125 [00:06<00:00, 20.44it/s]\n",
      "BLEU: 0.0369, loss: 5.1097 ||: 100%|██████████| 16/16 [00:01<00:00, 13.15it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['yoyo', 'ati']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['yoyo', 'ati']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['yoyo', 'ati']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿', 'jawe', 'ati', '?']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['yoyo', 'ati']\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.6388 ||: 100%|██████████| 125/125 [00:05<00:00, 20.94it/s]\n",
      "BLEU: 0.0425, loss: 4.9994 ||: 100%|██████████| 16/16 [00:01<00:00, 13.33it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['yoyo', 'ati', 'kopi']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['yoyo', 'ati']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['wishawe', 'itan', 'ainbo', 'bakebo']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿', 'jawe', 'akai', '?']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['tsinkiti']\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.4164 ||: 100%|██████████| 125/125 [00:05<00:00, 22.22it/s]\n",
      "BLEU: 0.0462, loss: 4.9462 ||: 100%|██████████| 16/16 [00:01<00:00, 11.31it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['jaskaaxon', 'yoyo', 'ati']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['yoyo', 'ati', 'atipanke']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['jaskaaxon', 'yoyo', 'ati', 'kopi']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿', 'jawekeskaaxonmein', 'non', 'akai', '?']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['ja', 'pekao']\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.2323 ||: 100%|██████████| 125/125 [00:03<00:00, 41.38it/s]\n",
      "BLEU: 0.0454, loss: 4.9051 ||: 100%|██████████| 16/16 [00:01<00:00, 15.02it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['jan', 'teeti', 'jawekibo']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['yoyo', 'awe']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['wishawe', 'itan', 'yokawe']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿', 'jawekeskaaxonmein', 'non', 'akai', '?']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['ja', 'pekao']\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.0623 ||: 100%|██████████| 125/125 [00:03<00:00, 37.38it/s]\n",
      "BLEU: 0.0476, loss: 4.9044 ||: 100%|██████████| 16/16 [00:01<00:00, 15.59it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['jan', 'teeti', 'jawekibo']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['jato', 'yokawe']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['wishawe', 'itan', 'jan', 'teeti', 'jawekibo']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿', 'jawekeskaaxonmein', 'non', 'akai', '?']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['ja', 'pekao']\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.9055 ||: 100%|██████████| 125/125 [00:02<00:00, 43.41it/s]\n",
      "BLEU: 0.0519, loss: 4.8805 ||: 100%|██████████| 16/16 [00:01<00:00, 13.98it/s]\n",
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['wishawe', 'mesko', 'shinanbo']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['yoyo', 'ati', 'kirika']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['wishawe', 'mesko', 'wishabo']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿', 'jawekeskaaxonmein', 'non', 'ati', 'iki', '?']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['ja', 'pekao']\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.7493 ||: 100%|██████████| 125/125 [00:06<00:00, 18.99it/s]\n",
      "BLEU: 0.0487, loss: 4.8917 ||: 100%|██████████| 16/16 [00:01<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: [@start@, menos, que, y, tantos, como, @end@]\n",
      "GOLD: [@start@, ichatamainoax, iki, ika, @end@]\n",
      "PRED: ['wishawe', 'ja', 'pekao']\n",
      "SOURCE: [@start@, sector, de, biblioteca, @end@]\n",
      "GOLD: [@start@, yoyo, ati, kirikabo, benxoatinko, @end@]\n",
      "PRED: ['yoyo', 'akanai']\n",
      "SOURCE: [@start@, anota, sus, ideas, en, la, pizarra, @end@]\n",
      "GOLD: [@start@, jaton, shinanbo, wishawe, pisarain, @end@]\n",
      "PRED: ['wishawe', 'mesko', 'shinanbo', 'ikainko']\n",
      "SOURCE: [@start@, ¿, qué, dificultades, tuvieron, para, escribir, ?, @end@]\n",
      "GOLD: [@start@, ¿, jawe, atikoma, jawekibomein, akanke, merakin, wishatiain, ?, @end@]\n",
      "PRED: ['¿', 'jawekeskaaxonmein', 'non', 'ati', 'iki', '?']\n",
      "SOURCE: [@start@, recursos, disponibles, @end@]\n",
      "GOLD: [@start@, jain, jayata, jawekibo, biboantibores, @end@]\n",
      "PRED: ['ja', 'pekao']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Epoch: {}'.format(i))\n",
    "    trainer.train()\n",
    "\n",
    "    predictor = SimpleSeq2SeqPredictor(model, reader)\n",
    "\n",
    "    for instance in itertools.islice(validation_dataset, 5):\n",
    "        print('SOURCE:', instance.fields['source_tokens'].tokens)\n",
    "        print('GOLD:', instance.fields['target_tokens'].tokens)\n",
    "        print('PRED:', predictor.predict_instance(instance)['predicted_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
